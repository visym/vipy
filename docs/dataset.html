<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<meta name="google-site-verification" content="aB8LkQegj94_TJPdrcJm2ldIRWyXY82Jp24Gtkdgyn0" />
<title>vipy.dataset API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}#lunr-search{width:100%;font-size:1em;padding:6px 9px 5px 9px;border:1px solid silver}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>vipy.dataset</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/visym/vipy/blob/2ff555b8a23b3e6299d0a41de1f179f6005da60c/vipy/dataset.py#L1-L979" class="git-link">Browse git</a>
</summary>
<pre><code class="python">import os
import numpy as np
from vipy.util import findpkl, toextension, filepath, filebase, jsonlist, ishtml, ispkl, filetail, temphtml, listpkl, listext, templike, tempdir, remkdir, tolist, fileext, writelist, tempcsv, newpathroot, listjson, extlist, filefull, tempdir, groupbyasdict
import random
import vipy
import vipy.util
import shutil
import uuid
import warnings
import copy 
from vipy.util import is_email_address
import hashlib
import pickle
import time
import json
import dill
from vipy.show import colorlist
import matplotlib.pyplot as plt
import gc 
import vipy.metrics


class Dataset():
    &#34;&#34;&#34;vipy.dataset.Dataset() class
    
    Common class to manipulate large sets of vipy objects in parallel

    ```python
    D = vipy.dataset.Dataset([vipy.video.RandomScene(), vipy.video.RandomScene()], id=&#39;random_scene&#39;)
    with vipy.globals.parallel(2):
        D = D.map(lambda v: v.frame(0))
    list(D)
    ```

    Create dataset and export as a directory of json files 

    ```python
    D = vipy.dataset.Dataset([vipy.video.RandomScene(), vipy.video.RandomScene()])
    D.tojsondir(&#39;/tmp/myjsondir&#39;)
    ```
    
    Create dataset from all json or pkl files recursively discovered in a directory and lazy loaded

    ```python
    D = vipy.dataset.Dataset(&#39;/tmp/myjsondir&#39;)  # lazy loading
    ```

    Create dataset from a list of json or pkl files and lazy loaded

    ```python
    D = vipy.dataset.Dataset([&#39;/path/to/file1.json&#39;, &#39;/path/to/file2.json&#39;])  # lazy loading
    ```
    
    Args:
    
        - abspath [bool]: If true, load all lazy elements with absolute path
        - loader [lambda]: a callable loader that will process the object .  This is useful for custom deerialization
        - lazy [bool]: If true, load all pkl or json files using the custom loader when accessed

    .. notes:: Be warned that using the jsondir constructor will load elements on demand, but there are some methods that require loading the entire dataset into memory, and will happily try to do so
    &#34;&#34;&#34;

    def __init__(self, objlist, id=None, abspath=True, loader=None, lazy=False):

        assert loader is None or callable(loader)

        self._saveas_ext = [&#39;pkl&#39;, &#39;json&#39;]
        self._id = id if id is not None else vipy.util.shortuuid(8)
        self._loader = loader  # may not be serializable if lambda is provided
        self._istype_strict = True
        self._lazy_loader = lazy
        self._abspath = abspath

        if isinstance(objlist, str) and (vipy.util.isjsonfile(objlist) or vipy.util.ispklfile(objlist)):
            self._objlist = vipy.util.load(objlist, abspath=abspath)
        elif isinstance(objlist, str) and os.path.isdir(objlist):
            self._objlist = vipy.util.findjson(objlist) + vipy.util.findpkl(objlist)  # recursive
            self._loader = lambda x,b=abspath:  vipy.util.load(x, abspath=b) if (vipy.util.ispkl(x) or vipy.util.isjsonfile(x)) else x
            self._istype_strict = False
            self._lazy_loader = True
        elif lazy and (isinstance(objlist, list) and all([(vipy.util.ispkl(x) or vipy.util.isjsonfile(x)) for x in objlist])):
            self._objlist = objlist 
            self._loader = lambda x,b=abspath:  vipy.util.load(x, abspath=b) if (vipy.util.ispkl(x) or vipy.util.isjsonfile(x)) else x            
            self._istype_strict = False
            self._lazy_loader = True
        else:
            self._objlist = objlist

        self._objlist = tolist(self._objlist)        
        assert len(self._objlist) &gt; 0, &#34;Empty dataset&#34;

        if self._lazy_loader:
            try:
                self[0]
            except Exception as e:
                raise ValueError(&#39;Invalid dataset - Lazy load failed with error &#34;%s&#34;&#39; % str(e))

    def __repr__(self):
        return str(&#39;&lt;vipy.dataset: id=&#34;%s&#34;, len=%d, type=%s&gt;&#39; % (self.id(), len(self), str(type(self[0])) if len(self)&gt;0 else &#39;None&#39;))

    def __iter__(self):
        for k in range(len(self)):
            yield self[k]

    def __getitem__(self, k):
        if isinstance(k, int) or isinstance(k, np.uint64):
            assert k&gt;=0 and k&lt;len(self._objlist), &#34;invalid index&#34;
            x = self._objlist[int(k)]
            return self._loader(x) if self._loader is not None else x
        elif isinstance(k, slice):
            return [self._loader(x) if self._loader is not None else x for x in self._objlist[k.start:k.stop:k.step]]
        else:
            raise ValueError()
            
    def __len__(self):
        return len(self._objlist)
        
    def json(self, encode=True):
        r = vipy.util.class_registry()
        d = {k:v for (k,v) in self.__dict__.items() if not k == &#39;_loader&#39;}
        d[&#39;_objlist&#39;] = [(str(type(v)), v.json(encode=False)) if str(type(v)) in r else v for v in self._objlist]
        return json.dumps(d) if encode else d
        
    @classmethod
    def from_json(cls, s):
        r = vipy.util.class_registry()
        d = json.loads(s) if not isinstance(s, dict) else s  
        return cls(objlist=[r[x[0]](x[1]) if (isinstance(x, tuple) and x[0] in r) else x for x in d[&#39;_objlist&#39;]],
                   id=d[&#39;_id&#39;],
                   abspath=d[&#39;_abspath&#39;])                            

    def id(self, n=None):
        &#34;&#34;&#34;Set or return the dataset id&#34;&#34;&#34;
        if n is None:
            return self._id
        else:
            self._id = n
            return self

    def list(self):
        &#34;&#34;&#34;Return the dataset as a list&#34;&#34;&#34;
        return list(self)
    def tolist(self):
        &#34;&#34;&#34;Alias for self.list()&#34;&#34;&#34;
        return list(self)

    def flatten(self):
        &#34;&#34;&#34;Convert dataset stored as a list of lists into a flat list&#34;&#34;&#34;
        self._objlist = [o for objlist in self._objlist for o in vipy.util.tolist(objlist)]
        return self

    def istype(self, validtype):
        &#34;&#34;&#34;Return True if the all elements (or just the first element if strict=False) in the dataset are of type &#39;validtype&#39;&#34;&#34;&#34;
        return all([any([isinstance(v,t) for t in tolist(validtype)]) for v in self]) if self._istype_strict else any([isinstance(self[0],t) for t in tolist(validtype)])

    def _isvipy(self):
        &#34;&#34;&#34;Return True if all elements in the dataset are of type `vipy.video.Video` or `vipy.image.Image`&#34;&#34;&#34;        
        return self.istype([vipy.image.Image, vipy.video.Video])

    def _is_vipy_video(self):
        &#34;&#34;&#34;Return True if all elements in the dataset are of type `vipy.video.Video`&#34;&#34;&#34;                
        return self.istype([vipy.video.Video])

    def _is_vipy_video_scene(self):
        &#34;&#34;&#34;Return True if all elements in the dataset are of type `vipy.video.Scene`&#34;&#34;&#34;                        
        return self.istype([vipy.video.Scene])

    def _is_vipy_image_scene(self):
        &#34;&#34;&#34;Return True if all elements in the dataset are of type `vipy.video.Scene`&#34;&#34;&#34;                        
        return self.istype([vipy.image.Scene])

    def clone(self, shallow=False):
        &#34;&#34;&#34;Return a deep copy of the dataset&#34;&#34;&#34;
        if shallow:
            objlist = self._objlist
            self._objlist = []  
            D = copy.deepcopy(self)
            self._objlist = objlist  # restore
            return D
        else:
            return copy.deepcopy(self)

    def archive(self, tarfile, delprefix, mediadir=&#39;videos&#39;, format=&#39;json&#39;, castas=vipy.video.Scene, verbose=False, extrafiles=None, novideos=False, md5=True, tmpdir=None, inplace=False, bycategory=False, annotationdir=&#39;annotations&#39;):
        &#34;&#34;&#34;Create a archive file for this dataset.  This will be archived as:

           /path/to/tarfile.{tar.gz|.tgz|.bz2}
              tarfilename
                 tarfilename.{json|pkl}
                 mediadir/
                     video.mp4
                 extras1.ext
                 extras2.ext
        
            Args:
                tarfile: /path/to/tarfilename.tar.gz
                delprefix:  the absolute file path contained in the media filenames to be removed.  If a video has a delprefix=&#39;/a/b&#39; then videos with path /a/b/c/d.mp4&#39; -&gt; &#39;c/d.mp4&#39;, and {JSON|PKL} will be saved with relative paths to mediadir.  This may be a list of delprefixes.
                mediadir:  the subdirectory name of the media to be contained in the archive.  Usually &#34;videos&#34;.             
                extrafiles: list of tuples or singletons [(abspath, filename_in_archive_relative_to_root), &#39;file_in_root_and_in_pwd&#39;, ...], 
                novideos [bool]:  generate a tarball without linking videos, just annotations
                md5 [bool]:  If True, generate the MD5 hash of the tarball using the system &#34;md5sum&#34;, or if md5=&#39;vipy&#39; use a slower python only md5 hash 
                castas [class]:  This should be a vipy class that the vipy objects should be cast to prior to archive.  This is useful for converting priveledged superclasses to a base class prior to export.
                tmpdir:  The path to the temporary directory for construting this dataset.  Defaults to system temp.  This directory will be emptied prior to archive.
                inplace [bool]:  If true, modify the dataset in place to prepare it for archive, else make a copy
                bycategory [bool]: If true, save the annotations in an annotations/ directory by category
                annotationdir [str]: The subdirectory name of annotations to be contained in the archive if bycategory=True.  Usually &#34;annotations&#34; or &#34;json&#34;.

            Example:  

              - Input files contain /path/to/oldvideos/category/video.mp4
              - Output will contain relative paths videos/category/video.mp4

        ```python
        d.archive(&#39;out.tar.gz&#39;, delprefix=&#39;/path/to/oldvideos&#39;, mediadir=&#39;videos&#39;)
        ```
        
            Returns:

                The absolute path to the tarball 
        &#34;&#34;&#34;
        assert self._isvipy(), &#34;Source dataset must contain vipy objects for staging&#34;
        assert all([os.path.isabs(v.filename()) for v in self]), &#34;Input dataset must have only absolute media paths&#34;
        assert len([v for v in self if any([d in v.filename() for d in tolist(delprefix)])]) == len(self), &#34;all media objects must have a provided delprefix for relative path construction&#34;
        assert vipy.util.istgz(tarfile) or vipy.util.isbz2(tarfile) or vipy.util.istar(tarfile), &#34;Allowable extensions are .tar.gz, .tgz, .bz2 or .tar&#34;
        assert shutil.which(&#39;tar&#39;) is not None, &#34;tar not found on path&#34;        
        
        D = self.clone() if not inplace else self   # large memory footprint if inplace=False
        tmpdir = tempdir() if tmpdir is None else remkdir(tmpdir, flush=True)
        stagedir = remkdir(os.path.join(tmpdir, filefull(filetail(tarfile))))
        print(&#39;[vipy.dataset]: creating staging directory &#34;%s&#34;&#39; % stagedir)
        delprefix = [[d for d in tolist(delprefix) if d in v.filename()][0] for v in self]  # select the delprefix per video
        D._objlist = [v.filename(v.filename().replace(os.path.normpath(p), os.path.normpath(os.path.join(stagedir, mediadir))), symlink=not novideos) for (p,v) in zip(delprefix, D.list())]

        # Save annotations:  Split large datasets into annotations grouped by category to help speed up loading         
        if bycategory:
            for (c,V) in vipy.util.groupbyasdict(list(D), lambda v: v.category()).items():
                Dataset(V, id=c).save(os.path.join(stagedir, annotationdir, &#39;%s.%s&#39; % (c, format)), relpath=True, nourl=True, sanitize=True, castas=castas, significant_digits=2, noemail=True, flush=True)
        else:
            pklfile = os.path.join(stagedir, &#39;%s.%s&#39; % (filetail(filefull(tarfile)), format))
            D.save(pklfile, relpath=True, nourl=True, sanitize=True, castas=castas, significant_digits=2, noemail=True, flush=True)
    
        # Copy extras (symlinked) to staging directory
        if extrafiles is not None:
            # extrafiles = [(&#34;/abs/path/in/filesystem.ext&#34;, &#34;rel/path/in/archive.ext&#34;), ... ]
            assert all([((isinstance(e, tuple) or isinstance(e, list)) and len(e) == 2) or isinstance(e, str) for e in extrafiles])
            extrafiles = [e if (isinstance(e, tuple) or isinstance(e, list)) else (e,e) for e in extrafiles]  # tuple-ify files in pwd() and should be put in the tarball root
            for (e, a) in tolist(extrafiles):
                assert os.path.exists(os.path.abspath(e)), &#34;Invalid extras file &#39;%s&#39; - file not found&#34; % e
                remkdir(filepath(os.path.join(stagedir, filetail(e) if a is None else a)))    # make directory in stagedir for symlink
                os.symlink(os.path.abspath(e), os.path.join(stagedir, filetail(e) if a is None else a))

        # System command to run tar
        cmd = (&#39;tar %scvf %s -C %s --dereference %s %s&#39; % (&#39;j&#39; if vipy.util.isbz2(tarfile) else (&#39;z&#39; if vipy.util.istgz(tarfile) else &#39;&#39;), 
                                                           tarfile,
                                                           filepath(stagedir),
                                                           filetail(stagedir),
                                                           &#39; &gt; /dev/null&#39; if not verbose else &#39;&#39;))

        print(&#39;[vipy.dataset]: executing &#34;%s&#34;&#39; % cmd)        
        os.system(cmd)  # too slow to use python &#34;tarfile&#34; package
        print(&#39;[vipy.dataset]: deleting staging directory &#34;%s&#34;&#39; % stagedir)        
        shutil.rmtree(stagedir)

        if md5:
            if shutil.which(&#39;md5sum&#39;) is not None:
                cmd = &#39;md5sum %s&#39; % tarfile
                print(&#39;[vipy.dataset]: executing &#34;%s&#34;&#39; % cmd)        
                os.system(cmd)  # too slow to use python &#34;vipy.downloader.generate_md5(tarball)&#34; for huge datasets
            else:
                print(&#39;[vipy.dataset]: %s, MD5=%s&#39; % (tarfile, vipy.downloader.generate_md5(tarfile)))  # too slow for large datasets, but does not require md5sum on path
        return tarfile
        
    def save(self, outfile, nourl=False, castas=None, relpath=False, sanitize=True, strict=True, significant_digits=2, noemail=True, flush=True, bycategory=False):
        &#34;&#34;&#34;Save the dataset to the provided output filename stored as pkl or json
        
        Args:
            outfile: [str]: The /path/to/out.pkl or /path/to/out.json
            nourl: [bool]: If true, remove all URLs from the media (if present)
            castas: [type]:  Cast all media to the provided type.  This is useful for downcasting to `vipy.video.Scene` from superclasses
            relpath: [bool]: If true, define all file paths in objects relative to the /path/to in /path/to/out.json
            sanitize: [bool]:  If trye, call sanitize() on all objects to remove all private attributes with prepended &#39;__&#39; 
            strict: [bool]: Unused
            significant_digits: [int]: Assign the requested number of significant digits to all bounding boxes in all tracks.  This requires dataset of `vipy.video.Scene`
            noemail: [bool]: If true, scrub the attributes for emails and replace with a hash
            flush: [bool]:  If true, flush the object buffers prior to save
            bycategory [bool[: If trye, then save the dataset to the provided output filename pattern outfile=&#39;/path/to/annotations/*.json&#39; where the wildcard is replaced with the category name

        Returns:        
            This dataset that is quivalent to vipy.dataset.Dataset(&#39;/path/to/outfile.json&#39;)
        &#34;&#34;&#34;
        n = len([v for v in self if v is None])
        if n &gt; 0:
            print(&#39;[vipy.dataset]: removing %d invalid elements&#39; % n)
        objlist = [v for v in self if v is not None]  
        if relpath or nourl or sanitize or flush or noemail or (significant_digits is not None):
            assert self._isvipy(), &#34;Invalid input&#34;
        if relpath:
            print(&#39;[vipy.dataset]: setting relative paths&#39;)
            objlist = [v.relpath(start=filepath(outfile)) if os.path.isabs(v.filename()) else v for v in objlist]
        if nourl: 
            print(&#39;[vipy.dataset]: removing URLs&#39;)
            objlist = [v.nourl() for v in objlist]           
        if sanitize:
            print(&#39;[vipy.dataset]: sanitizing attributes&#39;)                        
            objlist = [v.sanitize() for v in objlist]  # removes all attributes with &#39;__&#39; keys
        if castas is not None:
            assert hasattr(castas, &#39;cast&#39;), &#34;Invalid cast&#34;
            print(&#39;[vipy.dataset]: casting as &#34;%s&#34;&#39; % (str(castas)))
            objlist = [castas.cast(v) for v in objlist]                     
        if significant_digits is not None:
            assert self._is_vipy_video_scene()
            assert isinstance(significant_digits, int) and significant_digits &gt;= 1, &#34;Invalid input&#34;
            objlist = [o.trackmap(lambda t: t.significant_digits(significant_digits)) if o is not None else o for o in objlist]
        if noemail:
            print(&#39;[vipy.dataset]: removing emails&#39;)            
            for o in objlist:
                for (k,v) in o.attributes.items():
                    if isinstance(v, str) and is_email_address(v):
                        o.attributes[k] = hashlib.sha1(v.encode(&#34;UTF-8&#34;)).hexdigest()[0:10]
        if flush:
            objlist = [o.flush() for o in objlist]  

        if bycategory:
            for (c,V) in vipy.util.groupbyasdict(list(self), lambda v: v.category()).items():
                jsonfile = outfile.replace(&#39;*&#39;, c)  # outfile=&#34;/path/to/annotations/*.json&#34;
                d = Dataset(V, id=c).save(jsonfile, relpath=relpath, nourl=nourl, sanitize=sanitize, castas=castas, significant_digits=significant_digits, noemail=noemail, flush=flush, bycategory=False)
                print(&#39;[vipy.dataset]: Saving %s by category to &#34;%s&#34;&#39; % (str(d), jsonfile))                
        else:
            print(&#39;[vipy.dataset]: Saving %s to &#34;%s&#34;&#39; % (str(self), outfile))
            vipy.util.save(objlist, outfile)
        return self

    def classlist(self):
        &#34;&#34;&#34;Return a sorted list of categories in the dataset&#34;&#34;&#34;
        assert self._isvipy(), &#34;Invalid input&#34;
        return sorted(list(set([v.category() for v in self])))

    def classes(self):
        &#34;&#34;&#34;Alias for classlist&#34;&#34;&#34;
        return self.classlist()
    def categories(self):
        &#34;&#34;&#34;Alias for classlist&#34;&#34;&#34;
        return self.classlist()
    def num_classes(self):
        &#34;&#34;&#34;Return the number of unique categories in this dataset&#34;&#34;&#34;
        return len(self.classlist())
    def num_labels(self):
        &#34;&#34;&#34;Alias for num_classes&#34;&#34;&#34;
        return self.num_classes()
    def num_categories(self):
        &#34;&#34;&#34;Alias for num_classes&#34;&#34;&#34;
        return self.num_classes()
    
    
    def class_to_index(self):
        &#34;&#34;&#34;Return a dictionary mapping the unique classes to an integer index.  This is useful for defining a softmax index ordering for categorization&#34;&#34;&#34;
        return {v:k for (k,v) in enumerate(self.classlist())}

    def index_to_class(self):
        &#34;&#34;&#34;Return a dictionary mapping an integer index to the unique class names.  This is the inverse of class_to_index, swapping keys and values&#34;&#34;&#34;
        return {v:k for (k,v) in self.class_to_index().items()}

    def label_to_index(self):
        &#34;&#34;&#34;Alias for class_to_index&#34;&#34;&#34;
        return self.class_to_index()

    def powerset(self):
        return list(sorted(set([tuple(sorted(list(a))) for v in self for a in v.activitylabel() if len(a) &gt; 0])))        

    def powerset_to_index(self):        
        assert self._isvipy(), &#34;Invalid input&#34;
        return {c:k for (k,c) in enumerate(self.powerset())}

    def dedupe(self, key):
        self._objlist = list({key(v):v for v in self}.values())
        return self
        
    def countby(self, f):
        return len([v for v in self if f(v)])

    def union(self, other, key=None):
        assert isinstance(other, Dataset), &#34;invalid input&#34;
        if len(other) &gt; 0:
            try:
                if other._loader is not None:
                    other._loader(self._objlist[0])
                if self._loader is not None:
                    self._loader(other._objlist[0])
                self._objlist = self._objlist + other._objlist  # compatible loaders
            except:
                self._objlist = self.list() + other.list()  # incompatible loaders
                self._loader = None
        return self.dedupe(key) if key is not None else self
    
    def difference(self, other, key):
        assert isinstance(other, Dataset), &#34;invalid input&#34;
        idset = set([key(v) for v in self]).difference([key(v) for v in other])   # in A but not in B
        self._objlist = [v for v in self if key(v) in idset]
        return self
        
    def has(self, val, key):
        return any([key(obj) == val for obj in self])

    def replace(self, other, key):
        &#34;&#34;&#34;Replace elements in self with other with equality detemrined by the key lambda function&#34;&#34;&#34;
        assert isinstance(other, Dataset), &#34;invalid input&#34;
        d = {key(v):v for v in other}
        self._objlist = [v if key(v) not in d else d[key(v)] for v in self]
        return self

    def merge(self, outdir):
        &#34;&#34;&#34;Merge a dataset union into a single subdirectory with symlinked media ready to be archived.

        ```python
        D1 = vipy.dataset.Dataset(&#39;/path1/dataset.json&#39;)
        D2 = vipy.dataset.Dataset(&#39;/path2/dataset.json&#39;)
        D3 = D1.union(D2).merge(outdir=&#39;/path3&#39;)
        ```

        Media in D1 are in /path1, media in D2 are in /path2, media in D3 are all symlinked to /path3.
        We can now create a tarball for D3 with all of the media files in the same relative path.
        &#34;&#34;&#34;
        
        outdir = vipy.util.remkdir(os.path.abspath(os.path.normpath(outdir)))
        return self.clone().localmap(lambda v: v.filename(os.path.join(outdir, filetail(v.filename())), copy=False, symlink=True))

    def augment(self, f, n_augmentations):
        assert n_augmentations &gt;= 1
        self._objlist = [f(v.clone()) for v in self for k in range(n_augmentations)]  # This will remove the originals
        return self

    def filter(self, f):
        &#34;&#34;&#34;In place filter with lambda function f&#34;&#34;&#34;
        self._objlist = [v for v in self if f(v)]
        return self

    def valid(self):
        return self.filter(lambda v: v is not None)

    def takefilter(self, f, n=1):
        &#34;&#34;&#34;Apply the lambda function f and return n elements in a list where the filter returns true
        
        Args:
            f: [lambda] If f(x) returns true, then keep
            n: [int &gt;= 0] The number of elements to take
        
        Returns:
            [n=0] Returns empty list
            [n=1] Returns singleton element
            [n&gt;1] Returns list of elements of at most n such that each element f(x) is True            
        &#34;&#34;&#34;
        objlist = [obj for obj in self if f(obj)]
        return [] if (len(objlist) == 0 or n == 0) else (objlist[0] if n==1 else objlist[0:n])

    def jsondir(self, outdir=None, verbose=True, rekey=False, bycategory=False, byfilename=False, abspath=True):
        &#34;&#34;&#34;Export all objects to a directory of JSON files.
    
           Usage:

        ```python
        D = vipy.dataset.Dataset(...).jsondir(&#39;/path/to/jsondir&#39;)
        D = vipy.util.load(&#39;/path/to/jsondir&#39;)   # recursively discover and lazy load all json files 
        ```

           Args:

               outdir [str]:  The root directory to store the JSON files
               verbose [bool]: If True, print the save progress
               rekey [bool] If False, use the instance ID of the vipy object as the filename for the JSON file, otherwise assign a new UUID_dataset-index
               bycategory [bool]: If True, use the JSON structure &#39;$OUTDIR/$CATEGORY/$INSTANCEID.json&#39;
               byfilename [bool]: If True, use the JSON structure &#39;$FILENAME.json&#39; where $FILENAME is the underlying media filename of the vipy object
               abspath [bool]: If true, store absolute paths to media in JSON.  If false, store relative paths to media from JSON directory

           Returns:
               outdir: The directory containing the JSON files.
        &#34;&#34;&#34;
        assert self._isvipy()
        assert outdir is not None or byfilename 
        assert not byfilename and bycategory

        if outdir is not None:
            vipy.util.remkdir(outdir) 
        if bycategory:
            tojsonfile = lambda v,k: os.path.join(outdir, v.category(), (&#39;%s.json&#39; % v.instanceid()) if not rekey else (&#39;%s_%d.json&#39; % (uuid.uuid4().hex, k)))
        elif byfilename:
            tojsonfile = lambda v,k: vipy.util.toextension(v.filename(), &#39;.json&#39;)
        else:
            tojsonfile = lambda v,k: os.path.join(outdir, (&#39;%s.json&#39; % v.instanceid()) if not rekey else &#39;%s_%d.json&#39; % (uuid.uuid4().hex, k))
        
        for (k,v) in enumerate(self):            
            f = vipy.util.save(v.clone().relpath(start=filepath(tojsonfile(v,k))) if not abspath else v.clone().abspath(), tojsonfile(v,k))
            if verbose:
                print(&#39;[vipy.dataset.Dataset][%d/%d]: %s&#39; % (k, len(self), f))
        return outdir

    def tojsondir(self, outdir=None, verbose=True, rekey=False, bycategory=False, byfilename=False, abspath=True):
        &#34;&#34;&#34;Alias for `vipy.dataset.Dataset.jsondir`&#34;&#34;&#34;
        return self.jsondir(outdir, verbose=verbose, rekey=rekey, bycategory=bycategory, byfilename=byfilename, abspath=abspath)
    
    def takelist(self, n, category=None, seed=None):
        &#34;&#34;&#34;Take n elements of selected category and return list.  The elements are not cloned.&#34;&#34;&#34;
        assert n &gt;= 0, &#34;Invalid length&#34;
        K = list(range(len(self))) if category is None else [k for (k,v) in enumerate(self) if v.category() == category]
        if seed is not None:
            np.random.seed(seed)            
        outlist = [self[int(k)] for k in np.random.permutation(K)[0:n]]  # native python int
        if seed is not None:
            np.random.seed()
        return outlist

    def load(self):
        &#34;&#34;&#34;Load the entire dataset into memory.  This is useful for creating in-memory datasets from lazy load datasets&#34;&#34;&#34;
        self._objlist = self.list()
        self._loader = None
        return self

    def take(self, n=1, category=None, canload=False, seed=None):
        &#34;&#34;&#34;Randomlly Take n elements from the dataset, and return a dataset if n&gt;1, otherwise return the singleton element.  If seed=int, take will return the same results each time.&#34;&#34;&#34;
        assert isinstance(n, int) and n&gt;0
        D = self.clone(shallow=True)
        D._objlist = self.takelist(n, category=category, seed=seed)
        return D if n&gt;1 else D[0]

    def take_per_category(self, n, seed=None):
        D = self.clone(shallow=True)
        D._objlist = [v for c in self.categories() for v in self.takelist(n, category=c, seed=seed)]
        return D
    
    def shuffle(self):
        &#34;&#34;&#34;Randomly permute elements in this dataset&#34;&#34;&#34;
        self._objlist.sort(key=lambda x: random.random())  # in place
        return self

    def chunk(self, n):
        &#34;&#34;&#34;Yield n chunks of this dataset.  Last chunk will be ragged&#34;&#34;&#34;
        for (k,V) in enumerate(vipy.util.chunklist(self._objlist, n)):
            yield Dataset(V, id=&#39;%s_%d&#39; % (self.id(), k), loader=self._loader)

    def split(self, trainfraction=0.9, valfraction=0.1, testfraction=0, seed=42):
        &#34;&#34;&#34;Split the dataset by category by fraction so that video IDs are never in the same set&#34;&#34;&#34;
        assert self._isvipy(), &#34;Invalid input&#34;
        assert trainfraction &gt;=0 and trainfraction &lt;= 1
        assert valfraction &gt;=0 and valfraction &lt;= 1
        assert testfraction &gt;=0 and testfraction &lt;= 1
        assert trainfraction + valfraction + testfraction == 1.0
        np.random.seed(seed)  # deterministic
        
        # Video ID assignment
        A = self.list()
        videoid = list(set([a.videoid() for a in A]))
        np.random.shuffle(videoid)
        (testid, valid, trainid) = vipy.util.dividelist(videoid, (testfraction, valfraction, trainfraction))        
        (testid, valid, trainid) = (set(testid), set(valid), set(trainid))
        d = groupbyasdict(A, lambda a: &#39;testset&#39; if a.videoid() in testid else &#39;valset&#39; if a.videoid() in valid else &#39;trainset&#39;)
        (trainset, testset, valset) = (d[&#39;trainset&#39;] if &#39;trainset&#39; in d else [], 
                                       d[&#39;testset&#39;] if &#39;testset&#39; in d else [], 
                                       d[&#39;valset&#39;] if &#39;valset&#39; in d else [])

        print(&#39;[vipy.dataset]: trainset=%d (%1.2f)&#39; % (len(trainset), trainfraction))
        print(&#39;[vipy.dataset]: valset=%d (%1.2f)&#39; % (len(valset), valfraction))
        print(&#39;[vipy.dataset]: testset=%d (%1.2f)&#39; % (len(testset), testfraction))
        np.random.seed()  # re-initialize seed

        return (Dataset(trainset, id=&#39;trainset&#39;), Dataset(valset, id=&#39;valset&#39;), Dataset(testset, id=&#39;testset&#39;) if len(testset)&gt;0 else None)

    def tocsv(self, csvfile=None):
        csv = [v.csv() for v in self.list]        
        return vipy.util.writecsv(csv, csvfile) if csvfile is not None else (csv[0], csv[1:])

    def map(self, f_map, model=None, dst=None, id=None, strict=False, ascompleted=True, chunks=128, ordered=False):        
        &#34;&#34;&#34;Distributed map.

        To perform this in parallel across four processes:

        ```python
        D = vipy.dataset.Dataset(...)
        with vipy.globals.parallel(4):
            D.map(lambda v: ...)
        ```

        Args:
            f_map: [lambda] The lambda function to apply in parallel to all elements in the dataset.  This must return a JSON serializable object
            model: [torch.nn.Module] The model to scatter to all workers
            dst: [str] The ID to give to the resulting dataset
            id: [str] The ID to give to the resulting dataset (parameter alias for dst)
            strict: [bool] If true, raise exception on map failures, otherwise the map will return None for failed elements
            ascompleted: [bool] If true, return elements as they complete
            ordered: [bool] If true, preserve the order of objects in dataset as returned from distributed processing

        Returns:
            A `vipy.dataset.Dataset` containing the elements f_map(v).  This operation is order preserving if ordered=True.

        .. note:: 
            - This dataset must contain vipy objects of types defined in `vipy.util.class_registry` or JSON serializable objects
            - Serialization of large datasets can take a while, kick it off to a distributed dask scheduler and go get lunch
            - This method uses dask distributed and `vipy.batch.Batch` operations
            - All vipy objects are JSON serialized prior to parallel map to avoid reference cycle garbage collection which can introduce instabilities
            - Due to chunking, all error handling is caught by this method.  Use `vipy.batch.Batch` to leverage dask distributed futures error handling.
            - Operations must be chunked and serialized because each dask task comes with overhead, and lots of small tasks violates best practices
            - Serialized results are deserialized by the client and returned a a new dataset
        &#34;&#34;&#34;
        assert callable(f_map)
        from vipy.batch import Batch   # requires pip install vipy[all]

        # Distributed map using vipy.batch
        f_serialize = lambda v,d=vipy.util.class_registry(): (str(type(v)), v.json()) if str(type(v)) in d else (None, pickle.dumps(v))  # fallback on PKL dumps/loads
        f_deserialize = lambda x,d=vipy.util.class_registry(): d[x[0]](x[1])  # with closure capture
        f_catcher = lambda f, *args, **kwargs: vipy.util.loudcatcher(f, &#39;[vipy.dataset.Dataset.map]: &#39;, *args, **kwargs)  # catch exceptions when executing lambda, print errors and return (True, result) or (False, exception)
        f_loader = self._loader if self._loader is not None else lambda x: x
        S = [f_serialize(v) for v in self._objlist]  # local serialization
        B = Batch(vipy.util.chunklist(S, chunks), strict=strict, as_completed=ascompleted, warnme=False, minscatter=chunks, ordered=ordered)
        if model is None:
            f = lambda x, f_loader=f_loader, f_serializer=f_serialize, f_deserializer=f_deserialize, f_map=f_map, f_catcher=f_catcher: f_serializer(f_catcher(f_map, f_loader(f_deserializer(x))))  # with closure capture
            S = B.map(lambda X,f=f: [f(x) for x in X]).result()  # chunked, with caught exceptions, may return empty list
        else:
            f = lambda net, x, f_loader=f_loader, f_serializer=f_serialize, f_deserializer=f_deserialize, f_map=f_map, f_catcher=f_catcher: f_serializer(f_catcher(f_map, net, f_loader(f_deserializer(x))))  # with closure capture
            S = B.scattermap((lambda net, X, f=f: [f(net, x) for x in X]), model).result()  # chunked, scattered, caught exceptions
        if not isinstance(S, list) or any([not isinstance(s, list) for s in S]):
            raise ValueError(&#39;Distributed processing error - Batch returned: %s&#39; % (str(S)))
        V = [f_deserialize(x) for s in S for x in s]  # Local deserialization and chunk flattening
        (good, bad) = ([r for (b,r) in V if b], [r for (b,r) in V if not b])  # catcher returns (True, result) or (False, exception string)
        if len(bad) &gt; 0:
            print(&#39;[vipy.dataset.Dataset.map]: Exceptions in map distributed processing:\n%s&#39; % str(bad))
            print(&#39;[vipy.dataset.Dataset.map]: %d/%d items failed&#39; % (len(bad), len(self)))
        return Dataset(good, id=dst if dst is not None else id)

    def localmap(self, f):
        for (k,v) in enumerate(self):
            self._objlist[k] = f(v)  # in-place update
        return self

    def flatmap(self, f):
        self._objlist = [x for v in self for x in f(v)]
        return self
    
    def count(self, f=None):
        &#34;&#34;&#34;Counts for each label.  
        
        Args:
            f: [lambda] if provided, count the number of elements that return true.  This is the same as len(self.filter(f)) without modifying the dataset.

        Returns:
            A dictionary of counts per category [if f is None]
            A length of elements that satisfy f(v) = True [if f is not None]
        &#34;&#34;&#34;
        assert self._isvipy()
        assert f is None or callable(f)
        return len([v for v in self if f is None or f(v)])

    def countby(self, f=lambda v: v.category()):
        &#34;&#34;&#34;Count the number of elements that return the same value from the lambda function&#34;&#34;&#34;
        assert self._isvipy()
        assert f is None or callable(f)
        return vipy.util.countby(self, f)
        
    def frequency(self):
        return self.count()

    def synonym(self, synonymdict):
        &#34;&#34;&#34;Convert all categories in the dataset using the provided synonym dictionary mapping&#34;&#34;&#34;
        assert self._isvipy()
        assert isinstance(synonymdict, dict)
        
        if self._is_vipy_video_scene():
            return self.localmap(lambda v: v.trackmap(lambda t: t.categoryif(synonymdict)).activitymap(lambda a: a.categoryif(synonymdict)))
        elif self._is_vipy_image_scene():
            return self.localmap(lambda v: v.objectmap(lambda o: o.categoryif(synonymdict)))
        return self

    def histogram(self, outfile=None, fontsize=6, category_to_barcolor=None, category_to_xlabel=None):
        assert self._isvipy()
        assert category_to_barcolor is None or all([c in category_to_barcolor for c in self.categories()])
        assert category_to_xlabel is None or callable(category_to_xlabel) or all([c in category_to_xlabel for c in self.categories()])
        f_category_to_xlabel = category_to_xlabel if callable(category_to_xlabel) else ((lambda c: category_to_xlabel[c]) if category_to_xlabel is not None else (lambda c: c))
        
        d = self.countby(lambda v: v.category())
        if outfile is not None:
            (categories, freq) = zip(*reversed(sorted(list(d.items()), key=lambda x: x[1])))  # decreasing frequency
            barcolors = [&#39;blue&#39; if category_to_barcolor is None else category_to_barcolor[c] for c in categories]
            xlabels = [f_category_to_xlabel(c) for c in categories]
            print(&#39;[vipy.dataset]: histogram=&#34;%s&#34;&#39; % vipy.metrics.histogram(freq, xlabels, barcolors=barcolors, outfile=outfile, ylabel=&#39;Instances&#39;, fontsize=fontsize))
        return d
    
    def percentage(self):
        &#34;&#34;&#34;Fraction of dataset for each label&#34;&#34;&#34;
        d = self.count()
        n = sum(d.values())
        return {k:v/float(n) for (k,v) in d.items()}

    def multilabel_inverse_frequency_weight(self):
        &#34;&#34;&#34;Return an inverse frequency weight for multilabel activities, where label counts are the fractional label likelihood within a clip&#34;&#34;&#34;
        assert self._is_vipy_video()

        def _multilabel_inverse_frequency_weight(v):
            lbl_likelihood = {}
            if len(v.activities()) &gt; 0:
                (ef, sf) = (max([a.endframe() for a in v.activitylist()]), min([a.startframe() for a in v.activitylist()]))  # clip length 
                lbl_list = [a for A in v.activitylabel(sf, ef) for a in set(A)]  # list of all labels within clip (labels are unique in each frame)
                lbl_frequency = vipy.util.countby(lbl_list, lambda x: x)  # frequency of each label within clip
                lbl_weight = {k:v/float(len(lbl_list)) for (k,v) in lbl_frequency.items()}  # multi-label likelihood within clip, normalized frequency sums to one 
                for (k,w) in lbl_weight.items():
                    if k not in lbl_likelihood:
                        lbl_likelihood[k] = 0
                    lbl_likelihood[k] += w
            return lbl_likelihood
                    
        lbl_likelihood  = {}
        for d in self.map(lambda v: _multilabel_inverse_frequency_weight(v)):  # parallelizable
            for (k,v) in d.items():
                if k not in lbl_likelihood:
                    lbl_likelihood[k] = 0
                lbl_likelihood[k] += v

        # Inverse frequency weight on label likelihood per clip
        d = {k:1.0/max(v,1) for (k,v) in lbl_likelihood.items()}
        n = sum(d.values())  
        return {k:len(d)*(v/float(n)) for (k,v) in d.items()}

    def inverse_frequency_weight(self):
        &#34;&#34;&#34;Return inverse frequency weight for categories in dataset.  Useful for unbalanced class weighting during training&#34;&#34;&#34;
        d = {k:1.0/max(v,1) for (k,v) in self.count().items()}
        n = sum(d.values())
        return {k:len(d)*(v/float(n)) for (k,v) in d.items()}

    def duration_in_frames(self, outfile=None):
        assert self._isvipy()
        d = {k:np.mean([v[1] for v in v]) for (k,v) in groupbyasdict([(a.category(), len(a)) for v in self.list() for a in v.activitylist()], lambda x: x[0]).items()}
        if outfile is not None:
            vipy.metrics.histogram(d.values(), d.keys(), outfile=outfile, ylabel=&#39;Duration (frames)&#39;, fontsize=6)            
        return d

    def duration_in_seconds(self, outfile=None, fontsize=6, max_duration=None):
        &#34;&#34;&#34;Duration of activities&#34;&#34;&#34;
        assert self._isvipy()
        d = {k:np.mean([v[1] for v in v]) for (k,v) in groupbyasdict([(a.category(), len(a)/v.framerate()) for v in self.list() for a in v.activitylist()], lambda x: x[0]).items()}
        if outfile is not None:
            max_duration = max(d.values()) if max_duration is None else max_duration
            vipy.metrics.histogram([min(x, max_duration) for x in d.values()], d.keys(), outfile=outfile, ylabel=&#39;Duration (seconds)&#39;, fontsize=fontsize)            
        return d

    def video_duration_in_seconds(self, outfile=None, fontsize=6, max_duration=None):
        &#34;&#34;&#34;Duration of activities&#34;&#34;&#34;
        assert self._isvipy()
        d = {k:np.mean([d for (c,d) in D]) for (k,D) in groupbyasdict([(v.category(), v.duration()) for v in self.list()], lambda x: x[0]).items()}
        if outfile is not None:
            max_duration = max(d.values()) if max_duration is None else max_duration
            vipy.metrics.histogram([min(x, max_duration) for x in d.values()], d.keys(), outfile=outfile, ylabel=&#39;Duration (seconds)&#39;, fontsize=fontsize)            
        return d
    
    def framerate(self, outfile=None):
        assert self._isvipy()
        d = vipy.util.countby([int(round(v.framerate())) for v in self.list()], lambda x: x)
        if outfile is not None:
            vipy.metrics.pie(d.values(), [&#39;%d fps&#39; % k for k in d.keys()], explode=None, outfile=outfile,  shadow=False)
        return d
        
        
    def density(self, outfile=None, max=None):
        &#34;&#34;&#34;Compute the frequency that each video ID is represented.  This counts how many activities are in a video, truncated at max&#34;&#34;&#34;
        assert self._isvipy()
        d = [len(v) if (max is None or len(v)&lt;= max) else max for (k,v) in groupbyasdict(self.list(), lambda v: v.videoid()).items()]
        d = {k:v for (k,v) in sorted(vipy.util.countby(d, lambda x: x).items(), key=lambda x: x[1], reverse=True)}
        if outfile is not None:
            vipy.metrics.histogram(d.values(), d.keys(), outfile=outfile, ylabel=&#39;Frequency&#39;, xlabel=&#39;Activities per video&#39;, fontsize=6, xrot=None)            
        return d

    def boxsize(self, outfile=None, category_to_color=None, categories=None):
        # Scatterplot of object box sizes
        tracks = [t for s in self.list() for t in s.tracks().values()]        
        (x, y) = zip(*[(t.meanshape()[1], t.meanshape()[0]) for t in tracks])
        object_categories = set([t.category() for t in tracks]) if categories is None else categories

        d = {}        
        for c in object_categories:
            xcyc = [(t.meanshape()[1], t.meanshape()[0]) for t in tracks if ((t.category().lower() == c.lower()) and (t.meanshape() is not None))]
            d[c] = xcyc
        
        if outfile is not None:            
            plt.clf()
            plt.figure()
            plt.grid(True)
            for c in object_categories:
                xcyc = d[c]
                if len(xcyc) &gt; 0:
                    (xc, yc) = zip(*xcyc)
                    plt.scatter(xc, yc, c=category_to_color[c] if category_to_color is not None else &#39;blue&#39;, label=c)
            plt.xlabel(&#39;bounding box (width)&#39;)
            plt.ylabel(&#39;bounding box (height)&#39;)
            plt.axis([0, 1000, 0, 1000])                
            plt.legend()
            plt.gca().set_axisbelow(True)        
            plt.savefig(outfile)
        return d

    def boxsize_by_category(self, outfile=None):
        # Scatterplot of object box sizes
        tracks = [t for s in self.list() for t in s.tracks().values()]        
        (x, y) = zip(*[(t.meanshape()[1], t.meanshape()[0]) for t in tracks])
        object_categories = set([t.category() for t in tracks])
        
        # Mean track size per video category
        d_category_to_xy = {k:np.mean([t.meanshape() for v in vlist for t in v.tracklist()], axis=0) for (k,vlist) in groupbyasdict(self.list(), lambda v: v.category()).items()}

        if outfile is not None:
            plt.clf()
            plt.figure()
            plt.grid(True)
            colors = colorlist()            
            d_category_to_color = {c:colors[k % len(colors)] for (k,c) in enumerate(d_category_to_xy.keys())}
            for c in d_category_to_xy.keys():
                (xc, yc) = d_category_to_xy[c]
                plt.scatter(xc, yc, c=d_category_to_color[c], label=c)
            plt.xlabel(&#39;bounding box (width)&#39;)
            plt.ylabel(&#39;bounding box (height)&#39;)
            plt.axis([0, 600, 0, 600])                
            plt.gca().set_axisbelow(True)        
            lgd = plt.legend(bbox_to_anchor=(1.05, 1), loc=&#39;upper left&#39;, borderaxespad=0.)
            plt.savefig(outfile, bbox_extra_artists=(lgd,), bbox_inches=&#39;tight&#39;)
        return d_category_to_xy

    def boxsize_histogram(self, outfile=None):
        # Scatterplot of object box sizes
        tracks = [t for s in self.list() for t in s.tracks().values()]        
        (x, y) = zip(*[(t.meanshape()[1], t.meanshape()[0]) for t in tracks])
        object_categories = set([t.category() for t in tracks])

        
        # 2D histogram of object box sizes
        for c in object_categories:
            xcyc = [(t.meanshape()[1], t.meanshape()[0]) for t in tracks if ((t.category() == c) and (t.meanshape() is not None))]
            d[c] = xcyc

        if outfile is not None:
            for c in object_categories:            
                xcyc = d[c]
                if len(xcyc) &gt; 0:
                    (xc, yc) = zip(*xcyc)
                    plt.clf()
                    plt.figure()
                    plt.hist2d(xc, yc, bins=10)
                    plt.xlabel(&#39;Bounding box (width)&#39;)
                    plt.ylabel(&#39;Bounding box (height)&#39;)                    
                    plt.savefig(outfile % c)
        return d

    
    def to_torch(self, f_video_to_tensor):
        &#34;&#34;&#34;Return a torch dataset that will apply the lambda function f_video_to_tensor to each element in the dataset on demand&#34;&#34;&#34;
        import vipy.torch
        return vipy.torch.TorchDataset(f_video_to_tensor, self)

    def to_torch_tensordir(self, f_video_to_tensor, outdir, n_augmentations=20, sleep=None):
        &#34;&#34;&#34;Return a TorchTensordir dataset that will load a pkl.bz2 file that contains one of n_augmentations (tensor, label) pairs.
        
        This is useful for fast loading of datasets that contain many videos.

        &#34;&#34;&#34;
        import vipy.torch    # lazy import, requires vipy[all] 
        from vipy.batch import Batch   # requires pip install vipy[all]

        assert self._is_vipy_video_scene()
        outdir = vipy.util.remkdir(outdir)
        self.map(lambda v, f=f_video_to_tensor, outdir=outdir, n_augmentations=n_augmentations: vipy.util.bz2pkl(os.path.join(outdir, &#39;%s.pkl.bz2&#39; % v.instanceid()), [f(v.print(sleep=sleep).clone()) for k in range(0, n_augmentations)]))
        return vipy.torch.Tensordir(outdir)

    def annotate(self, outdir, mindim=512):
        assert self._isvipy()
        f = lambda v, outdir=outdir, mindim=mindim: v.mindim(mindim).annotate(outfile=os.path.join(outdir, &#39;%s.mp4&#39; % v.videoid())).print()
        return self.map(f, dst=&#39;annotate&#39;)

    def tohtml(self, outfile, mindim=512, title=&#39;Visualization&#39;, fraction=1.0, display=False, clip=True, activities=True, category=True):
        &#34;&#34;&#34;Generate a standalone HTML file containing quicklooks for each annotated activity in dataset, along with some helpful provenance information for where the annotation came from&#34;&#34;&#34;
    
        assert ishtml(outfile), &#34;Output file must be .html&#34;
        assert fraction &gt; 0 and fraction &lt;= 1.0, &#34;Fraction must be between [0,1]&#34;
        
        import vipy.util  # This should not be necessary, but we get &#34;UnboundLocalError&#34; without it, not sure why..
        import vipy.batch  # requires pip install vipy[all]

        dataset = self.list()
        assert all([isinstance(v, vipy.video.Video) for v in dataset])
        dataset = [dataset[int(k)] for k in np.random.permutation(range(len(dataset)))[0:int(len(dataset)*fraction)]]
        #dataset = [v for v in dataset if all([len(a) &lt; 15*v.framerate() for a in v.activitylist()])]  # remove extremely long videos

        quicklist = vipy.batch.Batch(dataset, strict=False, as_completed=True, minscatter=1).map(lambda v: (v.load().quicklook(), v.flush().print())).result()
        quicklist = [x for x in quicklist if x is not None]  # remove errors
        quicklooks = [imq for (imq, v) in quicklist]  # keep original video for HTML display purposes
        provenance = [{&#39;clip&#39;:str(v), &#39;activities&#39;:str(&#39;;&#39;.join([str(a) for a in v.activitylist()])), &#39;category&#39;:v.category()} for (imq, v) in quicklist]
        (quicklooks, provenance) = zip(*sorted([(q,p) for (q,p) in zip(quicklooks, provenance)], key=lambda x: x[1][&#39;category&#39;]))  # sorted in category order
        return vipy.visualize.tohtml(quicklooks, provenance, title=&#39;%s&#39; % title, outfile=outfile, mindim=mindim, display=display)


    def video_montage(self, outfile, gridrows, gridcols, mindim=64, bycategory=False, category=None, annotate=True, trackcrop=False, transpose=False, max_duration=None, framerate=30, fontsize=8):
        &#34;&#34;&#34;30x50 activity montage, each 64x64 elements.

        Args:
            outfile: [str] The name of the outfile for the video.  Must have a valid video extension. 
            gridrows: [int, None]  The number of rows to include in the montage.  If None, infer from other args
            gridcols: [int] The number of columns in the montage
            mindim: [int] The square size of each video in the montage
            bycategory: [bool]  Make the video such that each row is a category 
            category: [str, list] Make the video so that every element is of category.  May be a list of more than one categories
            annotate: [bool] If true, include boxes and captions for objects and activities
            trackcrop: [bool] If true, center the video elements on the tracks with dilation factor 1.5
            transpose: [bool] If true, organize categories columnwise, but still return a montage of size (gridrows, gridcols)
            max_duration: [float] If not None, then set a maximum duration in seconds for elements in the video.  If None, then the max duration is the duration of the longest element.

        Returns:
            A clone of the dataset containing the selected videos for the montage, ordered rowwise in the montage

        .. notes::  
            - If a category does not contain the required number of elements for bycategory, it is removed prior to visualization
            - Elements are looped if they exit prior to the end of the longest video (or max_duration)
        &#34;&#34;&#34;
        assert self._is_vipy_video()
        assert vipy.util.isvideo(outfile)
        assert gridrows is None or (isinstance(gridrows, int) and gridrows &gt;= 1)
        assert gridcols is None or (isinstance(gridcols, int) and gridcols &gt;= 1)
        assert isinstance(mindim, int) and mindim &gt;= 1
        assert category is None or isinstance(category, str)

        D = self.clone()
        if bycategory:
            (num_categories, num_elements) = (gridrows, gridcols) if not transpose else (gridcols, gridrows)
            assert num_elements is not None
            requested_categories = sorted(D.classlist()) if (num_categories is None) else sorted(D.classlist())[0:num_categories]             
            categories = [c for c in requested_categories if D.count()[c] &gt;= num_elements]  # filter those categories that do not have enough
            if set(categories) != set(requested_categories):
                warnings.warn(&#39;[vipy.dataset.video_montage]: removing &#34;%s&#34; without at least %d examples&#39; % (str(set(requested_categories).difference(set(categories))), num_elements))
            vidlist = sorted(D.filter(lambda v: v.category() in categories).take_per_category(num_elements).tolist(), key=lambda v: v.category())
            vidlist = vidlist if not transpose else [vidlist[k] for k in np.array(range(0, len(vidlist))).reshape( (len(categories), num_elements) ).transpose().flatten().tolist()] 
            (gridrows, gridcols) = (len(categories), num_elements) if not transpose else (num_elements, len(categories))
            assert len(vidlist) == gridrows*gridcols

        elif category is not None:
            vidlist = D.filter(lambda v: v.category() in vipy.util.tolist(category)).take(gridrows*gridcols, canload=True).tolist()            
        elif len(D) != gridrows*gridcols:
            vidlist = D.take(gridrows*gridcols, canload=True).tolist()
        else:
            vidlist = D.tolist()

        vidlist = [v.framerate(framerate) for v in vidlist]  # resample to common framerate (this may result in jittery tracks
        montage = Dataset(vidlist, id=&#39;video_montage&#39;).clone()  # for output
        vidlist = [v.trackcrop(dilate=1.5, maxsquare=True) if (v.trackbox() is not None) else v for v in vidlist] if trackcrop else vidlist  # may be None, if so return the video
        vidlist = [v.mindim(mindim) for v in vidlist]  # before annotate for common font size
        vidlist = [vipy.video.Video.cast(v) for v in vidlist] if not annotate else [v.annotate(verbose=False, fontsize=fontsize) for v in vidlist]  # pre-annotate
            
        vipy.visualize.videomontage(vidlist, mindim, mindim, gridrows=gridrows, gridcols=gridcols, framerate=framerate, max_duration=max_duration).saveas(outfile)
        return montage        
        
    def zip(self, other, sortkey=None):
        &#34;&#34;&#34;Zip two datasets.  Equivalent to zip(self, other).

        ```python
        for (d1,d2) in D1.zip(D2, sortkey=lambda v: v.instanceid()):
            pass
        
        for (d1, d2) in zip(D1, D2):
            pass
        ```

        Args:
            other: [`vipy.dataset.Dataset`] 
            sortkey: [lambda] sort both datasets using the provided sortkey lambda.
        
        Returns:
            Generator for the tuple sequence ( (self[0], other[0]), (self[1], other[1]), ... )
        &#34;&#34;&#34; 
        assert isinstance(other, Dataset)
        assert len(self) == len(other)

        for (vi, vj) in zip(self.sort(sortkey), other.sort(sortkey)):
            yield (vi, vj)

    def sort(self, key):
        &#34;&#34;&#34;Sort the dataset in-place using the sortkey lambda function&#34;&#34;&#34;
        if key is not None:
            self._objlist.sort(key=lambda x: key(self._loader(x)))
        return self
                </code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="vipy.dataset.Dataset"><code class="flex name class">
<span>class <span class="ident">Dataset</span></span>
<span>(</span><span>objlist, id=None, abspath=True, loader=None, lazy=False)</span>
</code></dt>
<dd>
<div class="desc"><p>vipy.dataset.Dataset() class</p>
<p>Common class to manipulate large sets of vipy objects in parallel</p>
<pre><code class="language-python">D = vipy.dataset.Dataset([vipy.video.RandomScene(), vipy.video.RandomScene()], id='random_scene')
with vipy.globals.parallel(2):
    D = D.map(lambda v: v.frame(0))
list(D)
</code></pre>
<p>Create dataset and export as a directory of json files </p>
<pre><code class="language-python">D = vipy.dataset.Dataset([vipy.video.RandomScene(), vipy.video.RandomScene()])
D.tojsondir('/tmp/myjsondir')
</code></pre>
<p>Create dataset from all json or pkl files recursively discovered in a directory and lazy loaded</p>
<pre><code class="language-python">D = vipy.dataset.Dataset('/tmp/myjsondir')  # lazy loading
</code></pre>
<p>Create dataset from a list of json or pkl files and lazy loaded</p>
<pre><code class="language-python">D = vipy.dataset.Dataset(['/path/to/file1.json', '/path/to/file2.json'])  # lazy loading
</code></pre>
<h2 id="args">Args</h2>
<ul>
<li>abspath [bool]: If true, load all lazy elements with absolute path</li>
<li>loader [lambda]: a callable loader that will process the object .
This is useful for custom deerialization</li>
<li>lazy [bool]: If true, load all pkl or json files using the custom loader when accessed</li>
</ul>
<div class="admonition notes">
<p class="admonition-title">Notes:&ensp;Be warned that using the jsondir constructor will load elements on demand, but there are some methods that require loading the entire dataset into memory, and will happily try to do so</p>
</div></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/visym/vipy/blob/2ff555b8a23b3e6299d0a41de1f179f6005da60c/vipy/dataset.py#L23-L978" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class Dataset():
    &#34;&#34;&#34;vipy.dataset.Dataset() class
    
    Common class to manipulate large sets of vipy objects in parallel

    ```python
    D = vipy.dataset.Dataset([vipy.video.RandomScene(), vipy.video.RandomScene()], id=&#39;random_scene&#39;)
    with vipy.globals.parallel(2):
        D = D.map(lambda v: v.frame(0))
    list(D)
    ```

    Create dataset and export as a directory of json files 

    ```python
    D = vipy.dataset.Dataset([vipy.video.RandomScene(), vipy.video.RandomScene()])
    D.tojsondir(&#39;/tmp/myjsondir&#39;)
    ```
    
    Create dataset from all json or pkl files recursively discovered in a directory and lazy loaded

    ```python
    D = vipy.dataset.Dataset(&#39;/tmp/myjsondir&#39;)  # lazy loading
    ```

    Create dataset from a list of json or pkl files and lazy loaded

    ```python
    D = vipy.dataset.Dataset([&#39;/path/to/file1.json&#39;, &#39;/path/to/file2.json&#39;])  # lazy loading
    ```
    
    Args:
    
        - abspath [bool]: If true, load all lazy elements with absolute path
        - loader [lambda]: a callable loader that will process the object .  This is useful for custom deerialization
        - lazy [bool]: If true, load all pkl or json files using the custom loader when accessed

    .. notes:: Be warned that using the jsondir constructor will load elements on demand, but there are some methods that require loading the entire dataset into memory, and will happily try to do so
    &#34;&#34;&#34;

    def __init__(self, objlist, id=None, abspath=True, loader=None, lazy=False):

        assert loader is None or callable(loader)

        self._saveas_ext = [&#39;pkl&#39;, &#39;json&#39;]
        self._id = id if id is not None else vipy.util.shortuuid(8)
        self._loader = loader  # may not be serializable if lambda is provided
        self._istype_strict = True
        self._lazy_loader = lazy
        self._abspath = abspath

        if isinstance(objlist, str) and (vipy.util.isjsonfile(objlist) or vipy.util.ispklfile(objlist)):
            self._objlist = vipy.util.load(objlist, abspath=abspath)
        elif isinstance(objlist, str) and os.path.isdir(objlist):
            self._objlist = vipy.util.findjson(objlist) + vipy.util.findpkl(objlist)  # recursive
            self._loader = lambda x,b=abspath:  vipy.util.load(x, abspath=b) if (vipy.util.ispkl(x) or vipy.util.isjsonfile(x)) else x
            self._istype_strict = False
            self._lazy_loader = True
        elif lazy and (isinstance(objlist, list) and all([(vipy.util.ispkl(x) or vipy.util.isjsonfile(x)) for x in objlist])):
            self._objlist = objlist 
            self._loader = lambda x,b=abspath:  vipy.util.load(x, abspath=b) if (vipy.util.ispkl(x) or vipy.util.isjsonfile(x)) else x            
            self._istype_strict = False
            self._lazy_loader = True
        else:
            self._objlist = objlist

        self._objlist = tolist(self._objlist)        
        assert len(self._objlist) &gt; 0, &#34;Empty dataset&#34;

        if self._lazy_loader:
            try:
                self[0]
            except Exception as e:
                raise ValueError(&#39;Invalid dataset - Lazy load failed with error &#34;%s&#34;&#39; % str(e))

    def __repr__(self):
        return str(&#39;&lt;vipy.dataset: id=&#34;%s&#34;, len=%d, type=%s&gt;&#39; % (self.id(), len(self), str(type(self[0])) if len(self)&gt;0 else &#39;None&#39;))

    def __iter__(self):
        for k in range(len(self)):
            yield self[k]

    def __getitem__(self, k):
        if isinstance(k, int) or isinstance(k, np.uint64):
            assert k&gt;=0 and k&lt;len(self._objlist), &#34;invalid index&#34;
            x = self._objlist[int(k)]
            return self._loader(x) if self._loader is not None else x
        elif isinstance(k, slice):
            return [self._loader(x) if self._loader is not None else x for x in self._objlist[k.start:k.stop:k.step]]
        else:
            raise ValueError()
            
    def __len__(self):
        return len(self._objlist)
        
    def json(self, encode=True):
        r = vipy.util.class_registry()
        d = {k:v for (k,v) in self.__dict__.items() if not k == &#39;_loader&#39;}
        d[&#39;_objlist&#39;] = [(str(type(v)), v.json(encode=False)) if str(type(v)) in r else v for v in self._objlist]
        return json.dumps(d) if encode else d
        
    @classmethod
    def from_json(cls, s):
        r = vipy.util.class_registry()
        d = json.loads(s) if not isinstance(s, dict) else s  
        return cls(objlist=[r[x[0]](x[1]) if (isinstance(x, tuple) and x[0] in r) else x for x in d[&#39;_objlist&#39;]],
                   id=d[&#39;_id&#39;],
                   abspath=d[&#39;_abspath&#39;])                            

    def id(self, n=None):
        &#34;&#34;&#34;Set or return the dataset id&#34;&#34;&#34;
        if n is None:
            return self._id
        else:
            self._id = n
            return self

    def list(self):
        &#34;&#34;&#34;Return the dataset as a list&#34;&#34;&#34;
        return list(self)
    def tolist(self):
        &#34;&#34;&#34;Alias for self.list()&#34;&#34;&#34;
        return list(self)

    def flatten(self):
        &#34;&#34;&#34;Convert dataset stored as a list of lists into a flat list&#34;&#34;&#34;
        self._objlist = [o for objlist in self._objlist for o in vipy.util.tolist(objlist)]
        return self

    def istype(self, validtype):
        &#34;&#34;&#34;Return True if the all elements (or just the first element if strict=False) in the dataset are of type &#39;validtype&#39;&#34;&#34;&#34;
        return all([any([isinstance(v,t) for t in tolist(validtype)]) for v in self]) if self._istype_strict else any([isinstance(self[0],t) for t in tolist(validtype)])

    def _isvipy(self):
        &#34;&#34;&#34;Return True if all elements in the dataset are of type `vipy.video.Video` or `vipy.image.Image`&#34;&#34;&#34;        
        return self.istype([vipy.image.Image, vipy.video.Video])

    def _is_vipy_video(self):
        &#34;&#34;&#34;Return True if all elements in the dataset are of type `vipy.video.Video`&#34;&#34;&#34;                
        return self.istype([vipy.video.Video])

    def _is_vipy_video_scene(self):
        &#34;&#34;&#34;Return True if all elements in the dataset are of type `vipy.video.Scene`&#34;&#34;&#34;                        
        return self.istype([vipy.video.Scene])

    def _is_vipy_image_scene(self):
        &#34;&#34;&#34;Return True if all elements in the dataset are of type `vipy.video.Scene`&#34;&#34;&#34;                        
        return self.istype([vipy.image.Scene])

    def clone(self, shallow=False):
        &#34;&#34;&#34;Return a deep copy of the dataset&#34;&#34;&#34;
        if shallow:
            objlist = self._objlist
            self._objlist = []  
            D = copy.deepcopy(self)
            self._objlist = objlist  # restore
            return D
        else:
            return copy.deepcopy(self)

    def archive(self, tarfile, delprefix, mediadir=&#39;videos&#39;, format=&#39;json&#39;, castas=vipy.video.Scene, verbose=False, extrafiles=None, novideos=False, md5=True, tmpdir=None, inplace=False, bycategory=False, annotationdir=&#39;annotations&#39;):
        &#34;&#34;&#34;Create a archive file for this dataset.  This will be archived as:

           /path/to/tarfile.{tar.gz|.tgz|.bz2}
              tarfilename
                 tarfilename.{json|pkl}
                 mediadir/
                     video.mp4
                 extras1.ext
                 extras2.ext
        
            Args:
                tarfile: /path/to/tarfilename.tar.gz
                delprefix:  the absolute file path contained in the media filenames to be removed.  If a video has a delprefix=&#39;/a/b&#39; then videos with path /a/b/c/d.mp4&#39; -&gt; &#39;c/d.mp4&#39;, and {JSON|PKL} will be saved with relative paths to mediadir.  This may be a list of delprefixes.
                mediadir:  the subdirectory name of the media to be contained in the archive.  Usually &#34;videos&#34;.             
                extrafiles: list of tuples or singletons [(abspath, filename_in_archive_relative_to_root), &#39;file_in_root_and_in_pwd&#39;, ...], 
                novideos [bool]:  generate a tarball without linking videos, just annotations
                md5 [bool]:  If True, generate the MD5 hash of the tarball using the system &#34;md5sum&#34;, or if md5=&#39;vipy&#39; use a slower python only md5 hash 
                castas [class]:  This should be a vipy class that the vipy objects should be cast to prior to archive.  This is useful for converting priveledged superclasses to a base class prior to export.
                tmpdir:  The path to the temporary directory for construting this dataset.  Defaults to system temp.  This directory will be emptied prior to archive.
                inplace [bool]:  If true, modify the dataset in place to prepare it for archive, else make a copy
                bycategory [bool]: If true, save the annotations in an annotations/ directory by category
                annotationdir [str]: The subdirectory name of annotations to be contained in the archive if bycategory=True.  Usually &#34;annotations&#34; or &#34;json&#34;.

            Example:  

              - Input files contain /path/to/oldvideos/category/video.mp4
              - Output will contain relative paths videos/category/video.mp4

        ```python
        d.archive(&#39;out.tar.gz&#39;, delprefix=&#39;/path/to/oldvideos&#39;, mediadir=&#39;videos&#39;)
        ```
        
            Returns:

                The absolute path to the tarball 
        &#34;&#34;&#34;
        assert self._isvipy(), &#34;Source dataset must contain vipy objects for staging&#34;
        assert all([os.path.isabs(v.filename()) for v in self]), &#34;Input dataset must have only absolute media paths&#34;
        assert len([v for v in self if any([d in v.filename() for d in tolist(delprefix)])]) == len(self), &#34;all media objects must have a provided delprefix for relative path construction&#34;
        assert vipy.util.istgz(tarfile) or vipy.util.isbz2(tarfile) or vipy.util.istar(tarfile), &#34;Allowable extensions are .tar.gz, .tgz, .bz2 or .tar&#34;
        assert shutil.which(&#39;tar&#39;) is not None, &#34;tar not found on path&#34;        
        
        D = self.clone() if not inplace else self   # large memory footprint if inplace=False
        tmpdir = tempdir() if tmpdir is None else remkdir(tmpdir, flush=True)
        stagedir = remkdir(os.path.join(tmpdir, filefull(filetail(tarfile))))
        print(&#39;[vipy.dataset]: creating staging directory &#34;%s&#34;&#39; % stagedir)
        delprefix = [[d for d in tolist(delprefix) if d in v.filename()][0] for v in self]  # select the delprefix per video
        D._objlist = [v.filename(v.filename().replace(os.path.normpath(p), os.path.normpath(os.path.join(stagedir, mediadir))), symlink=not novideos) for (p,v) in zip(delprefix, D.list())]

        # Save annotations:  Split large datasets into annotations grouped by category to help speed up loading         
        if bycategory:
            for (c,V) in vipy.util.groupbyasdict(list(D), lambda v: v.category()).items():
                Dataset(V, id=c).save(os.path.join(stagedir, annotationdir, &#39;%s.%s&#39; % (c, format)), relpath=True, nourl=True, sanitize=True, castas=castas, significant_digits=2, noemail=True, flush=True)
        else:
            pklfile = os.path.join(stagedir, &#39;%s.%s&#39; % (filetail(filefull(tarfile)), format))
            D.save(pklfile, relpath=True, nourl=True, sanitize=True, castas=castas, significant_digits=2, noemail=True, flush=True)
    
        # Copy extras (symlinked) to staging directory
        if extrafiles is not None:
            # extrafiles = [(&#34;/abs/path/in/filesystem.ext&#34;, &#34;rel/path/in/archive.ext&#34;), ... ]
            assert all([((isinstance(e, tuple) or isinstance(e, list)) and len(e) == 2) or isinstance(e, str) for e in extrafiles])
            extrafiles = [e if (isinstance(e, tuple) or isinstance(e, list)) else (e,e) for e in extrafiles]  # tuple-ify files in pwd() and should be put in the tarball root
            for (e, a) in tolist(extrafiles):
                assert os.path.exists(os.path.abspath(e)), &#34;Invalid extras file &#39;%s&#39; - file not found&#34; % e
                remkdir(filepath(os.path.join(stagedir, filetail(e) if a is None else a)))    # make directory in stagedir for symlink
                os.symlink(os.path.abspath(e), os.path.join(stagedir, filetail(e) if a is None else a))

        # System command to run tar
        cmd = (&#39;tar %scvf %s -C %s --dereference %s %s&#39; % (&#39;j&#39; if vipy.util.isbz2(tarfile) else (&#39;z&#39; if vipy.util.istgz(tarfile) else &#39;&#39;), 
                                                           tarfile,
                                                           filepath(stagedir),
                                                           filetail(stagedir),
                                                           &#39; &gt; /dev/null&#39; if not verbose else &#39;&#39;))

        print(&#39;[vipy.dataset]: executing &#34;%s&#34;&#39; % cmd)        
        os.system(cmd)  # too slow to use python &#34;tarfile&#34; package
        print(&#39;[vipy.dataset]: deleting staging directory &#34;%s&#34;&#39; % stagedir)        
        shutil.rmtree(stagedir)

        if md5:
            if shutil.which(&#39;md5sum&#39;) is not None:
                cmd = &#39;md5sum %s&#39; % tarfile
                print(&#39;[vipy.dataset]: executing &#34;%s&#34;&#39; % cmd)        
                os.system(cmd)  # too slow to use python &#34;vipy.downloader.generate_md5(tarball)&#34; for huge datasets
            else:
                print(&#39;[vipy.dataset]: %s, MD5=%s&#39; % (tarfile, vipy.downloader.generate_md5(tarfile)))  # too slow for large datasets, but does not require md5sum on path
        return tarfile
        
    def save(self, outfile, nourl=False, castas=None, relpath=False, sanitize=True, strict=True, significant_digits=2, noemail=True, flush=True, bycategory=False):
        &#34;&#34;&#34;Save the dataset to the provided output filename stored as pkl or json
        
        Args:
            outfile: [str]: The /path/to/out.pkl or /path/to/out.json
            nourl: [bool]: If true, remove all URLs from the media (if present)
            castas: [type]:  Cast all media to the provided type.  This is useful for downcasting to `vipy.video.Scene` from superclasses
            relpath: [bool]: If true, define all file paths in objects relative to the /path/to in /path/to/out.json
            sanitize: [bool]:  If trye, call sanitize() on all objects to remove all private attributes with prepended &#39;__&#39; 
            strict: [bool]: Unused
            significant_digits: [int]: Assign the requested number of significant digits to all bounding boxes in all tracks.  This requires dataset of `vipy.video.Scene`
            noemail: [bool]: If true, scrub the attributes for emails and replace with a hash
            flush: [bool]:  If true, flush the object buffers prior to save
            bycategory [bool[: If trye, then save the dataset to the provided output filename pattern outfile=&#39;/path/to/annotations/*.json&#39; where the wildcard is replaced with the category name

        Returns:        
            This dataset that is quivalent to vipy.dataset.Dataset(&#39;/path/to/outfile.json&#39;)
        &#34;&#34;&#34;
        n = len([v for v in self if v is None])
        if n &gt; 0:
            print(&#39;[vipy.dataset]: removing %d invalid elements&#39; % n)
        objlist = [v for v in self if v is not None]  
        if relpath or nourl or sanitize or flush or noemail or (significant_digits is not None):
            assert self._isvipy(), &#34;Invalid input&#34;
        if relpath:
            print(&#39;[vipy.dataset]: setting relative paths&#39;)
            objlist = [v.relpath(start=filepath(outfile)) if os.path.isabs(v.filename()) else v for v in objlist]
        if nourl: 
            print(&#39;[vipy.dataset]: removing URLs&#39;)
            objlist = [v.nourl() for v in objlist]           
        if sanitize:
            print(&#39;[vipy.dataset]: sanitizing attributes&#39;)                        
            objlist = [v.sanitize() for v in objlist]  # removes all attributes with &#39;__&#39; keys
        if castas is not None:
            assert hasattr(castas, &#39;cast&#39;), &#34;Invalid cast&#34;
            print(&#39;[vipy.dataset]: casting as &#34;%s&#34;&#39; % (str(castas)))
            objlist = [castas.cast(v) for v in objlist]                     
        if significant_digits is not None:
            assert self._is_vipy_video_scene()
            assert isinstance(significant_digits, int) and significant_digits &gt;= 1, &#34;Invalid input&#34;
            objlist = [o.trackmap(lambda t: t.significant_digits(significant_digits)) if o is not None else o for o in objlist]
        if noemail:
            print(&#39;[vipy.dataset]: removing emails&#39;)            
            for o in objlist:
                for (k,v) in o.attributes.items():
                    if isinstance(v, str) and is_email_address(v):
                        o.attributes[k] = hashlib.sha1(v.encode(&#34;UTF-8&#34;)).hexdigest()[0:10]
        if flush:
            objlist = [o.flush() for o in objlist]  

        if bycategory:
            for (c,V) in vipy.util.groupbyasdict(list(self), lambda v: v.category()).items():
                jsonfile = outfile.replace(&#39;*&#39;, c)  # outfile=&#34;/path/to/annotations/*.json&#34;
                d = Dataset(V, id=c).save(jsonfile, relpath=relpath, nourl=nourl, sanitize=sanitize, castas=castas, significant_digits=significant_digits, noemail=noemail, flush=flush, bycategory=False)
                print(&#39;[vipy.dataset]: Saving %s by category to &#34;%s&#34;&#39; % (str(d), jsonfile))                
        else:
            print(&#39;[vipy.dataset]: Saving %s to &#34;%s&#34;&#39; % (str(self), outfile))
            vipy.util.save(objlist, outfile)
        return self

    def classlist(self):
        &#34;&#34;&#34;Return a sorted list of categories in the dataset&#34;&#34;&#34;
        assert self._isvipy(), &#34;Invalid input&#34;
        return sorted(list(set([v.category() for v in self])))

    def classes(self):
        &#34;&#34;&#34;Alias for classlist&#34;&#34;&#34;
        return self.classlist()
    def categories(self):
        &#34;&#34;&#34;Alias for classlist&#34;&#34;&#34;
        return self.classlist()
    def num_classes(self):
        &#34;&#34;&#34;Return the number of unique categories in this dataset&#34;&#34;&#34;
        return len(self.classlist())
    def num_labels(self):
        &#34;&#34;&#34;Alias for num_classes&#34;&#34;&#34;
        return self.num_classes()
    def num_categories(self):
        &#34;&#34;&#34;Alias for num_classes&#34;&#34;&#34;
        return self.num_classes()
    
    
    def class_to_index(self):
        &#34;&#34;&#34;Return a dictionary mapping the unique classes to an integer index.  This is useful for defining a softmax index ordering for categorization&#34;&#34;&#34;
        return {v:k for (k,v) in enumerate(self.classlist())}

    def index_to_class(self):
        &#34;&#34;&#34;Return a dictionary mapping an integer index to the unique class names.  This is the inverse of class_to_index, swapping keys and values&#34;&#34;&#34;
        return {v:k for (k,v) in self.class_to_index().items()}

    def label_to_index(self):
        &#34;&#34;&#34;Alias for class_to_index&#34;&#34;&#34;
        return self.class_to_index()

    def powerset(self):
        return list(sorted(set([tuple(sorted(list(a))) for v in self for a in v.activitylabel() if len(a) &gt; 0])))        

    def powerset_to_index(self):        
        assert self._isvipy(), &#34;Invalid input&#34;
        return {c:k for (k,c) in enumerate(self.powerset())}

    def dedupe(self, key):
        self._objlist = list({key(v):v for v in self}.values())
        return self
        
    def countby(self, f):
        return len([v for v in self if f(v)])

    def union(self, other, key=None):
        assert isinstance(other, Dataset), &#34;invalid input&#34;
        if len(other) &gt; 0:
            try:
                if other._loader is not None:
                    other._loader(self._objlist[0])
                if self._loader is not None:
                    self._loader(other._objlist[0])
                self._objlist = self._objlist + other._objlist  # compatible loaders
            except:
                self._objlist = self.list() + other.list()  # incompatible loaders
                self._loader = None
        return self.dedupe(key) if key is not None else self
    
    def difference(self, other, key):
        assert isinstance(other, Dataset), &#34;invalid input&#34;
        idset = set([key(v) for v in self]).difference([key(v) for v in other])   # in A but not in B
        self._objlist = [v for v in self if key(v) in idset]
        return self
        
    def has(self, val, key):
        return any([key(obj) == val for obj in self])

    def replace(self, other, key):
        &#34;&#34;&#34;Replace elements in self with other with equality detemrined by the key lambda function&#34;&#34;&#34;
        assert isinstance(other, Dataset), &#34;invalid input&#34;
        d = {key(v):v for v in other}
        self._objlist = [v if key(v) not in d else d[key(v)] for v in self]
        return self

    def merge(self, outdir):
        &#34;&#34;&#34;Merge a dataset union into a single subdirectory with symlinked media ready to be archived.

        ```python
        D1 = vipy.dataset.Dataset(&#39;/path1/dataset.json&#39;)
        D2 = vipy.dataset.Dataset(&#39;/path2/dataset.json&#39;)
        D3 = D1.union(D2).merge(outdir=&#39;/path3&#39;)
        ```

        Media in D1 are in /path1, media in D2 are in /path2, media in D3 are all symlinked to /path3.
        We can now create a tarball for D3 with all of the media files in the same relative path.
        &#34;&#34;&#34;
        
        outdir = vipy.util.remkdir(os.path.abspath(os.path.normpath(outdir)))
        return self.clone().localmap(lambda v: v.filename(os.path.join(outdir, filetail(v.filename())), copy=False, symlink=True))

    def augment(self, f, n_augmentations):
        assert n_augmentations &gt;= 1
        self._objlist = [f(v.clone()) for v in self for k in range(n_augmentations)]  # This will remove the originals
        return self

    def filter(self, f):
        &#34;&#34;&#34;In place filter with lambda function f&#34;&#34;&#34;
        self._objlist = [v for v in self if f(v)]
        return self

    def valid(self):
        return self.filter(lambda v: v is not None)

    def takefilter(self, f, n=1):
        &#34;&#34;&#34;Apply the lambda function f and return n elements in a list where the filter returns true
        
        Args:
            f: [lambda] If f(x) returns true, then keep
            n: [int &gt;= 0] The number of elements to take
        
        Returns:
            [n=0] Returns empty list
            [n=1] Returns singleton element
            [n&gt;1] Returns list of elements of at most n such that each element f(x) is True            
        &#34;&#34;&#34;
        objlist = [obj for obj in self if f(obj)]
        return [] if (len(objlist) == 0 or n == 0) else (objlist[0] if n==1 else objlist[0:n])

    def jsondir(self, outdir=None, verbose=True, rekey=False, bycategory=False, byfilename=False, abspath=True):
        &#34;&#34;&#34;Export all objects to a directory of JSON files.
    
           Usage:

        ```python
        D = vipy.dataset.Dataset(...).jsondir(&#39;/path/to/jsondir&#39;)
        D = vipy.util.load(&#39;/path/to/jsondir&#39;)   # recursively discover and lazy load all json files 
        ```

           Args:

               outdir [str]:  The root directory to store the JSON files
               verbose [bool]: If True, print the save progress
               rekey [bool] If False, use the instance ID of the vipy object as the filename for the JSON file, otherwise assign a new UUID_dataset-index
               bycategory [bool]: If True, use the JSON structure &#39;$OUTDIR/$CATEGORY/$INSTANCEID.json&#39;
               byfilename [bool]: If True, use the JSON structure &#39;$FILENAME.json&#39; where $FILENAME is the underlying media filename of the vipy object
               abspath [bool]: If true, store absolute paths to media in JSON.  If false, store relative paths to media from JSON directory

           Returns:
               outdir: The directory containing the JSON files.
        &#34;&#34;&#34;
        assert self._isvipy()
        assert outdir is not None or byfilename 
        assert not byfilename and bycategory

        if outdir is not None:
            vipy.util.remkdir(outdir) 
        if bycategory:
            tojsonfile = lambda v,k: os.path.join(outdir, v.category(), (&#39;%s.json&#39; % v.instanceid()) if not rekey else (&#39;%s_%d.json&#39; % (uuid.uuid4().hex, k)))
        elif byfilename:
            tojsonfile = lambda v,k: vipy.util.toextension(v.filename(), &#39;.json&#39;)
        else:
            tojsonfile = lambda v,k: os.path.join(outdir, (&#39;%s.json&#39; % v.instanceid()) if not rekey else &#39;%s_%d.json&#39; % (uuid.uuid4().hex, k))
        
        for (k,v) in enumerate(self):            
            f = vipy.util.save(v.clone().relpath(start=filepath(tojsonfile(v,k))) if not abspath else v.clone().abspath(), tojsonfile(v,k))
            if verbose:
                print(&#39;[vipy.dataset.Dataset][%d/%d]: %s&#39; % (k, len(self), f))
        return outdir

    def tojsondir(self, outdir=None, verbose=True, rekey=False, bycategory=False, byfilename=False, abspath=True):
        &#34;&#34;&#34;Alias for `vipy.dataset.Dataset.jsondir`&#34;&#34;&#34;
        return self.jsondir(outdir, verbose=verbose, rekey=rekey, bycategory=bycategory, byfilename=byfilename, abspath=abspath)
    
    def takelist(self, n, category=None, seed=None):
        &#34;&#34;&#34;Take n elements of selected category and return list.  The elements are not cloned.&#34;&#34;&#34;
        assert n &gt;= 0, &#34;Invalid length&#34;
        K = list(range(len(self))) if category is None else [k for (k,v) in enumerate(self) if v.category() == category]
        if seed is not None:
            np.random.seed(seed)            
        outlist = [self[int(k)] for k in np.random.permutation(K)[0:n]]  # native python int
        if seed is not None:
            np.random.seed()
        return outlist

    def load(self):
        &#34;&#34;&#34;Load the entire dataset into memory.  This is useful for creating in-memory datasets from lazy load datasets&#34;&#34;&#34;
        self._objlist = self.list()
        self._loader = None
        return self

    def take(self, n=1, category=None, canload=False, seed=None):
        &#34;&#34;&#34;Randomlly Take n elements from the dataset, and return a dataset if n&gt;1, otherwise return the singleton element.  If seed=int, take will return the same results each time.&#34;&#34;&#34;
        assert isinstance(n, int) and n&gt;0
        D = self.clone(shallow=True)
        D._objlist = self.takelist(n, category=category, seed=seed)
        return D if n&gt;1 else D[0]

    def take_per_category(self, n, seed=None):
        D = self.clone(shallow=True)
        D._objlist = [v for c in self.categories() for v in self.takelist(n, category=c, seed=seed)]
        return D
    
    def shuffle(self):
        &#34;&#34;&#34;Randomly permute elements in this dataset&#34;&#34;&#34;
        self._objlist.sort(key=lambda x: random.random())  # in place
        return self

    def chunk(self, n):
        &#34;&#34;&#34;Yield n chunks of this dataset.  Last chunk will be ragged&#34;&#34;&#34;
        for (k,V) in enumerate(vipy.util.chunklist(self._objlist, n)):
            yield Dataset(V, id=&#39;%s_%d&#39; % (self.id(), k), loader=self._loader)

    def split(self, trainfraction=0.9, valfraction=0.1, testfraction=0, seed=42):
        &#34;&#34;&#34;Split the dataset by category by fraction so that video IDs are never in the same set&#34;&#34;&#34;
        assert self._isvipy(), &#34;Invalid input&#34;
        assert trainfraction &gt;=0 and trainfraction &lt;= 1
        assert valfraction &gt;=0 and valfraction &lt;= 1
        assert testfraction &gt;=0 and testfraction &lt;= 1
        assert trainfraction + valfraction + testfraction == 1.0
        np.random.seed(seed)  # deterministic
        
        # Video ID assignment
        A = self.list()
        videoid = list(set([a.videoid() for a in A]))
        np.random.shuffle(videoid)
        (testid, valid, trainid) = vipy.util.dividelist(videoid, (testfraction, valfraction, trainfraction))        
        (testid, valid, trainid) = (set(testid), set(valid), set(trainid))
        d = groupbyasdict(A, lambda a: &#39;testset&#39; if a.videoid() in testid else &#39;valset&#39; if a.videoid() in valid else &#39;trainset&#39;)
        (trainset, testset, valset) = (d[&#39;trainset&#39;] if &#39;trainset&#39; in d else [], 
                                       d[&#39;testset&#39;] if &#39;testset&#39; in d else [], 
                                       d[&#39;valset&#39;] if &#39;valset&#39; in d else [])

        print(&#39;[vipy.dataset]: trainset=%d (%1.2f)&#39; % (len(trainset), trainfraction))
        print(&#39;[vipy.dataset]: valset=%d (%1.2f)&#39; % (len(valset), valfraction))
        print(&#39;[vipy.dataset]: testset=%d (%1.2f)&#39; % (len(testset), testfraction))
        np.random.seed()  # re-initialize seed

        return (Dataset(trainset, id=&#39;trainset&#39;), Dataset(valset, id=&#39;valset&#39;), Dataset(testset, id=&#39;testset&#39;) if len(testset)&gt;0 else None)

    def tocsv(self, csvfile=None):
        csv = [v.csv() for v in self.list]        
        return vipy.util.writecsv(csv, csvfile) if csvfile is not None else (csv[0], csv[1:])

    def map(self, f_map, model=None, dst=None, id=None, strict=False, ascompleted=True, chunks=128, ordered=False):        
        &#34;&#34;&#34;Distributed map.

        To perform this in parallel across four processes:

        ```python
        D = vipy.dataset.Dataset(...)
        with vipy.globals.parallel(4):
            D.map(lambda v: ...)
        ```

        Args:
            f_map: [lambda] The lambda function to apply in parallel to all elements in the dataset.  This must return a JSON serializable object
            model: [torch.nn.Module] The model to scatter to all workers
            dst: [str] The ID to give to the resulting dataset
            id: [str] The ID to give to the resulting dataset (parameter alias for dst)
            strict: [bool] If true, raise exception on map failures, otherwise the map will return None for failed elements
            ascompleted: [bool] If true, return elements as they complete
            ordered: [bool] If true, preserve the order of objects in dataset as returned from distributed processing

        Returns:
            A `vipy.dataset.Dataset` containing the elements f_map(v).  This operation is order preserving if ordered=True.

        .. note:: 
            - This dataset must contain vipy objects of types defined in `vipy.util.class_registry` or JSON serializable objects
            - Serialization of large datasets can take a while, kick it off to a distributed dask scheduler and go get lunch
            - This method uses dask distributed and `vipy.batch.Batch` operations
            - All vipy objects are JSON serialized prior to parallel map to avoid reference cycle garbage collection which can introduce instabilities
            - Due to chunking, all error handling is caught by this method.  Use `vipy.batch.Batch` to leverage dask distributed futures error handling.
            - Operations must be chunked and serialized because each dask task comes with overhead, and lots of small tasks violates best practices
            - Serialized results are deserialized by the client and returned a a new dataset
        &#34;&#34;&#34;
        assert callable(f_map)
        from vipy.batch import Batch   # requires pip install vipy[all]

        # Distributed map using vipy.batch
        f_serialize = lambda v,d=vipy.util.class_registry(): (str(type(v)), v.json()) if str(type(v)) in d else (None, pickle.dumps(v))  # fallback on PKL dumps/loads
        f_deserialize = lambda x,d=vipy.util.class_registry(): d[x[0]](x[1])  # with closure capture
        f_catcher = lambda f, *args, **kwargs: vipy.util.loudcatcher(f, &#39;[vipy.dataset.Dataset.map]: &#39;, *args, **kwargs)  # catch exceptions when executing lambda, print errors and return (True, result) or (False, exception)
        f_loader = self._loader if self._loader is not None else lambda x: x
        S = [f_serialize(v) for v in self._objlist]  # local serialization
        B = Batch(vipy.util.chunklist(S, chunks), strict=strict, as_completed=ascompleted, warnme=False, minscatter=chunks, ordered=ordered)
        if model is None:
            f = lambda x, f_loader=f_loader, f_serializer=f_serialize, f_deserializer=f_deserialize, f_map=f_map, f_catcher=f_catcher: f_serializer(f_catcher(f_map, f_loader(f_deserializer(x))))  # with closure capture
            S = B.map(lambda X,f=f: [f(x) for x in X]).result()  # chunked, with caught exceptions, may return empty list
        else:
            f = lambda net, x, f_loader=f_loader, f_serializer=f_serialize, f_deserializer=f_deserialize, f_map=f_map, f_catcher=f_catcher: f_serializer(f_catcher(f_map, net, f_loader(f_deserializer(x))))  # with closure capture
            S = B.scattermap((lambda net, X, f=f: [f(net, x) for x in X]), model).result()  # chunked, scattered, caught exceptions
        if not isinstance(S, list) or any([not isinstance(s, list) for s in S]):
            raise ValueError(&#39;Distributed processing error - Batch returned: %s&#39; % (str(S)))
        V = [f_deserialize(x) for s in S for x in s]  # Local deserialization and chunk flattening
        (good, bad) = ([r for (b,r) in V if b], [r for (b,r) in V if not b])  # catcher returns (True, result) or (False, exception string)
        if len(bad) &gt; 0:
            print(&#39;[vipy.dataset.Dataset.map]: Exceptions in map distributed processing:\n%s&#39; % str(bad))
            print(&#39;[vipy.dataset.Dataset.map]: %d/%d items failed&#39; % (len(bad), len(self)))
        return Dataset(good, id=dst if dst is not None else id)

    def localmap(self, f):
        for (k,v) in enumerate(self):
            self._objlist[k] = f(v)  # in-place update
        return self

    def flatmap(self, f):
        self._objlist = [x for v in self for x in f(v)]
        return self
    
    def count(self, f=None):
        &#34;&#34;&#34;Counts for each label.  
        
        Args:
            f: [lambda] if provided, count the number of elements that return true.  This is the same as len(self.filter(f)) without modifying the dataset.

        Returns:
            A dictionary of counts per category [if f is None]
            A length of elements that satisfy f(v) = True [if f is not None]
        &#34;&#34;&#34;
        assert self._isvipy()
        assert f is None or callable(f)
        return len([v for v in self if f is None or f(v)])

    def countby(self, f=lambda v: v.category()):
        &#34;&#34;&#34;Count the number of elements that return the same value from the lambda function&#34;&#34;&#34;
        assert self._isvipy()
        assert f is None or callable(f)
        return vipy.util.countby(self, f)
        
    def frequency(self):
        return self.count()

    def synonym(self, synonymdict):
        &#34;&#34;&#34;Convert all categories in the dataset using the provided synonym dictionary mapping&#34;&#34;&#34;
        assert self._isvipy()
        assert isinstance(synonymdict, dict)
        
        if self._is_vipy_video_scene():
            return self.localmap(lambda v: v.trackmap(lambda t: t.categoryif(synonymdict)).activitymap(lambda a: a.categoryif(synonymdict)))
        elif self._is_vipy_image_scene():
            return self.localmap(lambda v: v.objectmap(lambda o: o.categoryif(synonymdict)))
        return self

    def histogram(self, outfile=None, fontsize=6, category_to_barcolor=None, category_to_xlabel=None):
        assert self._isvipy()
        assert category_to_barcolor is None or all([c in category_to_barcolor for c in self.categories()])
        assert category_to_xlabel is None or callable(category_to_xlabel) or all([c in category_to_xlabel for c in self.categories()])
        f_category_to_xlabel = category_to_xlabel if callable(category_to_xlabel) else ((lambda c: category_to_xlabel[c]) if category_to_xlabel is not None else (lambda c: c))
        
        d = self.countby(lambda v: v.category())
        if outfile is not None:
            (categories, freq) = zip(*reversed(sorted(list(d.items()), key=lambda x: x[1])))  # decreasing frequency
            barcolors = [&#39;blue&#39; if category_to_barcolor is None else category_to_barcolor[c] for c in categories]
            xlabels = [f_category_to_xlabel(c) for c in categories]
            print(&#39;[vipy.dataset]: histogram=&#34;%s&#34;&#39; % vipy.metrics.histogram(freq, xlabels, barcolors=barcolors, outfile=outfile, ylabel=&#39;Instances&#39;, fontsize=fontsize))
        return d
    
    def percentage(self):
        &#34;&#34;&#34;Fraction of dataset for each label&#34;&#34;&#34;
        d = self.count()
        n = sum(d.values())
        return {k:v/float(n) for (k,v) in d.items()}

    def multilabel_inverse_frequency_weight(self):
        &#34;&#34;&#34;Return an inverse frequency weight for multilabel activities, where label counts are the fractional label likelihood within a clip&#34;&#34;&#34;
        assert self._is_vipy_video()

        def _multilabel_inverse_frequency_weight(v):
            lbl_likelihood = {}
            if len(v.activities()) &gt; 0:
                (ef, sf) = (max([a.endframe() for a in v.activitylist()]), min([a.startframe() for a in v.activitylist()]))  # clip length 
                lbl_list = [a for A in v.activitylabel(sf, ef) for a in set(A)]  # list of all labels within clip (labels are unique in each frame)
                lbl_frequency = vipy.util.countby(lbl_list, lambda x: x)  # frequency of each label within clip
                lbl_weight = {k:v/float(len(lbl_list)) for (k,v) in lbl_frequency.items()}  # multi-label likelihood within clip, normalized frequency sums to one 
                for (k,w) in lbl_weight.items():
                    if k not in lbl_likelihood:
                        lbl_likelihood[k] = 0
                    lbl_likelihood[k] += w
            return lbl_likelihood
                    
        lbl_likelihood  = {}
        for d in self.map(lambda v: _multilabel_inverse_frequency_weight(v)):  # parallelizable
            for (k,v) in d.items():
                if k not in lbl_likelihood:
                    lbl_likelihood[k] = 0
                lbl_likelihood[k] += v

        # Inverse frequency weight on label likelihood per clip
        d = {k:1.0/max(v,1) for (k,v) in lbl_likelihood.items()}
        n = sum(d.values())  
        return {k:len(d)*(v/float(n)) for (k,v) in d.items()}

    def inverse_frequency_weight(self):
        &#34;&#34;&#34;Return inverse frequency weight for categories in dataset.  Useful for unbalanced class weighting during training&#34;&#34;&#34;
        d = {k:1.0/max(v,1) for (k,v) in self.count().items()}
        n = sum(d.values())
        return {k:len(d)*(v/float(n)) for (k,v) in d.items()}

    def duration_in_frames(self, outfile=None):
        assert self._isvipy()
        d = {k:np.mean([v[1] for v in v]) for (k,v) in groupbyasdict([(a.category(), len(a)) for v in self.list() for a in v.activitylist()], lambda x: x[0]).items()}
        if outfile is not None:
            vipy.metrics.histogram(d.values(), d.keys(), outfile=outfile, ylabel=&#39;Duration (frames)&#39;, fontsize=6)            
        return d

    def duration_in_seconds(self, outfile=None, fontsize=6, max_duration=None):
        &#34;&#34;&#34;Duration of activities&#34;&#34;&#34;
        assert self._isvipy()
        d = {k:np.mean([v[1] for v in v]) for (k,v) in groupbyasdict([(a.category(), len(a)/v.framerate()) for v in self.list() for a in v.activitylist()], lambda x: x[0]).items()}
        if outfile is not None:
            max_duration = max(d.values()) if max_duration is None else max_duration
            vipy.metrics.histogram([min(x, max_duration) for x in d.values()], d.keys(), outfile=outfile, ylabel=&#39;Duration (seconds)&#39;, fontsize=fontsize)            
        return d

    def video_duration_in_seconds(self, outfile=None, fontsize=6, max_duration=None):
        &#34;&#34;&#34;Duration of activities&#34;&#34;&#34;
        assert self._isvipy()
        d = {k:np.mean([d for (c,d) in D]) for (k,D) in groupbyasdict([(v.category(), v.duration()) for v in self.list()], lambda x: x[0]).items()}
        if outfile is not None:
            max_duration = max(d.values()) if max_duration is None else max_duration
            vipy.metrics.histogram([min(x, max_duration) for x in d.values()], d.keys(), outfile=outfile, ylabel=&#39;Duration (seconds)&#39;, fontsize=fontsize)            
        return d
    
    def framerate(self, outfile=None):
        assert self._isvipy()
        d = vipy.util.countby([int(round(v.framerate())) for v in self.list()], lambda x: x)
        if outfile is not None:
            vipy.metrics.pie(d.values(), [&#39;%d fps&#39; % k for k in d.keys()], explode=None, outfile=outfile,  shadow=False)
        return d
        
        
    def density(self, outfile=None, max=None):
        &#34;&#34;&#34;Compute the frequency that each video ID is represented.  This counts how many activities are in a video, truncated at max&#34;&#34;&#34;
        assert self._isvipy()
        d = [len(v) if (max is None or len(v)&lt;= max) else max for (k,v) in groupbyasdict(self.list(), lambda v: v.videoid()).items()]
        d = {k:v for (k,v) in sorted(vipy.util.countby(d, lambda x: x).items(), key=lambda x: x[1], reverse=True)}
        if outfile is not None:
            vipy.metrics.histogram(d.values(), d.keys(), outfile=outfile, ylabel=&#39;Frequency&#39;, xlabel=&#39;Activities per video&#39;, fontsize=6, xrot=None)            
        return d

    def boxsize(self, outfile=None, category_to_color=None, categories=None):
        # Scatterplot of object box sizes
        tracks = [t for s in self.list() for t in s.tracks().values()]        
        (x, y) = zip(*[(t.meanshape()[1], t.meanshape()[0]) for t in tracks])
        object_categories = set([t.category() for t in tracks]) if categories is None else categories

        d = {}        
        for c in object_categories:
            xcyc = [(t.meanshape()[1], t.meanshape()[0]) for t in tracks if ((t.category().lower() == c.lower()) and (t.meanshape() is not None))]
            d[c] = xcyc
        
        if outfile is not None:            
            plt.clf()
            plt.figure()
            plt.grid(True)
            for c in object_categories:
                xcyc = d[c]
                if len(xcyc) &gt; 0:
                    (xc, yc) = zip(*xcyc)
                    plt.scatter(xc, yc, c=category_to_color[c] if category_to_color is not None else &#39;blue&#39;, label=c)
            plt.xlabel(&#39;bounding box (width)&#39;)
            plt.ylabel(&#39;bounding box (height)&#39;)
            plt.axis([0, 1000, 0, 1000])                
            plt.legend()
            plt.gca().set_axisbelow(True)        
            plt.savefig(outfile)
        return d

    def boxsize_by_category(self, outfile=None):
        # Scatterplot of object box sizes
        tracks = [t for s in self.list() for t in s.tracks().values()]        
        (x, y) = zip(*[(t.meanshape()[1], t.meanshape()[0]) for t in tracks])
        object_categories = set([t.category() for t in tracks])
        
        # Mean track size per video category
        d_category_to_xy = {k:np.mean([t.meanshape() for v in vlist for t in v.tracklist()], axis=0) for (k,vlist) in groupbyasdict(self.list(), lambda v: v.category()).items()}

        if outfile is not None:
            plt.clf()
            plt.figure()
            plt.grid(True)
            colors = colorlist()            
            d_category_to_color = {c:colors[k % len(colors)] for (k,c) in enumerate(d_category_to_xy.keys())}
            for c in d_category_to_xy.keys():
                (xc, yc) = d_category_to_xy[c]
                plt.scatter(xc, yc, c=d_category_to_color[c], label=c)
            plt.xlabel(&#39;bounding box (width)&#39;)
            plt.ylabel(&#39;bounding box (height)&#39;)
            plt.axis([0, 600, 0, 600])                
            plt.gca().set_axisbelow(True)        
            lgd = plt.legend(bbox_to_anchor=(1.05, 1), loc=&#39;upper left&#39;, borderaxespad=0.)
            plt.savefig(outfile, bbox_extra_artists=(lgd,), bbox_inches=&#39;tight&#39;)
        return d_category_to_xy

    def boxsize_histogram(self, outfile=None):
        # Scatterplot of object box sizes
        tracks = [t for s in self.list() for t in s.tracks().values()]        
        (x, y) = zip(*[(t.meanshape()[1], t.meanshape()[0]) for t in tracks])
        object_categories = set([t.category() for t in tracks])

        
        # 2D histogram of object box sizes
        for c in object_categories:
            xcyc = [(t.meanshape()[1], t.meanshape()[0]) for t in tracks if ((t.category() == c) and (t.meanshape() is not None))]
            d[c] = xcyc

        if outfile is not None:
            for c in object_categories:            
                xcyc = d[c]
                if len(xcyc) &gt; 0:
                    (xc, yc) = zip(*xcyc)
                    plt.clf()
                    plt.figure()
                    plt.hist2d(xc, yc, bins=10)
                    plt.xlabel(&#39;Bounding box (width)&#39;)
                    plt.ylabel(&#39;Bounding box (height)&#39;)                    
                    plt.savefig(outfile % c)
        return d

    
    def to_torch(self, f_video_to_tensor):
        &#34;&#34;&#34;Return a torch dataset that will apply the lambda function f_video_to_tensor to each element in the dataset on demand&#34;&#34;&#34;
        import vipy.torch
        return vipy.torch.TorchDataset(f_video_to_tensor, self)

    def to_torch_tensordir(self, f_video_to_tensor, outdir, n_augmentations=20, sleep=None):
        &#34;&#34;&#34;Return a TorchTensordir dataset that will load a pkl.bz2 file that contains one of n_augmentations (tensor, label) pairs.
        
        This is useful for fast loading of datasets that contain many videos.

        &#34;&#34;&#34;
        import vipy.torch    # lazy import, requires vipy[all] 
        from vipy.batch import Batch   # requires pip install vipy[all]

        assert self._is_vipy_video_scene()
        outdir = vipy.util.remkdir(outdir)
        self.map(lambda v, f=f_video_to_tensor, outdir=outdir, n_augmentations=n_augmentations: vipy.util.bz2pkl(os.path.join(outdir, &#39;%s.pkl.bz2&#39; % v.instanceid()), [f(v.print(sleep=sleep).clone()) for k in range(0, n_augmentations)]))
        return vipy.torch.Tensordir(outdir)

    def annotate(self, outdir, mindim=512):
        assert self._isvipy()
        f = lambda v, outdir=outdir, mindim=mindim: v.mindim(mindim).annotate(outfile=os.path.join(outdir, &#39;%s.mp4&#39; % v.videoid())).print()
        return self.map(f, dst=&#39;annotate&#39;)

    def tohtml(self, outfile, mindim=512, title=&#39;Visualization&#39;, fraction=1.0, display=False, clip=True, activities=True, category=True):
        &#34;&#34;&#34;Generate a standalone HTML file containing quicklooks for each annotated activity in dataset, along with some helpful provenance information for where the annotation came from&#34;&#34;&#34;
    
        assert ishtml(outfile), &#34;Output file must be .html&#34;
        assert fraction &gt; 0 and fraction &lt;= 1.0, &#34;Fraction must be between [0,1]&#34;
        
        import vipy.util  # This should not be necessary, but we get &#34;UnboundLocalError&#34; without it, not sure why..
        import vipy.batch  # requires pip install vipy[all]

        dataset = self.list()
        assert all([isinstance(v, vipy.video.Video) for v in dataset])
        dataset = [dataset[int(k)] for k in np.random.permutation(range(len(dataset)))[0:int(len(dataset)*fraction)]]
        #dataset = [v for v in dataset if all([len(a) &lt; 15*v.framerate() for a in v.activitylist()])]  # remove extremely long videos

        quicklist = vipy.batch.Batch(dataset, strict=False, as_completed=True, minscatter=1).map(lambda v: (v.load().quicklook(), v.flush().print())).result()
        quicklist = [x for x in quicklist if x is not None]  # remove errors
        quicklooks = [imq for (imq, v) in quicklist]  # keep original video for HTML display purposes
        provenance = [{&#39;clip&#39;:str(v), &#39;activities&#39;:str(&#39;;&#39;.join([str(a) for a in v.activitylist()])), &#39;category&#39;:v.category()} for (imq, v) in quicklist]
        (quicklooks, provenance) = zip(*sorted([(q,p) for (q,p) in zip(quicklooks, provenance)], key=lambda x: x[1][&#39;category&#39;]))  # sorted in category order
        return vipy.visualize.tohtml(quicklooks, provenance, title=&#39;%s&#39; % title, outfile=outfile, mindim=mindim, display=display)


    def video_montage(self, outfile, gridrows, gridcols, mindim=64, bycategory=False, category=None, annotate=True, trackcrop=False, transpose=False, max_duration=None, framerate=30, fontsize=8):
        &#34;&#34;&#34;30x50 activity montage, each 64x64 elements.

        Args:
            outfile: [str] The name of the outfile for the video.  Must have a valid video extension. 
            gridrows: [int, None]  The number of rows to include in the montage.  If None, infer from other args
            gridcols: [int] The number of columns in the montage
            mindim: [int] The square size of each video in the montage
            bycategory: [bool]  Make the video such that each row is a category 
            category: [str, list] Make the video so that every element is of category.  May be a list of more than one categories
            annotate: [bool] If true, include boxes and captions for objects and activities
            trackcrop: [bool] If true, center the video elements on the tracks with dilation factor 1.5
            transpose: [bool] If true, organize categories columnwise, but still return a montage of size (gridrows, gridcols)
            max_duration: [float] If not None, then set a maximum duration in seconds for elements in the video.  If None, then the max duration is the duration of the longest element.

        Returns:
            A clone of the dataset containing the selected videos for the montage, ordered rowwise in the montage

        .. notes::  
            - If a category does not contain the required number of elements for bycategory, it is removed prior to visualization
            - Elements are looped if they exit prior to the end of the longest video (or max_duration)
        &#34;&#34;&#34;
        assert self._is_vipy_video()
        assert vipy.util.isvideo(outfile)
        assert gridrows is None or (isinstance(gridrows, int) and gridrows &gt;= 1)
        assert gridcols is None or (isinstance(gridcols, int) and gridcols &gt;= 1)
        assert isinstance(mindim, int) and mindim &gt;= 1
        assert category is None or isinstance(category, str)

        D = self.clone()
        if bycategory:
            (num_categories, num_elements) = (gridrows, gridcols) if not transpose else (gridcols, gridrows)
            assert num_elements is not None
            requested_categories = sorted(D.classlist()) if (num_categories is None) else sorted(D.classlist())[0:num_categories]             
            categories = [c for c in requested_categories if D.count()[c] &gt;= num_elements]  # filter those categories that do not have enough
            if set(categories) != set(requested_categories):
                warnings.warn(&#39;[vipy.dataset.video_montage]: removing &#34;%s&#34; without at least %d examples&#39; % (str(set(requested_categories).difference(set(categories))), num_elements))
            vidlist = sorted(D.filter(lambda v: v.category() in categories).take_per_category(num_elements).tolist(), key=lambda v: v.category())
            vidlist = vidlist if not transpose else [vidlist[k] for k in np.array(range(0, len(vidlist))).reshape( (len(categories), num_elements) ).transpose().flatten().tolist()] 
            (gridrows, gridcols) = (len(categories), num_elements) if not transpose else (num_elements, len(categories))
            assert len(vidlist) == gridrows*gridcols

        elif category is not None:
            vidlist = D.filter(lambda v: v.category() in vipy.util.tolist(category)).take(gridrows*gridcols, canload=True).tolist()            
        elif len(D) != gridrows*gridcols:
            vidlist = D.take(gridrows*gridcols, canload=True).tolist()
        else:
            vidlist = D.tolist()

        vidlist = [v.framerate(framerate) for v in vidlist]  # resample to common framerate (this may result in jittery tracks
        montage = Dataset(vidlist, id=&#39;video_montage&#39;).clone()  # for output
        vidlist = [v.trackcrop(dilate=1.5, maxsquare=True) if (v.trackbox() is not None) else v for v in vidlist] if trackcrop else vidlist  # may be None, if so return the video
        vidlist = [v.mindim(mindim) for v in vidlist]  # before annotate for common font size
        vidlist = [vipy.video.Video.cast(v) for v in vidlist] if not annotate else [v.annotate(verbose=False, fontsize=fontsize) for v in vidlist]  # pre-annotate
            
        vipy.visualize.videomontage(vidlist, mindim, mindim, gridrows=gridrows, gridcols=gridcols, framerate=framerate, max_duration=max_duration).saveas(outfile)
        return montage        
        
    def zip(self, other, sortkey=None):
        &#34;&#34;&#34;Zip two datasets.  Equivalent to zip(self, other).

        ```python
        for (d1,d2) in D1.zip(D2, sortkey=lambda v: v.instanceid()):
            pass
        
        for (d1, d2) in zip(D1, D2):
            pass
        ```

        Args:
            other: [`vipy.dataset.Dataset`] 
            sortkey: [lambda] sort both datasets using the provided sortkey lambda.
        
        Returns:
            Generator for the tuple sequence ( (self[0], other[0]), (self[1], other[1]), ... )
        &#34;&#34;&#34; 
        assert isinstance(other, Dataset)
        assert len(self) == len(other)

        for (vi, vj) in zip(self.sort(sortkey), other.sort(sortkey)):
            yield (vi, vj)

    def sort(self, key):
        &#34;&#34;&#34;Sort the dataset in-place using the sortkey lambda function&#34;&#34;&#34;
        if key is not None:
            self._objlist.sort(key=lambda x: key(self._loader(x)))
        return self</code></pre>
</details>
<h3>Static methods</h3>
<dl>
<dt id="vipy.dataset.Dataset.from_json"><code class="name flex">
<span>def <span class="ident">from_json</span></span>(<span>s)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/visym/vipy/blob/2ff555b8a23b3e6299d0a41de1f179f6005da60c/vipy/dataset.py#L124-L130" class="git-link">Browse git</a>
</summary>
<pre><code class="python">@classmethod
def from_json(cls, s):
    r = vipy.util.class_registry()
    d = json.loads(s) if not isinstance(s, dict) else s  
    return cls(objlist=[r[x[0]](x[1]) if (isinstance(x, tuple) and x[0] in r) else x for x in d[&#39;_objlist&#39;]],
               id=d[&#39;_id&#39;],
               abspath=d[&#39;_abspath&#39;])                            </code></pre>
</details>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="vipy.dataset.Dataset.annotate"><code class="name flex">
<span>def <span class="ident">annotate</span></span>(<span>self, outdir, mindim=512)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/visym/vipy/blob/2ff555b8a23b3e6299d0a41de1f179f6005da60c/vipy/dataset.py#L865-L868" class="git-link">Browse git</a>
</summary>
<pre><code class="python">def annotate(self, outdir, mindim=512):
    assert self._isvipy()
    f = lambda v, outdir=outdir, mindim=mindim: v.mindim(mindim).annotate(outfile=os.path.join(outdir, &#39;%s.mp4&#39; % v.videoid())).print()
    return self.map(f, dst=&#39;annotate&#39;)</code></pre>
</details>
</dd>
<dt id="vipy.dataset.Dataset.archive"><code class="name flex">
<span>def <span class="ident">archive</span></span>(<span>self, tarfile, delprefix, mediadir='videos', format='json', castas=vipy.video.Scene, verbose=False, extrafiles=None, novideos=False, md5=True, tmpdir=None, inplace=False, bycategory=False, annotationdir='annotations')</span>
</code></dt>
<dd>
<div class="desc"><p>Create a archive file for this dataset.
This will be archived as:</p>
<p>/path/to/tarfile.{tar.gz|.tgz|.bz2}
tarfilename
tarfilename.{json|pkl}
mediadir/
video.mp4
extras1.ext
extras2.ext</p>
<pre><code>Args:
    tarfile: /path/to/tarfilename.tar.gz
    delprefix:  the absolute file path contained in the media filenames to be removed.  If a video has a delprefix='/a/b' then videos with path /a/b/c/d.mp4' -&gt; 'c/d.mp4', and {JSON|PKL} will be saved with relative paths to mediadir.  This may be a list of delprefixes.
    mediadir:  the subdirectory name of the media to be contained in the archive.  Usually "videos".             
    extrafiles: list of tuples or singletons [(abspath, filename_in_archive_relative_to_root), 'file_in_root_and_in_pwd', ...], 
    novideos [bool]:  generate a tarball without linking videos, just annotations
    md5 [bool]:  If True, generate the MD5 hash of the tarball using the system "md5sum", or if md5='vipy' use a slower python only md5 hash 
    castas [class]:  This should be a vipy class that the vipy objects should be cast to prior to archive.  This is useful for converting priveledged superclasses to a base class prior to export.
    tmpdir:  The path to the temporary directory for construting this dataset.  Defaults to system temp.  This directory will be emptied prior to archive.
    inplace [bool]:  If true, modify the dataset in place to prepare it for archive, else make a copy
    bycategory [bool]: If true, save the annotations in an annotations/ directory by category
    annotationdir [str]: The subdirectory name of annotations to be contained in the archive if bycategory=True.  Usually "annotations" or "json".

Example:

  - Input files contain /path/to/oldvideos/category/video.mp4
  - Output will contain relative paths videos/category/video.mp4
</code></pre>
<pre><code class="language-python">d.archive('out.tar.gz', delprefix='/path/to/oldvideos', mediadir='videos')
</code></pre>
<pre><code>Returns:

    The absolute path to the tarball
</code></pre></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/visym/vipy/blob/2ff555b8a23b3e6299d0a41de1f179f6005da60c/vipy/dataset.py#L183-L270" class="git-link">Browse git</a>
</summary>
<pre><code class="python">def archive(self, tarfile, delprefix, mediadir=&#39;videos&#39;, format=&#39;json&#39;, castas=vipy.video.Scene, verbose=False, extrafiles=None, novideos=False, md5=True, tmpdir=None, inplace=False, bycategory=False, annotationdir=&#39;annotations&#39;):
    &#34;&#34;&#34;Create a archive file for this dataset.  This will be archived as:

       /path/to/tarfile.{tar.gz|.tgz|.bz2}
          tarfilename
             tarfilename.{json|pkl}
             mediadir/
                 video.mp4
             extras1.ext
             extras2.ext
    
        Args:
            tarfile: /path/to/tarfilename.tar.gz
            delprefix:  the absolute file path contained in the media filenames to be removed.  If a video has a delprefix=&#39;/a/b&#39; then videos with path /a/b/c/d.mp4&#39; -&gt; &#39;c/d.mp4&#39;, and {JSON|PKL} will be saved with relative paths to mediadir.  This may be a list of delprefixes.
            mediadir:  the subdirectory name of the media to be contained in the archive.  Usually &#34;videos&#34;.             
            extrafiles: list of tuples or singletons [(abspath, filename_in_archive_relative_to_root), &#39;file_in_root_and_in_pwd&#39;, ...], 
            novideos [bool]:  generate a tarball without linking videos, just annotations
            md5 [bool]:  If True, generate the MD5 hash of the tarball using the system &#34;md5sum&#34;, or if md5=&#39;vipy&#39; use a slower python only md5 hash 
            castas [class]:  This should be a vipy class that the vipy objects should be cast to prior to archive.  This is useful for converting priveledged superclasses to a base class prior to export.
            tmpdir:  The path to the temporary directory for construting this dataset.  Defaults to system temp.  This directory will be emptied prior to archive.
            inplace [bool]:  If true, modify the dataset in place to prepare it for archive, else make a copy
            bycategory [bool]: If true, save the annotations in an annotations/ directory by category
            annotationdir [str]: The subdirectory name of annotations to be contained in the archive if bycategory=True.  Usually &#34;annotations&#34; or &#34;json&#34;.

        Example:  

          - Input files contain /path/to/oldvideos/category/video.mp4
          - Output will contain relative paths videos/category/video.mp4

    ```python
    d.archive(&#39;out.tar.gz&#39;, delprefix=&#39;/path/to/oldvideos&#39;, mediadir=&#39;videos&#39;)
    ```
    
        Returns:

            The absolute path to the tarball 
    &#34;&#34;&#34;
    assert self._isvipy(), &#34;Source dataset must contain vipy objects for staging&#34;
    assert all([os.path.isabs(v.filename()) for v in self]), &#34;Input dataset must have only absolute media paths&#34;
    assert len([v for v in self if any([d in v.filename() for d in tolist(delprefix)])]) == len(self), &#34;all media objects must have a provided delprefix for relative path construction&#34;
    assert vipy.util.istgz(tarfile) or vipy.util.isbz2(tarfile) or vipy.util.istar(tarfile), &#34;Allowable extensions are .tar.gz, .tgz, .bz2 or .tar&#34;
    assert shutil.which(&#39;tar&#39;) is not None, &#34;tar not found on path&#34;        
    
    D = self.clone() if not inplace else self   # large memory footprint if inplace=False
    tmpdir = tempdir() if tmpdir is None else remkdir(tmpdir, flush=True)
    stagedir = remkdir(os.path.join(tmpdir, filefull(filetail(tarfile))))
    print(&#39;[vipy.dataset]: creating staging directory &#34;%s&#34;&#39; % stagedir)
    delprefix = [[d for d in tolist(delprefix) if d in v.filename()][0] for v in self]  # select the delprefix per video
    D._objlist = [v.filename(v.filename().replace(os.path.normpath(p), os.path.normpath(os.path.join(stagedir, mediadir))), symlink=not novideos) for (p,v) in zip(delprefix, D.list())]

    # Save annotations:  Split large datasets into annotations grouped by category to help speed up loading         
    if bycategory:
        for (c,V) in vipy.util.groupbyasdict(list(D), lambda v: v.category()).items():
            Dataset(V, id=c).save(os.path.join(stagedir, annotationdir, &#39;%s.%s&#39; % (c, format)), relpath=True, nourl=True, sanitize=True, castas=castas, significant_digits=2, noemail=True, flush=True)
    else:
        pklfile = os.path.join(stagedir, &#39;%s.%s&#39; % (filetail(filefull(tarfile)), format))
        D.save(pklfile, relpath=True, nourl=True, sanitize=True, castas=castas, significant_digits=2, noemail=True, flush=True)

    # Copy extras (symlinked) to staging directory
    if extrafiles is not None:
        # extrafiles = [(&#34;/abs/path/in/filesystem.ext&#34;, &#34;rel/path/in/archive.ext&#34;), ... ]
        assert all([((isinstance(e, tuple) or isinstance(e, list)) and len(e) == 2) or isinstance(e, str) for e in extrafiles])
        extrafiles = [e if (isinstance(e, tuple) or isinstance(e, list)) else (e,e) for e in extrafiles]  # tuple-ify files in pwd() and should be put in the tarball root
        for (e, a) in tolist(extrafiles):
            assert os.path.exists(os.path.abspath(e)), &#34;Invalid extras file &#39;%s&#39; - file not found&#34; % e
            remkdir(filepath(os.path.join(stagedir, filetail(e) if a is None else a)))    # make directory in stagedir for symlink
            os.symlink(os.path.abspath(e), os.path.join(stagedir, filetail(e) if a is None else a))

    # System command to run tar
    cmd = (&#39;tar %scvf %s -C %s --dereference %s %s&#39; % (&#39;j&#39; if vipy.util.isbz2(tarfile) else (&#39;z&#39; if vipy.util.istgz(tarfile) else &#39;&#39;), 
                                                       tarfile,
                                                       filepath(stagedir),
                                                       filetail(stagedir),
                                                       &#39; &gt; /dev/null&#39; if not verbose else &#39;&#39;))

    print(&#39;[vipy.dataset]: executing &#34;%s&#34;&#39; % cmd)        
    os.system(cmd)  # too slow to use python &#34;tarfile&#34; package
    print(&#39;[vipy.dataset]: deleting staging directory &#34;%s&#34;&#39; % stagedir)        
    shutil.rmtree(stagedir)

    if md5:
        if shutil.which(&#39;md5sum&#39;) is not None:
            cmd = &#39;md5sum %s&#39; % tarfile
            print(&#39;[vipy.dataset]: executing &#34;%s&#34;&#39; % cmd)        
            os.system(cmd)  # too slow to use python &#34;vipy.downloader.generate_md5(tarball)&#34; for huge datasets
        else:
            print(&#39;[vipy.dataset]: %s, MD5=%s&#39; % (tarfile, vipy.downloader.generate_md5(tarfile)))  # too slow for large datasets, but does not require md5sum on path
    return tarfile</code></pre>
</details>
</dd>
<dt id="vipy.dataset.Dataset.augment"><code class="name flex">
<span>def <span class="ident">augment</span></span>(<span>self, f, n_augmentations)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/visym/vipy/blob/2ff555b8a23b3e6299d0a41de1f179f6005da60c/vipy/dataset.py#L426-L429" class="git-link">Browse git</a>
</summary>
<pre><code class="python">def augment(self, f, n_augmentations):
    assert n_augmentations &gt;= 1
    self._objlist = [f(v.clone()) for v in self for k in range(n_augmentations)]  # This will remove the originals
    return self</code></pre>
</details>
</dd>
<dt id="vipy.dataset.Dataset.boxsize"><code class="name flex">
<span>def <span class="ident">boxsize</span></span>(<span>self, outfile=None, category_to_color=None, categories=None)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/visym/vipy/blob/2ff555b8a23b3e6299d0a41de1f179f6005da60c/vipy/dataset.py#L766-L792" class="git-link">Browse git</a>
</summary>
<pre><code class="python">def boxsize(self, outfile=None, category_to_color=None, categories=None):
    # Scatterplot of object box sizes
    tracks = [t for s in self.list() for t in s.tracks().values()]        
    (x, y) = zip(*[(t.meanshape()[1], t.meanshape()[0]) for t in tracks])
    object_categories = set([t.category() for t in tracks]) if categories is None else categories

    d = {}        
    for c in object_categories:
        xcyc = [(t.meanshape()[1], t.meanshape()[0]) for t in tracks if ((t.category().lower() == c.lower()) and (t.meanshape() is not None))]
        d[c] = xcyc
    
    if outfile is not None:            
        plt.clf()
        plt.figure()
        plt.grid(True)
        for c in object_categories:
            xcyc = d[c]
            if len(xcyc) &gt; 0:
                (xc, yc) = zip(*xcyc)
                plt.scatter(xc, yc, c=category_to_color[c] if category_to_color is not None else &#39;blue&#39;, label=c)
        plt.xlabel(&#39;bounding box (width)&#39;)
        plt.ylabel(&#39;bounding box (height)&#39;)
        plt.axis([0, 1000, 0, 1000])                
        plt.legend()
        plt.gca().set_axisbelow(True)        
        plt.savefig(outfile)
    return d</code></pre>
</details>
</dd>
<dt id="vipy.dataset.Dataset.boxsize_by_category"><code class="name flex">
<span>def <span class="ident">boxsize_by_category</span></span>(<span>self, outfile=None)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/visym/vipy/blob/2ff555b8a23b3e6299d0a41de1f179f6005da60c/vipy/dataset.py#L794-L818" class="git-link">Browse git</a>
</summary>
<pre><code class="python">def boxsize_by_category(self, outfile=None):
    # Scatterplot of object box sizes
    tracks = [t for s in self.list() for t in s.tracks().values()]        
    (x, y) = zip(*[(t.meanshape()[1], t.meanshape()[0]) for t in tracks])
    object_categories = set([t.category() for t in tracks])
    
    # Mean track size per video category
    d_category_to_xy = {k:np.mean([t.meanshape() for v in vlist for t in v.tracklist()], axis=0) for (k,vlist) in groupbyasdict(self.list(), lambda v: v.category()).items()}

    if outfile is not None:
        plt.clf()
        plt.figure()
        plt.grid(True)
        colors = colorlist()            
        d_category_to_color = {c:colors[k % len(colors)] for (k,c) in enumerate(d_category_to_xy.keys())}
        for c in d_category_to_xy.keys():
            (xc, yc) = d_category_to_xy[c]
            plt.scatter(xc, yc, c=d_category_to_color[c], label=c)
        plt.xlabel(&#39;bounding box (width)&#39;)
        plt.ylabel(&#39;bounding box (height)&#39;)
        plt.axis([0, 600, 0, 600])                
        plt.gca().set_axisbelow(True)        
        lgd = plt.legend(bbox_to_anchor=(1.05, 1), loc=&#39;upper left&#39;, borderaxespad=0.)
        plt.savefig(outfile, bbox_extra_artists=(lgd,), bbox_inches=&#39;tight&#39;)
    return d_category_to_xy</code></pre>
</details>
</dd>
<dt id="vipy.dataset.Dataset.boxsize_histogram"><code class="name flex">
<span>def <span class="ident">boxsize_histogram</span></span>(<span>self, outfile=None)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/visym/vipy/blob/2ff555b8a23b3e6299d0a41de1f179f6005da60c/vipy/dataset.py#L820-L843" class="git-link">Browse git</a>
</summary>
<pre><code class="python">def boxsize_histogram(self, outfile=None):
    # Scatterplot of object box sizes
    tracks = [t for s in self.list() for t in s.tracks().values()]        
    (x, y) = zip(*[(t.meanshape()[1], t.meanshape()[0]) for t in tracks])
    object_categories = set([t.category() for t in tracks])

    
    # 2D histogram of object box sizes
    for c in object_categories:
        xcyc = [(t.meanshape()[1], t.meanshape()[0]) for t in tracks if ((t.category() == c) and (t.meanshape() is not None))]
        d[c] = xcyc

    if outfile is not None:
        for c in object_categories:            
            xcyc = d[c]
            if len(xcyc) &gt; 0:
                (xc, yc) = zip(*xcyc)
                plt.clf()
                plt.figure()
                plt.hist2d(xc, yc, bins=10)
                plt.xlabel(&#39;Bounding box (width)&#39;)
                plt.ylabel(&#39;Bounding box (height)&#39;)                    
                plt.savefig(outfile % c)
    return d</code></pre>
</details>
</dd>
<dt id="vipy.dataset.Dataset.categories"><code class="name flex">
<span>def <span class="ident">categories</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Alias for classlist</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/visym/vipy/blob/2ff555b8a23b3e6299d0a41de1f179f6005da60c/vipy/dataset.py#L340-L342" class="git-link">Browse git</a>
</summary>
<pre><code class="python">def categories(self):
    &#34;&#34;&#34;Alias for classlist&#34;&#34;&#34;
    return self.classlist()</code></pre>
</details>
</dd>
<dt id="vipy.dataset.Dataset.chunk"><code class="name flex">
<span>def <span class="ident">chunk</span></span>(<span>self, n)</span>
</code></dt>
<dd>
<div class="desc"><p>Yield n chunks of this dataset.
Last chunk will be ragged</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/visym/vipy/blob/2ff555b8a23b3e6299d0a41de1f179f6005da60c/vipy/dataset.py#L533-L536" class="git-link">Browse git</a>
</summary>
<pre><code class="python">def chunk(self, n):
    &#34;&#34;&#34;Yield n chunks of this dataset.  Last chunk will be ragged&#34;&#34;&#34;
    for (k,V) in enumerate(vipy.util.chunklist(self._objlist, n)):
        yield Dataset(V, id=&#39;%s_%d&#39; % (self.id(), k), loader=self._loader)</code></pre>
</details>
</dd>
<dt id="vipy.dataset.Dataset.class_to_index"><code class="name flex">
<span>def <span class="ident">class_to_index</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Return a dictionary mapping the unique classes to an integer index.
This is useful for defining a softmax index ordering for categorization</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/visym/vipy/blob/2ff555b8a23b3e6299d0a41de1f179f6005da60c/vipy/dataset.py#L354-L356" class="git-link">Browse git</a>
</summary>
<pre><code class="python">def class_to_index(self):
    &#34;&#34;&#34;Return a dictionary mapping the unique classes to an integer index.  This is useful for defining a softmax index ordering for categorization&#34;&#34;&#34;
    return {v:k for (k,v) in enumerate(self.classlist())}</code></pre>
</details>
</dd>
<dt id="vipy.dataset.Dataset.classes"><code class="name flex">
<span>def <span class="ident">classes</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Alias for classlist</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/visym/vipy/blob/2ff555b8a23b3e6299d0a41de1f179f6005da60c/vipy/dataset.py#L337-L339" class="git-link">Browse git</a>
</summary>
<pre><code class="python">def classes(self):
    &#34;&#34;&#34;Alias for classlist&#34;&#34;&#34;
    return self.classlist()</code></pre>
</details>
</dd>
<dt id="vipy.dataset.Dataset.classlist"><code class="name flex">
<span>def <span class="ident">classlist</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Return a sorted list of categories in the dataset</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/visym/vipy/blob/2ff555b8a23b3e6299d0a41de1f179f6005da60c/vipy/dataset.py#L332-L335" class="git-link">Browse git</a>
</summary>
<pre><code class="python">def classlist(self):
    &#34;&#34;&#34;Return a sorted list of categories in the dataset&#34;&#34;&#34;
    assert self._isvipy(), &#34;Invalid input&#34;
    return sorted(list(set([v.category() for v in self])))</code></pre>
</details>
</dd>
<dt id="vipy.dataset.Dataset.clone"><code class="name flex">
<span>def <span class="ident">clone</span></span>(<span>self, shallow=False)</span>
</code></dt>
<dd>
<div class="desc"><p>Return a deep copy of the dataset</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/visym/vipy/blob/2ff555b8a23b3e6299d0a41de1f179f6005da60c/vipy/dataset.py#L172-L181" class="git-link">Browse git</a>
</summary>
<pre><code class="python">def clone(self, shallow=False):
    &#34;&#34;&#34;Return a deep copy of the dataset&#34;&#34;&#34;
    if shallow:
        objlist = self._objlist
        self._objlist = []  
        D = copy.deepcopy(self)
        self._objlist = objlist  # restore
        return D
    else:
        return copy.deepcopy(self)</code></pre>
</details>
</dd>
<dt id="vipy.dataset.Dataset.count"><code class="name flex">
<span>def <span class="ident">count</span></span>(<span>self, f=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Counts for each label.
</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>f</code></strong></dt>
<dd>[lambda] if provided, count the number of elements that return true.
This is the same as len(self.filter(f)) without modifying the dataset.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>A dictionary of counts per category [if f is None]
A length of elements that satisfy f(v) = True [if f is not None]</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/visym/vipy/blob/2ff555b8a23b3e6299d0a41de1f179f6005da60c/vipy/dataset.py#L635-L647" class="git-link">Browse git</a>
</summary>
<pre><code class="python">def count(self, f=None):
    &#34;&#34;&#34;Counts for each label.  
    
    Args:
        f: [lambda] if provided, count the number of elements that return true.  This is the same as len(self.filter(f)) without modifying the dataset.

    Returns:
        A dictionary of counts per category [if f is None]
        A length of elements that satisfy f(v) = True [if f is not None]
    &#34;&#34;&#34;
    assert self._isvipy()
    assert f is None or callable(f)
    return len([v for v in self if f is None or f(v)])</code></pre>
</details>
</dd>
<dt id="vipy.dataset.Dataset.countby"><code class="name flex">
<span>def <span class="ident">countby</span></span>(<span>self, f=&lt;function Dataset.&lt;lambda&gt;&gt;)</span>
</code></dt>
<dd>
<div class="desc"><p>Count the number of elements that return the same value from the lambda function</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/visym/vipy/blob/2ff555b8a23b3e6299d0a41de1f179f6005da60c/vipy/dataset.py#L649-L653" class="git-link">Browse git</a>
</summary>
<pre><code class="python">def countby(self, f=lambda v: v.category()):
    &#34;&#34;&#34;Count the number of elements that return the same value from the lambda function&#34;&#34;&#34;
    assert self._isvipy()
    assert f is None or callable(f)
    return vipy.util.countby(self, f)</code></pre>
</details>
</dd>
<dt id="vipy.dataset.Dataset.dedupe"><code class="name flex">
<span>def <span class="ident">dedupe</span></span>(<span>self, key)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/visym/vipy/blob/2ff555b8a23b3e6299d0a41de1f179f6005da60c/vipy/dataset.py#L373-L375" class="git-link">Browse git</a>
</summary>
<pre><code class="python">def dedupe(self, key):
    self._objlist = list({key(v):v for v in self}.values())
    return self</code></pre>
</details>
</dd>
<dt id="vipy.dataset.Dataset.density"><code class="name flex">
<span>def <span class="ident">density</span></span>(<span>self, outfile=None, max=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Compute the frequency that each video ID is represented.
This counts how many activities are in a video, truncated at max</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/visym/vipy/blob/2ff555b8a23b3e6299d0a41de1f179f6005da60c/vipy/dataset.py#L757-L764" class="git-link">Browse git</a>
</summary>
<pre><code class="python">def density(self, outfile=None, max=None):
    &#34;&#34;&#34;Compute the frequency that each video ID is represented.  This counts how many activities are in a video, truncated at max&#34;&#34;&#34;
    assert self._isvipy()
    d = [len(v) if (max is None or len(v)&lt;= max) else max for (k,v) in groupbyasdict(self.list(), lambda v: v.videoid()).items()]
    d = {k:v for (k,v) in sorted(vipy.util.countby(d, lambda x: x).items(), key=lambda x: x[1], reverse=True)}
    if outfile is not None:
        vipy.metrics.histogram(d.values(), d.keys(), outfile=outfile, ylabel=&#39;Frequency&#39;, xlabel=&#39;Activities per video&#39;, fontsize=6, xrot=None)            
    return d</code></pre>
</details>
</dd>
<dt id="vipy.dataset.Dataset.difference"><code class="name flex">
<span>def <span class="ident">difference</span></span>(<span>self, other, key)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/visym/vipy/blob/2ff555b8a23b3e6299d0a41de1f179f6005da60c/vipy/dataset.py#L394-L398" class="git-link">Browse git</a>
</summary>
<pre><code class="python">def difference(self, other, key):
    assert isinstance(other, Dataset), &#34;invalid input&#34;
    idset = set([key(v) for v in self]).difference([key(v) for v in other])   # in A but not in B
    self._objlist = [v for v in self if key(v) in idset]
    return self</code></pre>
</details>
</dd>
<dt id="vipy.dataset.Dataset.duration_in_frames"><code class="name flex">
<span>def <span class="ident">duration_in_frames</span></span>(<span>self, outfile=None)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/visym/vipy/blob/2ff555b8a23b3e6299d0a41de1f179f6005da60c/vipy/dataset.py#L724-L729" class="git-link">Browse git</a>
</summary>
<pre><code class="python">def duration_in_frames(self, outfile=None):
    assert self._isvipy()
    d = {k:np.mean([v[1] for v in v]) for (k,v) in groupbyasdict([(a.category(), len(a)) for v in self.list() for a in v.activitylist()], lambda x: x[0]).items()}
    if outfile is not None:
        vipy.metrics.histogram(d.values(), d.keys(), outfile=outfile, ylabel=&#39;Duration (frames)&#39;, fontsize=6)            
    return d</code></pre>
</details>
</dd>
<dt id="vipy.dataset.Dataset.duration_in_seconds"><code class="name flex">
<span>def <span class="ident">duration_in_seconds</span></span>(<span>self, outfile=None, fontsize=6, max_duration=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Duration of activities</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/visym/vipy/blob/2ff555b8a23b3e6299d0a41de1f179f6005da60c/vipy/dataset.py#L731-L738" class="git-link">Browse git</a>
</summary>
<pre><code class="python">def duration_in_seconds(self, outfile=None, fontsize=6, max_duration=None):
    &#34;&#34;&#34;Duration of activities&#34;&#34;&#34;
    assert self._isvipy()
    d = {k:np.mean([v[1] for v in v]) for (k,v) in groupbyasdict([(a.category(), len(a)/v.framerate()) for v in self.list() for a in v.activitylist()], lambda x: x[0]).items()}
    if outfile is not None:
        max_duration = max(d.values()) if max_duration is None else max_duration
        vipy.metrics.histogram([min(x, max_duration) for x in d.values()], d.keys(), outfile=outfile, ylabel=&#39;Duration (seconds)&#39;, fontsize=fontsize)            
    return d</code></pre>
</details>
</dd>
<dt id="vipy.dataset.Dataset.filter"><code class="name flex">
<span>def <span class="ident">filter</span></span>(<span>self, f)</span>
</code></dt>
<dd>
<div class="desc"><p>In place filter with lambda function f</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/visym/vipy/blob/2ff555b8a23b3e6299d0a41de1f179f6005da60c/vipy/dataset.py#L431-L434" class="git-link">Browse git</a>
</summary>
<pre><code class="python">def filter(self, f):
    &#34;&#34;&#34;In place filter with lambda function f&#34;&#34;&#34;
    self._objlist = [v for v in self if f(v)]
    return self</code></pre>
</details>
</dd>
<dt id="vipy.dataset.Dataset.flatmap"><code class="name flex">
<span>def <span class="ident">flatmap</span></span>(<span>self, f)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/visym/vipy/blob/2ff555b8a23b3e6299d0a41de1f179f6005da60c/vipy/dataset.py#L631-L633" class="git-link">Browse git</a>
</summary>
<pre><code class="python">def flatmap(self, f):
    self._objlist = [x for v in self for x in f(v)]
    return self</code></pre>
</details>
</dd>
<dt id="vipy.dataset.Dataset.flatten"><code class="name flex">
<span>def <span class="ident">flatten</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Convert dataset stored as a list of lists into a flat list</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/visym/vipy/blob/2ff555b8a23b3e6299d0a41de1f179f6005da60c/vipy/dataset.py#L147-L150" class="git-link">Browse git</a>
</summary>
<pre><code class="python">def flatten(self):
    &#34;&#34;&#34;Convert dataset stored as a list of lists into a flat list&#34;&#34;&#34;
    self._objlist = [o for objlist in self._objlist for o in vipy.util.tolist(objlist)]
    return self</code></pre>
</details>
</dd>
<dt id="vipy.dataset.Dataset.framerate"><code class="name flex">
<span>def <span class="ident">framerate</span></span>(<span>self, outfile=None)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/visym/vipy/blob/2ff555b8a23b3e6299d0a41de1f179f6005da60c/vipy/dataset.py#L749-L754" class="git-link">Browse git</a>
</summary>
<pre><code class="python">def framerate(self, outfile=None):
    assert self._isvipy()
    d = vipy.util.countby([int(round(v.framerate())) for v in self.list()], lambda x: x)
    if outfile is not None:
        vipy.metrics.pie(d.values(), [&#39;%d fps&#39; % k for k in d.keys()], explode=None, outfile=outfile,  shadow=False)
    return d</code></pre>
</details>
</dd>
<dt id="vipy.dataset.Dataset.frequency"><code class="name flex">
<span>def <span class="ident">frequency</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/visym/vipy/blob/2ff555b8a23b3e6299d0a41de1f179f6005da60c/vipy/dataset.py#L655-L656" class="git-link">Browse git</a>
</summary>
<pre><code class="python">def frequency(self):
    return self.count()</code></pre>
</details>
</dd>
<dt id="vipy.dataset.Dataset.has"><code class="name flex">
<span>def <span class="ident">has</span></span>(<span>self, val, key)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/visym/vipy/blob/2ff555b8a23b3e6299d0a41de1f179f6005da60c/vipy/dataset.py#L400-L401" class="git-link">Browse git</a>
</summary>
<pre><code class="python">def has(self, val, key):
    return any([key(obj) == val for obj in self])</code></pre>
</details>
</dd>
<dt id="vipy.dataset.Dataset.histogram"><code class="name flex">
<span>def <span class="ident">histogram</span></span>(<span>self, outfile=None, fontsize=6, category_to_barcolor=None, category_to_xlabel=None)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/visym/vipy/blob/2ff555b8a23b3e6299d0a41de1f179f6005da60c/vipy/dataset.py#L669-L681" class="git-link">Browse git</a>
</summary>
<pre><code class="python">def histogram(self, outfile=None, fontsize=6, category_to_barcolor=None, category_to_xlabel=None):
    assert self._isvipy()
    assert category_to_barcolor is None or all([c in category_to_barcolor for c in self.categories()])
    assert category_to_xlabel is None or callable(category_to_xlabel) or all([c in category_to_xlabel for c in self.categories()])
    f_category_to_xlabel = category_to_xlabel if callable(category_to_xlabel) else ((lambda c: category_to_xlabel[c]) if category_to_xlabel is not None else (lambda c: c))
    
    d = self.countby(lambda v: v.category())
    if outfile is not None:
        (categories, freq) = zip(*reversed(sorted(list(d.items()), key=lambda x: x[1])))  # decreasing frequency
        barcolors = [&#39;blue&#39; if category_to_barcolor is None else category_to_barcolor[c] for c in categories]
        xlabels = [f_category_to_xlabel(c) for c in categories]
        print(&#39;[vipy.dataset]: histogram=&#34;%s&#34;&#39; % vipy.metrics.histogram(freq, xlabels, barcolors=barcolors, outfile=outfile, ylabel=&#39;Instances&#39;, fontsize=fontsize))
    return d</code></pre>
</details>
</dd>
<dt id="vipy.dataset.Dataset.id"><code class="name flex">
<span>def <span class="ident">id</span></span>(<span>self, n=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Set or return the dataset id</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/visym/vipy/blob/2ff555b8a23b3e6299d0a41de1f179f6005da60c/vipy/dataset.py#L132-L138" class="git-link">Browse git</a>
</summary>
<pre><code class="python">def id(self, n=None):
    &#34;&#34;&#34;Set or return the dataset id&#34;&#34;&#34;
    if n is None:
        return self._id
    else:
        self._id = n
        return self</code></pre>
</details>
</dd>
<dt id="vipy.dataset.Dataset.index_to_class"><code class="name flex">
<span>def <span class="ident">index_to_class</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Return a dictionary mapping an integer index to the unique class names.
This is the inverse of class_to_index, swapping keys and values</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/visym/vipy/blob/2ff555b8a23b3e6299d0a41de1f179f6005da60c/vipy/dataset.py#L358-L360" class="git-link">Browse git</a>
</summary>
<pre><code class="python">def index_to_class(self):
    &#34;&#34;&#34;Return a dictionary mapping an integer index to the unique class names.  This is the inverse of class_to_index, swapping keys and values&#34;&#34;&#34;
    return {v:k for (k,v) in self.class_to_index().items()}</code></pre>
</details>
</dd>
<dt id="vipy.dataset.Dataset.inverse_frequency_weight"><code class="name flex">
<span>def <span class="ident">inverse_frequency_weight</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Return inverse frequency weight for categories in dataset.
Useful for unbalanced class weighting during training</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/visym/vipy/blob/2ff555b8a23b3e6299d0a41de1f179f6005da60c/vipy/dataset.py#L718-L722" class="git-link">Browse git</a>
</summary>
<pre><code class="python">def inverse_frequency_weight(self):
    &#34;&#34;&#34;Return inverse frequency weight for categories in dataset.  Useful for unbalanced class weighting during training&#34;&#34;&#34;
    d = {k:1.0/max(v,1) for (k,v) in self.count().items()}
    n = sum(d.values())
    return {k:len(d)*(v/float(n)) for (k,v) in d.items()}</code></pre>
</details>
</dd>
<dt id="vipy.dataset.Dataset.istype"><code class="name flex">
<span>def <span class="ident">istype</span></span>(<span>self, validtype)</span>
</code></dt>
<dd>
<div class="desc"><p>Return True if the all elements (or just the first element if strict=False) in the dataset are of type 'validtype'</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/visym/vipy/blob/2ff555b8a23b3e6299d0a41de1f179f6005da60c/vipy/dataset.py#L152-L154" class="git-link">Browse git</a>
</summary>
<pre><code class="python">def istype(self, validtype):
    &#34;&#34;&#34;Return True if the all elements (or just the first element if strict=False) in the dataset are of type &#39;validtype&#39;&#34;&#34;&#34;
    return all([any([isinstance(v,t) for t in tolist(validtype)]) for v in self]) if self._istype_strict else any([isinstance(self[0],t) for t in tolist(validtype)])</code></pre>
</details>
</dd>
<dt id="vipy.dataset.Dataset.json"><code class="name flex">
<span>def <span class="ident">json</span></span>(<span>self, encode=True)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/visym/vipy/blob/2ff555b8a23b3e6299d0a41de1f179f6005da60c/vipy/dataset.py#L118-L122" class="git-link">Browse git</a>
</summary>
<pre><code class="python">def json(self, encode=True):
    r = vipy.util.class_registry()
    d = {k:v for (k,v) in self.__dict__.items() if not k == &#39;_loader&#39;}
    d[&#39;_objlist&#39;] = [(str(type(v)), v.json(encode=False)) if str(type(v)) in r else v for v in self._objlist]
    return json.dumps(d) if encode else d</code></pre>
</details>
</dd>
<dt id="vipy.dataset.Dataset.jsondir"><code class="name flex">
<span>def <span class="ident">jsondir</span></span>(<span>self, outdir=None, verbose=True, rekey=False, bycategory=False, byfilename=False, abspath=True)</span>
</code></dt>
<dd>
<div class="desc"><p>Export all objects to a directory of JSON files.</p>
<p>Usage:</p>
<pre><code class="language-python">D = vipy.dataset.Dataset(...).jsondir('/path/to/jsondir')
D = vipy.util.load('/path/to/jsondir')   # recursively discover and lazy load all json files 
</code></pre>
<p>Args:</p>
<pre><code>   outdir [str]:  The root directory to store the JSON files
   verbose [bool]: If True, print the save progress
   rekey [bool] If False, use the instance ID of the vipy object as the filename for the JSON file, otherwise assign a new UUID_dataset-index
   bycategory [bool]: If True, use the JSON structure '$OUTDIR/$CATEGORY/$INSTANCEID.json'
   byfilename [bool]: If True, use the JSON structure '$FILENAME.json' where $FILENAME is the underlying media filename of the vipy object
   abspath [bool]: If true, store absolute paths to media in JSON.  If false, store relative paths to media from JSON directory
</code></pre>
<p>Returns:
outdir: The directory containing the JSON files.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/visym/vipy/blob/2ff555b8a23b3e6299d0a41de1f179f6005da60c/vipy/dataset.py#L454-L493" class="git-link">Browse git</a>
</summary>
<pre><code class="python">def jsondir(self, outdir=None, verbose=True, rekey=False, bycategory=False, byfilename=False, abspath=True):
    &#34;&#34;&#34;Export all objects to a directory of JSON files.

       Usage:

    ```python
    D = vipy.dataset.Dataset(...).jsondir(&#39;/path/to/jsondir&#39;)
    D = vipy.util.load(&#39;/path/to/jsondir&#39;)   # recursively discover and lazy load all json files 
    ```

       Args:

           outdir [str]:  The root directory to store the JSON files
           verbose [bool]: If True, print the save progress
           rekey [bool] If False, use the instance ID of the vipy object as the filename for the JSON file, otherwise assign a new UUID_dataset-index
           bycategory [bool]: If True, use the JSON structure &#39;$OUTDIR/$CATEGORY/$INSTANCEID.json&#39;
           byfilename [bool]: If True, use the JSON structure &#39;$FILENAME.json&#39; where $FILENAME is the underlying media filename of the vipy object
           abspath [bool]: If true, store absolute paths to media in JSON.  If false, store relative paths to media from JSON directory

       Returns:
           outdir: The directory containing the JSON files.
    &#34;&#34;&#34;
    assert self._isvipy()
    assert outdir is not None or byfilename 
    assert not byfilename and bycategory

    if outdir is not None:
        vipy.util.remkdir(outdir) 
    if bycategory:
        tojsonfile = lambda v,k: os.path.join(outdir, v.category(), (&#39;%s.json&#39; % v.instanceid()) if not rekey else (&#39;%s_%d.json&#39; % (uuid.uuid4().hex, k)))
    elif byfilename:
        tojsonfile = lambda v,k: vipy.util.toextension(v.filename(), &#39;.json&#39;)
    else:
        tojsonfile = lambda v,k: os.path.join(outdir, (&#39;%s.json&#39; % v.instanceid()) if not rekey else &#39;%s_%d.json&#39; % (uuid.uuid4().hex, k))
    
    for (k,v) in enumerate(self):            
        f = vipy.util.save(v.clone().relpath(start=filepath(tojsonfile(v,k))) if not abspath else v.clone().abspath(), tojsonfile(v,k))
        if verbose:
            print(&#39;[vipy.dataset.Dataset][%d/%d]: %s&#39; % (k, len(self), f))
    return outdir</code></pre>
</details>
</dd>
<dt id="vipy.dataset.Dataset.label_to_index"><code class="name flex">
<span>def <span class="ident">label_to_index</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Alias for class_to_index</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/visym/vipy/blob/2ff555b8a23b3e6299d0a41de1f179f6005da60c/vipy/dataset.py#L362-L364" class="git-link">Browse git</a>
</summary>
<pre><code class="python">def label_to_index(self):
    &#34;&#34;&#34;Alias for class_to_index&#34;&#34;&#34;
    return self.class_to_index()</code></pre>
</details>
</dd>
<dt id="vipy.dataset.Dataset.list"><code class="name flex">
<span>def <span class="ident">list</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Return the dataset as a list</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/visym/vipy/blob/2ff555b8a23b3e6299d0a41de1f179f6005da60c/vipy/dataset.py#L140-L142" class="git-link">Browse git</a>
</summary>
<pre><code class="python">def list(self):
    &#34;&#34;&#34;Return the dataset as a list&#34;&#34;&#34;
    return list(self)</code></pre>
</details>
</dd>
<dt id="vipy.dataset.Dataset.load"><code class="name flex">
<span>def <span class="ident">load</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Load the entire dataset into memory.
This is useful for creating in-memory datasets from lazy load datasets</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/visym/vipy/blob/2ff555b8a23b3e6299d0a41de1f179f6005da60c/vipy/dataset.py#L510-L514" class="git-link">Browse git</a>
</summary>
<pre><code class="python">def load(self):
    &#34;&#34;&#34;Load the entire dataset into memory.  This is useful for creating in-memory datasets from lazy load datasets&#34;&#34;&#34;
    self._objlist = self.list()
    self._loader = None
    return self</code></pre>
</details>
</dd>
<dt id="vipy.dataset.Dataset.localmap"><code class="name flex">
<span>def <span class="ident">localmap</span></span>(<span>self, f)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/visym/vipy/blob/2ff555b8a23b3e6299d0a41de1f179f6005da60c/vipy/dataset.py#L626-L629" class="git-link">Browse git</a>
</summary>
<pre><code class="python">def localmap(self, f):
    for (k,v) in enumerate(self):
        self._objlist[k] = f(v)  # in-place update
    return self</code></pre>
</details>
</dd>
<dt id="vipy.dataset.Dataset.map"><code class="name flex">
<span>def <span class="ident">map</span></span>(<span>self, f_map, model=None, dst=None, id=None, strict=False, ascompleted=True, chunks=128, ordered=False)</span>
</code></dt>
<dd>
<div class="desc"><p>Distributed map.</p>
<p>To perform this in parallel across four processes:</p>
<pre><code class="language-python">D = vipy.dataset.Dataset(...)
with vipy.globals.parallel(4):
    D.map(lambda v: ...)
</code></pre>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>f_map</code></strong></dt>
<dd>[lambda] The lambda function to apply in parallel to all elements in the dataset.
This must return a JSON serializable object</dd>
<dt><strong><code>model</code></strong></dt>
<dd>[torch.nn.Module] The model to scatter to all workers</dd>
<dt><strong><code>dst</code></strong></dt>
<dd>[str] The ID to give to the resulting dataset</dd>
<dt><strong><code>id</code></strong></dt>
<dd>[str] The ID to give to the resulting dataset (parameter alias for dst)</dd>
<dt><strong><code>strict</code></strong></dt>
<dd>[bool] If true, raise exception on map failures, otherwise the map will return None for failed elements</dd>
<dt><strong><code>ascompleted</code></strong></dt>
<dd>[bool] If true, return elements as they complete</dd>
<dt><strong><code>ordered</code></strong></dt>
<dd>[bool] If true, preserve the order of objects in dataset as returned from distributed processing</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>A <code><a title="vipy.dataset.Dataset" href="#vipy.dataset.Dataset">Dataset</a></code> containing the elements f_map(v).
This operation is order preserving if ordered=True.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<ul>
<li>This dataset must contain vipy objects of types defined in <code><a title="vipy.util.class_registry" href="util.html#vipy.util.class_registry">class_registry()</a></code> or JSON serializable objects</li>
<li>Serialization of large datasets can take a while, kick it off to a distributed dask scheduler and go get lunch</li>
<li>This method uses dask distributed and <code><a title="vipy.batch.Batch" href="batch.html#vipy.batch.Batch">Batch</a></code> operations</li>
<li>All vipy objects are JSON serialized prior to parallel map to avoid reference cycle garbage collection which can introduce instabilities</li>
<li>Due to chunking, all error handling is caught by this method.
Use <code><a title="vipy.batch.Batch" href="batch.html#vipy.batch.Batch">Batch</a></code> to leverage dask distributed futures error handling.</li>
<li>Operations must be chunked and serialized because each dask task comes with overhead, and lots of small tasks violates best practices</li>
<li>Serialized results are deserialized by the client and returned a a new dataset</li>
</ul>
</div></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/visym/vipy/blob/2ff555b8a23b3e6299d0a41de1f179f6005da60c/vipy/dataset.py#L569-L624" class="git-link">Browse git</a>
</summary>
<pre><code class="python">def map(self, f_map, model=None, dst=None, id=None, strict=False, ascompleted=True, chunks=128, ordered=False):        
    &#34;&#34;&#34;Distributed map.

    To perform this in parallel across four processes:

    ```python
    D = vipy.dataset.Dataset(...)
    with vipy.globals.parallel(4):
        D.map(lambda v: ...)
    ```

    Args:
        f_map: [lambda] The lambda function to apply in parallel to all elements in the dataset.  This must return a JSON serializable object
        model: [torch.nn.Module] The model to scatter to all workers
        dst: [str] The ID to give to the resulting dataset
        id: [str] The ID to give to the resulting dataset (parameter alias for dst)
        strict: [bool] If true, raise exception on map failures, otherwise the map will return None for failed elements
        ascompleted: [bool] If true, return elements as they complete
        ordered: [bool] If true, preserve the order of objects in dataset as returned from distributed processing

    Returns:
        A `vipy.dataset.Dataset` containing the elements f_map(v).  This operation is order preserving if ordered=True.

    .. note:: 
        - This dataset must contain vipy objects of types defined in `vipy.util.class_registry` or JSON serializable objects
        - Serialization of large datasets can take a while, kick it off to a distributed dask scheduler and go get lunch
        - This method uses dask distributed and `vipy.batch.Batch` operations
        - All vipy objects are JSON serialized prior to parallel map to avoid reference cycle garbage collection which can introduce instabilities
        - Due to chunking, all error handling is caught by this method.  Use `vipy.batch.Batch` to leverage dask distributed futures error handling.
        - Operations must be chunked and serialized because each dask task comes with overhead, and lots of small tasks violates best practices
        - Serialized results are deserialized by the client and returned a a new dataset
    &#34;&#34;&#34;
    assert callable(f_map)
    from vipy.batch import Batch   # requires pip install vipy[all]

    # Distributed map using vipy.batch
    f_serialize = lambda v,d=vipy.util.class_registry(): (str(type(v)), v.json()) if str(type(v)) in d else (None, pickle.dumps(v))  # fallback on PKL dumps/loads
    f_deserialize = lambda x,d=vipy.util.class_registry(): d[x[0]](x[1])  # with closure capture
    f_catcher = lambda f, *args, **kwargs: vipy.util.loudcatcher(f, &#39;[vipy.dataset.Dataset.map]: &#39;, *args, **kwargs)  # catch exceptions when executing lambda, print errors and return (True, result) or (False, exception)
    f_loader = self._loader if self._loader is not None else lambda x: x
    S = [f_serialize(v) for v in self._objlist]  # local serialization
    B = Batch(vipy.util.chunklist(S, chunks), strict=strict, as_completed=ascompleted, warnme=False, minscatter=chunks, ordered=ordered)
    if model is None:
        f = lambda x, f_loader=f_loader, f_serializer=f_serialize, f_deserializer=f_deserialize, f_map=f_map, f_catcher=f_catcher: f_serializer(f_catcher(f_map, f_loader(f_deserializer(x))))  # with closure capture
        S = B.map(lambda X,f=f: [f(x) for x in X]).result()  # chunked, with caught exceptions, may return empty list
    else:
        f = lambda net, x, f_loader=f_loader, f_serializer=f_serialize, f_deserializer=f_deserialize, f_map=f_map, f_catcher=f_catcher: f_serializer(f_catcher(f_map, net, f_loader(f_deserializer(x))))  # with closure capture
        S = B.scattermap((lambda net, X, f=f: [f(net, x) for x in X]), model).result()  # chunked, scattered, caught exceptions
    if not isinstance(S, list) or any([not isinstance(s, list) for s in S]):
        raise ValueError(&#39;Distributed processing error - Batch returned: %s&#39; % (str(S)))
    V = [f_deserialize(x) for s in S for x in s]  # Local deserialization and chunk flattening
    (good, bad) = ([r for (b,r) in V if b], [r for (b,r) in V if not b])  # catcher returns (True, result) or (False, exception string)
    if len(bad) &gt; 0:
        print(&#39;[vipy.dataset.Dataset.map]: Exceptions in map distributed processing:\n%s&#39; % str(bad))
        print(&#39;[vipy.dataset.Dataset.map]: %d/%d items failed&#39; % (len(bad), len(self)))
    return Dataset(good, id=dst if dst is not None else id)</code></pre>
</details>
</dd>
<dt id="vipy.dataset.Dataset.merge"><code class="name flex">
<span>def <span class="ident">merge</span></span>(<span>self, outdir)</span>
</code></dt>
<dd>
<div class="desc"><p>Merge a dataset union into a single subdirectory with symlinked media ready to be archived.</p>
<pre><code class="language-python">D1 = vipy.dataset.Dataset('/path1/dataset.json')
D2 = vipy.dataset.Dataset('/path2/dataset.json')
D3 = D1.union(D2).merge(outdir='/path3')
</code></pre>
<p>Media in D1 are in /path1, media in D2 are in /path2, media in D3 are all symlinked to /path3.
We can now create a tarball for D3 with all of the media files in the same relative path.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/visym/vipy/blob/2ff555b8a23b3e6299d0a41de1f179f6005da60c/vipy/dataset.py#L410-L424" class="git-link">Browse git</a>
</summary>
<pre><code class="python">def merge(self, outdir):
    &#34;&#34;&#34;Merge a dataset union into a single subdirectory with symlinked media ready to be archived.

    ```python
    D1 = vipy.dataset.Dataset(&#39;/path1/dataset.json&#39;)
    D2 = vipy.dataset.Dataset(&#39;/path2/dataset.json&#39;)
    D3 = D1.union(D2).merge(outdir=&#39;/path3&#39;)
    ```

    Media in D1 are in /path1, media in D2 are in /path2, media in D3 are all symlinked to /path3.
    We can now create a tarball for D3 with all of the media files in the same relative path.
    &#34;&#34;&#34;
    
    outdir = vipy.util.remkdir(os.path.abspath(os.path.normpath(outdir)))
    return self.clone().localmap(lambda v: v.filename(os.path.join(outdir, filetail(v.filename())), copy=False, symlink=True))</code></pre>
</details>
</dd>
<dt id="vipy.dataset.Dataset.multilabel_inverse_frequency_weight"><code class="name flex">
<span>def <span class="ident">multilabel_inverse_frequency_weight</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Return an inverse frequency weight for multilabel activities, where label counts are the fractional label likelihood within a clip</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/visym/vipy/blob/2ff555b8a23b3e6299d0a41de1f179f6005da60c/vipy/dataset.py#L689-L716" class="git-link">Browse git</a>
</summary>
<pre><code class="python">def multilabel_inverse_frequency_weight(self):
    &#34;&#34;&#34;Return an inverse frequency weight for multilabel activities, where label counts are the fractional label likelihood within a clip&#34;&#34;&#34;
    assert self._is_vipy_video()

    def _multilabel_inverse_frequency_weight(v):
        lbl_likelihood = {}
        if len(v.activities()) &gt; 0:
            (ef, sf) = (max([a.endframe() for a in v.activitylist()]), min([a.startframe() for a in v.activitylist()]))  # clip length 
            lbl_list = [a for A in v.activitylabel(sf, ef) for a in set(A)]  # list of all labels within clip (labels are unique in each frame)
            lbl_frequency = vipy.util.countby(lbl_list, lambda x: x)  # frequency of each label within clip
            lbl_weight = {k:v/float(len(lbl_list)) for (k,v) in lbl_frequency.items()}  # multi-label likelihood within clip, normalized frequency sums to one 
            for (k,w) in lbl_weight.items():
                if k not in lbl_likelihood:
                    lbl_likelihood[k] = 0
                lbl_likelihood[k] += w
        return lbl_likelihood
                
    lbl_likelihood  = {}
    for d in self.map(lambda v: _multilabel_inverse_frequency_weight(v)):  # parallelizable
        for (k,v) in d.items():
            if k not in lbl_likelihood:
                lbl_likelihood[k] = 0
            lbl_likelihood[k] += v

    # Inverse frequency weight on label likelihood per clip
    d = {k:1.0/max(v,1) for (k,v) in lbl_likelihood.items()}
    n = sum(d.values())  
    return {k:len(d)*(v/float(n)) for (k,v) in d.items()}</code></pre>
</details>
</dd>
<dt id="vipy.dataset.Dataset.num_categories"><code class="name flex">
<span>def <span class="ident">num_categories</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Alias for num_classes</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/visym/vipy/blob/2ff555b8a23b3e6299d0a41de1f179f6005da60c/vipy/dataset.py#L349-L351" class="git-link">Browse git</a>
</summary>
<pre><code class="python">def num_categories(self):
    &#34;&#34;&#34;Alias for num_classes&#34;&#34;&#34;
    return self.num_classes()</code></pre>
</details>
</dd>
<dt id="vipy.dataset.Dataset.num_classes"><code class="name flex">
<span>def <span class="ident">num_classes</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Return the number of unique categories in this dataset</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/visym/vipy/blob/2ff555b8a23b3e6299d0a41de1f179f6005da60c/vipy/dataset.py#L343-L345" class="git-link">Browse git</a>
</summary>
<pre><code class="python">def num_classes(self):
    &#34;&#34;&#34;Return the number of unique categories in this dataset&#34;&#34;&#34;
    return len(self.classlist())</code></pre>
</details>
</dd>
<dt id="vipy.dataset.Dataset.num_labels"><code class="name flex">
<span>def <span class="ident">num_labels</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Alias for num_classes</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/visym/vipy/blob/2ff555b8a23b3e6299d0a41de1f179f6005da60c/vipy/dataset.py#L346-L348" class="git-link">Browse git</a>
</summary>
<pre><code class="python">def num_labels(self):
    &#34;&#34;&#34;Alias for num_classes&#34;&#34;&#34;
    return self.num_classes()</code></pre>
</details>
</dd>
<dt id="vipy.dataset.Dataset.percentage"><code class="name flex">
<span>def <span class="ident">percentage</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Fraction of dataset for each label</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/visym/vipy/blob/2ff555b8a23b3e6299d0a41de1f179f6005da60c/vipy/dataset.py#L683-L687" class="git-link">Browse git</a>
</summary>
<pre><code class="python">def percentage(self):
    &#34;&#34;&#34;Fraction of dataset for each label&#34;&#34;&#34;
    d = self.count()
    n = sum(d.values())
    return {k:v/float(n) for (k,v) in d.items()}</code></pre>
</details>
</dd>
<dt id="vipy.dataset.Dataset.powerset"><code class="name flex">
<span>def <span class="ident">powerset</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/visym/vipy/blob/2ff555b8a23b3e6299d0a41de1f179f6005da60c/vipy/dataset.py#L366-L367" class="git-link">Browse git</a>
</summary>
<pre><code class="python">def powerset(self):
    return list(sorted(set([tuple(sorted(list(a))) for v in self for a in v.activitylabel() if len(a) &gt; 0])))        </code></pre>
</details>
</dd>
<dt id="vipy.dataset.Dataset.powerset_to_index"><code class="name flex">
<span>def <span class="ident">powerset_to_index</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/visym/vipy/blob/2ff555b8a23b3e6299d0a41de1f179f6005da60c/vipy/dataset.py#L369-L371" class="git-link">Browse git</a>
</summary>
<pre><code class="python">def powerset_to_index(self):        
    assert self._isvipy(), &#34;Invalid input&#34;
    return {c:k for (k,c) in enumerate(self.powerset())}</code></pre>
</details>
</dd>
<dt id="vipy.dataset.Dataset.replace"><code class="name flex">
<span>def <span class="ident">replace</span></span>(<span>self, other, key)</span>
</code></dt>
<dd>
<div class="desc"><p>Replace elements in self with other with equality detemrined by the key lambda function</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/visym/vipy/blob/2ff555b8a23b3e6299d0a41de1f179f6005da60c/vipy/dataset.py#L403-L408" class="git-link">Browse git</a>
</summary>
<pre><code class="python">def replace(self, other, key):
    &#34;&#34;&#34;Replace elements in self with other with equality detemrined by the key lambda function&#34;&#34;&#34;
    assert isinstance(other, Dataset), &#34;invalid input&#34;
    d = {key(v):v for v in other}
    self._objlist = [v if key(v) not in d else d[key(v)] for v in self]
    return self</code></pre>
</details>
</dd>
<dt id="vipy.dataset.Dataset.save"><code class="name flex">
<span>def <span class="ident">save</span></span>(<span>self, outfile, nourl=False, castas=None, relpath=False, sanitize=True, strict=True, significant_digits=2, noemail=True, flush=True, bycategory=False)</span>
</code></dt>
<dd>
<div class="desc"><p>Save the dataset to the provided output filename stored as pkl or json</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>outfile</code></strong></dt>
<dd>[str]: The /path/to/out.pkl or /path/to/out.json</dd>
<dt><strong><code>nourl</code></strong></dt>
<dd><a href="Unused">bool</a>: If true, remove all URLs from the media (if present)</dd>
<dt><strong><code>castas</code></strong></dt>
<dd>[type]:
Cast all media to the provided type.
This is useful for downcasting to <code><a title="vipy.video.Scene" href="video.html#vipy.video.Scene">Scene</a></code> from superclasses</dd>
<dt><strong><code>relpath</code></strong></dt>
<dd><a href="Unused">bool</a>: If true, define all file paths in objects relative to the /path/to in /path/to/out.json</dd>
<dt><strong><code>sanitize</code></strong></dt>
<dd><a href="Unused">bool</a>:
If trye, call sanitize() on all objects to remove all private attributes with prepended '__' </dd>
<dt><strong><code>strict</code></strong></dt>
<dd></dd>
<dt><strong><code>significant_digits</code></strong></dt>
<dd>[int]: Assign the requested number of significant digits to all bounding boxes in all tracks.
This requires dataset of <code><a title="vipy.video.Scene" href="video.html#vipy.video.Scene">Scene</a></code></dd>
<dt><strong><code>noemail</code></strong></dt>
<dd><a href="Unused">bool</a>: If true, scrub the attributes for emails and replace with a hash</dd>
<dt><strong><code>flush</code></strong></dt>
<dd><a href="Unused">bool</a>:
If true, flush the object buffers prior to save</dd>
</dl>
<p>bycategory [bool[: If trye, then save the dataset to the provided output filename pattern outfile='/path/to/annotations/*.json' where the wildcard is replaced with the category name
Returns:
<br>
This dataset that is quivalent to vipy.dataset.Dataset('/path/to/outfile.json')</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/visym/vipy/blob/2ff555b8a23b3e6299d0a41de1f179f6005da60c/vipy/dataset.py#L272-L330" class="git-link">Browse git</a>
</summary>
<pre><code class="python">def save(self, outfile, nourl=False, castas=None, relpath=False, sanitize=True, strict=True, significant_digits=2, noemail=True, flush=True, bycategory=False):
    &#34;&#34;&#34;Save the dataset to the provided output filename stored as pkl or json
    
    Args:
        outfile: [str]: The /path/to/out.pkl or /path/to/out.json
        nourl: [bool]: If true, remove all URLs from the media (if present)
        castas: [type]:  Cast all media to the provided type.  This is useful for downcasting to `vipy.video.Scene` from superclasses
        relpath: [bool]: If true, define all file paths in objects relative to the /path/to in /path/to/out.json
        sanitize: [bool]:  If trye, call sanitize() on all objects to remove all private attributes with prepended &#39;__&#39; 
        strict: [bool]: Unused
        significant_digits: [int]: Assign the requested number of significant digits to all bounding boxes in all tracks.  This requires dataset of `vipy.video.Scene`
        noemail: [bool]: If true, scrub the attributes for emails and replace with a hash
        flush: [bool]:  If true, flush the object buffers prior to save
        bycategory [bool[: If trye, then save the dataset to the provided output filename pattern outfile=&#39;/path/to/annotations/*.json&#39; where the wildcard is replaced with the category name

    Returns:        
        This dataset that is quivalent to vipy.dataset.Dataset(&#39;/path/to/outfile.json&#39;)
    &#34;&#34;&#34;
    n = len([v for v in self if v is None])
    if n &gt; 0:
        print(&#39;[vipy.dataset]: removing %d invalid elements&#39; % n)
    objlist = [v for v in self if v is not None]  
    if relpath or nourl or sanitize or flush or noemail or (significant_digits is not None):
        assert self._isvipy(), &#34;Invalid input&#34;
    if relpath:
        print(&#39;[vipy.dataset]: setting relative paths&#39;)
        objlist = [v.relpath(start=filepath(outfile)) if os.path.isabs(v.filename()) else v for v in objlist]
    if nourl: 
        print(&#39;[vipy.dataset]: removing URLs&#39;)
        objlist = [v.nourl() for v in objlist]           
    if sanitize:
        print(&#39;[vipy.dataset]: sanitizing attributes&#39;)                        
        objlist = [v.sanitize() for v in objlist]  # removes all attributes with &#39;__&#39; keys
    if castas is not None:
        assert hasattr(castas, &#39;cast&#39;), &#34;Invalid cast&#34;
        print(&#39;[vipy.dataset]: casting as &#34;%s&#34;&#39; % (str(castas)))
        objlist = [castas.cast(v) for v in objlist]                     
    if significant_digits is not None:
        assert self._is_vipy_video_scene()
        assert isinstance(significant_digits, int) and significant_digits &gt;= 1, &#34;Invalid input&#34;
        objlist = [o.trackmap(lambda t: t.significant_digits(significant_digits)) if o is not None else o for o in objlist]
    if noemail:
        print(&#39;[vipy.dataset]: removing emails&#39;)            
        for o in objlist:
            for (k,v) in o.attributes.items():
                if isinstance(v, str) and is_email_address(v):
                    o.attributes[k] = hashlib.sha1(v.encode(&#34;UTF-8&#34;)).hexdigest()[0:10]
    if flush:
        objlist = [o.flush() for o in objlist]  

    if bycategory:
        for (c,V) in vipy.util.groupbyasdict(list(self), lambda v: v.category()).items():
            jsonfile = outfile.replace(&#39;*&#39;, c)  # outfile=&#34;/path/to/annotations/*.json&#34;
            d = Dataset(V, id=c).save(jsonfile, relpath=relpath, nourl=nourl, sanitize=sanitize, castas=castas, significant_digits=significant_digits, noemail=noemail, flush=flush, bycategory=False)
            print(&#39;[vipy.dataset]: Saving %s by category to &#34;%s&#34;&#39; % (str(d), jsonfile))                
    else:
        print(&#39;[vipy.dataset]: Saving %s to &#34;%s&#34;&#39; % (str(self), outfile))
        vipy.util.save(objlist, outfile)
    return self</code></pre>
</details>
</dd>
<dt id="vipy.dataset.Dataset.shuffle"><code class="name flex">
<span>def <span class="ident">shuffle</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Randomly permute elements in this dataset</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/visym/vipy/blob/2ff555b8a23b3e6299d0a41de1f179f6005da60c/vipy/dataset.py#L528-L531" class="git-link">Browse git</a>
</summary>
<pre><code class="python">def shuffle(self):
    &#34;&#34;&#34;Randomly permute elements in this dataset&#34;&#34;&#34;
    self._objlist.sort(key=lambda x: random.random())  # in place
    return self</code></pre>
</details>
</dd>
<dt id="vipy.dataset.Dataset.sort"><code class="name flex">
<span>def <span class="ident">sort</span></span>(<span>self, key)</span>
</code></dt>
<dd>
<div class="desc"><p>Sort the dataset in-place using the sortkey lambda function</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/visym/vipy/blob/2ff555b8a23b3e6299d0a41de1f179f6005da60c/vipy/dataset.py#L974-L978" class="git-link">Browse git</a>
</summary>
<pre><code class="python">def sort(self, key):
    &#34;&#34;&#34;Sort the dataset in-place using the sortkey lambda function&#34;&#34;&#34;
    if key is not None:
        self._objlist.sort(key=lambda x: key(self._loader(x)))
    return self</code></pre>
</details>
</dd>
<dt id="vipy.dataset.Dataset.split"><code class="name flex">
<span>def <span class="ident">split</span></span>(<span>self, trainfraction=0.9, valfraction=0.1, testfraction=0, seed=42)</span>
</code></dt>
<dd>
<div class="desc"><p>Split the dataset by category by fraction so that video IDs are never in the same set</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/visym/vipy/blob/2ff555b8a23b3e6299d0a41de1f179f6005da60c/vipy/dataset.py#L538-L563" class="git-link">Browse git</a>
</summary>
<pre><code class="python">def split(self, trainfraction=0.9, valfraction=0.1, testfraction=0, seed=42):
    &#34;&#34;&#34;Split the dataset by category by fraction so that video IDs are never in the same set&#34;&#34;&#34;
    assert self._isvipy(), &#34;Invalid input&#34;
    assert trainfraction &gt;=0 and trainfraction &lt;= 1
    assert valfraction &gt;=0 and valfraction &lt;= 1
    assert testfraction &gt;=0 and testfraction &lt;= 1
    assert trainfraction + valfraction + testfraction == 1.0
    np.random.seed(seed)  # deterministic
    
    # Video ID assignment
    A = self.list()
    videoid = list(set([a.videoid() for a in A]))
    np.random.shuffle(videoid)
    (testid, valid, trainid) = vipy.util.dividelist(videoid, (testfraction, valfraction, trainfraction))        
    (testid, valid, trainid) = (set(testid), set(valid), set(trainid))
    d = groupbyasdict(A, lambda a: &#39;testset&#39; if a.videoid() in testid else &#39;valset&#39; if a.videoid() in valid else &#39;trainset&#39;)
    (trainset, testset, valset) = (d[&#39;trainset&#39;] if &#39;trainset&#39; in d else [], 
                                   d[&#39;testset&#39;] if &#39;testset&#39; in d else [], 
                                   d[&#39;valset&#39;] if &#39;valset&#39; in d else [])

    print(&#39;[vipy.dataset]: trainset=%d (%1.2f)&#39; % (len(trainset), trainfraction))
    print(&#39;[vipy.dataset]: valset=%d (%1.2f)&#39; % (len(valset), valfraction))
    print(&#39;[vipy.dataset]: testset=%d (%1.2f)&#39; % (len(testset), testfraction))
    np.random.seed()  # re-initialize seed

    return (Dataset(trainset, id=&#39;trainset&#39;), Dataset(valset, id=&#39;valset&#39;), Dataset(testset, id=&#39;testset&#39;) if len(testset)&gt;0 else None)</code></pre>
</details>
</dd>
<dt id="vipy.dataset.Dataset.synonym"><code class="name flex">
<span>def <span class="ident">synonym</span></span>(<span>self, synonymdict)</span>
</code></dt>
<dd>
<div class="desc"><p>Convert all categories in the dataset using the provided synonym dictionary mapping</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/visym/vipy/blob/2ff555b8a23b3e6299d0a41de1f179f6005da60c/vipy/dataset.py#L658-L667" class="git-link">Browse git</a>
</summary>
<pre><code class="python">def synonym(self, synonymdict):
    &#34;&#34;&#34;Convert all categories in the dataset using the provided synonym dictionary mapping&#34;&#34;&#34;
    assert self._isvipy()
    assert isinstance(synonymdict, dict)
    
    if self._is_vipy_video_scene():
        return self.localmap(lambda v: v.trackmap(lambda t: t.categoryif(synonymdict)).activitymap(lambda a: a.categoryif(synonymdict)))
    elif self._is_vipy_image_scene():
        return self.localmap(lambda v: v.objectmap(lambda o: o.categoryif(synonymdict)))
    return self</code></pre>
</details>
</dd>
<dt id="vipy.dataset.Dataset.take"><code class="name flex">
<span>def <span class="ident">take</span></span>(<span>self, n=1, category=None, canload=False, seed=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Randomlly Take n elements from the dataset, and return a dataset if n&gt;1, otherwise return the singleton element.
If seed=int, take will return the same results each time.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/visym/vipy/blob/2ff555b8a23b3e6299d0a41de1f179f6005da60c/vipy/dataset.py#L516-L521" class="git-link">Browse git</a>
</summary>
<pre><code class="python">def take(self, n=1, category=None, canload=False, seed=None):
    &#34;&#34;&#34;Randomlly Take n elements from the dataset, and return a dataset if n&gt;1, otherwise return the singleton element.  If seed=int, take will return the same results each time.&#34;&#34;&#34;
    assert isinstance(n, int) and n&gt;0
    D = self.clone(shallow=True)
    D._objlist = self.takelist(n, category=category, seed=seed)
    return D if n&gt;1 else D[0]</code></pre>
</details>
</dd>
<dt id="vipy.dataset.Dataset.take_per_category"><code class="name flex">
<span>def <span class="ident">take_per_category</span></span>(<span>self, n, seed=None)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/visym/vipy/blob/2ff555b8a23b3e6299d0a41de1f179f6005da60c/vipy/dataset.py#L523-L526" class="git-link">Browse git</a>
</summary>
<pre><code class="python">def take_per_category(self, n, seed=None):
    D = self.clone(shallow=True)
    D._objlist = [v for c in self.categories() for v in self.takelist(n, category=c, seed=seed)]
    return D</code></pre>
</details>
</dd>
<dt id="vipy.dataset.Dataset.takefilter"><code class="name flex">
<span>def <span class="ident">takefilter</span></span>(<span>self, f, n=1)</span>
</code></dt>
<dd>
<div class="desc"><p>Apply the lambda function f and return n elements in a list where the filter returns true</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>f</code></strong></dt>
<dd>[lambda] If f(x) returns true, then keep</dd>
<dt><strong><code>n</code></strong></dt>
<dd>[int &gt;= 0] The number of elements to take</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>[n=0] Returns empty list
[n=1] Returns singleton element
[n&gt;1] Returns list of elements of at most n such that each element f(x) is True</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/visym/vipy/blob/2ff555b8a23b3e6299d0a41de1f179f6005da60c/vipy/dataset.py#L439-L452" class="git-link">Browse git</a>
</summary>
<pre><code class="python">def takefilter(self, f, n=1):
    &#34;&#34;&#34;Apply the lambda function f and return n elements in a list where the filter returns true
    
    Args:
        f: [lambda] If f(x) returns true, then keep
        n: [int &gt;= 0] The number of elements to take
    
    Returns:
        [n=0] Returns empty list
        [n=1] Returns singleton element
        [n&gt;1] Returns list of elements of at most n such that each element f(x) is True            
    &#34;&#34;&#34;
    objlist = [obj for obj in self if f(obj)]
    return [] if (len(objlist) == 0 or n == 0) else (objlist[0] if n==1 else objlist[0:n])</code></pre>
</details>
</dd>
<dt id="vipy.dataset.Dataset.takelist"><code class="name flex">
<span>def <span class="ident">takelist</span></span>(<span>self, n, category=None, seed=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Take n elements of selected category and return list.
The elements are not cloned.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/visym/vipy/blob/2ff555b8a23b3e6299d0a41de1f179f6005da60c/vipy/dataset.py#L499-L508" class="git-link">Browse git</a>
</summary>
<pre><code class="python">def takelist(self, n, category=None, seed=None):
    &#34;&#34;&#34;Take n elements of selected category and return list.  The elements are not cloned.&#34;&#34;&#34;
    assert n &gt;= 0, &#34;Invalid length&#34;
    K = list(range(len(self))) if category is None else [k for (k,v) in enumerate(self) if v.category() == category]
    if seed is not None:
        np.random.seed(seed)            
    outlist = [self[int(k)] for k in np.random.permutation(K)[0:n]]  # native python int
    if seed is not None:
        np.random.seed()
    return outlist</code></pre>
</details>
</dd>
<dt id="vipy.dataset.Dataset.to_torch"><code class="name flex">
<span>def <span class="ident">to_torch</span></span>(<span>self, f_video_to_tensor)</span>
</code></dt>
<dd>
<div class="desc"><p>Return a torch dataset that will apply the lambda function f_video_to_tensor to each element in the dataset on demand</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/visym/vipy/blob/2ff555b8a23b3e6299d0a41de1f179f6005da60c/vipy/dataset.py#L846-L849" class="git-link">Browse git</a>
</summary>
<pre><code class="python">def to_torch(self, f_video_to_tensor):
    &#34;&#34;&#34;Return a torch dataset that will apply the lambda function f_video_to_tensor to each element in the dataset on demand&#34;&#34;&#34;
    import vipy.torch
    return vipy.torch.TorchDataset(f_video_to_tensor, self)</code></pre>
</details>
</dd>
<dt id="vipy.dataset.Dataset.to_torch_tensordir"><code class="name flex">
<span>def <span class="ident">to_torch_tensordir</span></span>(<span>self, f_video_to_tensor, outdir, n_augmentations=20, sleep=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Return a TorchTensordir dataset that will load a pkl.bz2 file that contains one of n_augmentations (tensor, label) pairs.</p>
<p>This is useful for fast loading of datasets that contain many videos.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/visym/vipy/blob/2ff555b8a23b3e6299d0a41de1f179f6005da60c/vipy/dataset.py#L851-L863" class="git-link">Browse git</a>
</summary>
<pre><code class="python">def to_torch_tensordir(self, f_video_to_tensor, outdir, n_augmentations=20, sleep=None):
    &#34;&#34;&#34;Return a TorchTensordir dataset that will load a pkl.bz2 file that contains one of n_augmentations (tensor, label) pairs.
    
    This is useful for fast loading of datasets that contain many videos.

    &#34;&#34;&#34;
    import vipy.torch    # lazy import, requires vipy[all] 
    from vipy.batch import Batch   # requires pip install vipy[all]

    assert self._is_vipy_video_scene()
    outdir = vipy.util.remkdir(outdir)
    self.map(lambda v, f=f_video_to_tensor, outdir=outdir, n_augmentations=n_augmentations: vipy.util.bz2pkl(os.path.join(outdir, &#39;%s.pkl.bz2&#39; % v.instanceid()), [f(v.print(sleep=sleep).clone()) for k in range(0, n_augmentations)]))
    return vipy.torch.Tensordir(outdir)</code></pre>
</details>
</dd>
<dt id="vipy.dataset.Dataset.tocsv"><code class="name flex">
<span>def <span class="ident">tocsv</span></span>(<span>self, csvfile=None)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/visym/vipy/blob/2ff555b8a23b3e6299d0a41de1f179f6005da60c/vipy/dataset.py#L565-L567" class="git-link">Browse git</a>
</summary>
<pre><code class="python">def tocsv(self, csvfile=None):
    csv = [v.csv() for v in self.list]        
    return vipy.util.writecsv(csv, csvfile) if csvfile is not None else (csv[0], csv[1:])</code></pre>
</details>
</dd>
<dt id="vipy.dataset.Dataset.tohtml"><code class="name flex">
<span>def <span class="ident">tohtml</span></span>(<span>self, outfile, mindim=512, title='Visualization', fraction=1.0, display=False, clip=True, activities=True, category=True)</span>
</code></dt>
<dd>
<div class="desc"><p>Generate a standalone HTML file containing quicklooks for each annotated activity in dataset, along with some helpful provenance information for where the annotation came from</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/visym/vipy/blob/2ff555b8a23b3e6299d0a41de1f179f6005da60c/vipy/dataset.py#L870-L889" class="git-link">Browse git</a>
</summary>
<pre><code class="python">def tohtml(self, outfile, mindim=512, title=&#39;Visualization&#39;, fraction=1.0, display=False, clip=True, activities=True, category=True):
    &#34;&#34;&#34;Generate a standalone HTML file containing quicklooks for each annotated activity in dataset, along with some helpful provenance information for where the annotation came from&#34;&#34;&#34;

    assert ishtml(outfile), &#34;Output file must be .html&#34;
    assert fraction &gt; 0 and fraction &lt;= 1.0, &#34;Fraction must be between [0,1]&#34;
    
    import vipy.util  # This should not be necessary, but we get &#34;UnboundLocalError&#34; without it, not sure why..
    import vipy.batch  # requires pip install vipy[all]

    dataset = self.list()
    assert all([isinstance(v, vipy.video.Video) for v in dataset])
    dataset = [dataset[int(k)] for k in np.random.permutation(range(len(dataset)))[0:int(len(dataset)*fraction)]]
    #dataset = [v for v in dataset if all([len(a) &lt; 15*v.framerate() for a in v.activitylist()])]  # remove extremely long videos

    quicklist = vipy.batch.Batch(dataset, strict=False, as_completed=True, minscatter=1).map(lambda v: (v.load().quicklook(), v.flush().print())).result()
    quicklist = [x for x in quicklist if x is not None]  # remove errors
    quicklooks = [imq for (imq, v) in quicklist]  # keep original video for HTML display purposes
    provenance = [{&#39;clip&#39;:str(v), &#39;activities&#39;:str(&#39;;&#39;.join([str(a) for a in v.activitylist()])), &#39;category&#39;:v.category()} for (imq, v) in quicklist]
    (quicklooks, provenance) = zip(*sorted([(q,p) for (q,p) in zip(quicklooks, provenance)], key=lambda x: x[1][&#39;category&#39;]))  # sorted in category order
    return vipy.visualize.tohtml(quicklooks, provenance, title=&#39;%s&#39; % title, outfile=outfile, mindim=mindim, display=display)</code></pre>
</details>
</dd>
<dt id="vipy.dataset.Dataset.tojsondir"><code class="name flex">
<span>def <span class="ident">tojsondir</span></span>(<span>self, outdir=None, verbose=True, rekey=False, bycategory=False, byfilename=False, abspath=True)</span>
</code></dt>
<dd>
<div class="desc"><p>Alias for <code><a title="vipy.dataset.Dataset.jsondir" href="#vipy.dataset.Dataset.jsondir">Dataset.jsondir()</a></code></p></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/visym/vipy/blob/2ff555b8a23b3e6299d0a41de1f179f6005da60c/vipy/dataset.py#L495-L497" class="git-link">Browse git</a>
</summary>
<pre><code class="python">def tojsondir(self, outdir=None, verbose=True, rekey=False, bycategory=False, byfilename=False, abspath=True):
    &#34;&#34;&#34;Alias for `vipy.dataset.Dataset.jsondir`&#34;&#34;&#34;
    return self.jsondir(outdir, verbose=verbose, rekey=rekey, bycategory=bycategory, byfilename=byfilename, abspath=abspath)</code></pre>
</details>
</dd>
<dt id="vipy.dataset.Dataset.tolist"><code class="name flex">
<span>def <span class="ident">tolist</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Alias for self.list()</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/visym/vipy/blob/2ff555b8a23b3e6299d0a41de1f179f6005da60c/vipy/dataset.py#L143-L145" class="git-link">Browse git</a>
</summary>
<pre><code class="python">def tolist(self):
    &#34;&#34;&#34;Alias for self.list()&#34;&#34;&#34;
    return list(self)</code></pre>
</details>
</dd>
<dt id="vipy.dataset.Dataset.union"><code class="name flex">
<span>def <span class="ident">union</span></span>(<span>self, other, key=None)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/visym/vipy/blob/2ff555b8a23b3e6299d0a41de1f179f6005da60c/vipy/dataset.py#L380-L392" class="git-link">Browse git</a>
</summary>
<pre><code class="python">def union(self, other, key=None):
    assert isinstance(other, Dataset), &#34;invalid input&#34;
    if len(other) &gt; 0:
        try:
            if other._loader is not None:
                other._loader(self._objlist[0])
            if self._loader is not None:
                self._loader(other._objlist[0])
            self._objlist = self._objlist + other._objlist  # compatible loaders
        except:
            self._objlist = self.list() + other.list()  # incompatible loaders
            self._loader = None
    return self.dedupe(key) if key is not None else self</code></pre>
</details>
</dd>
<dt id="vipy.dataset.Dataset.valid"><code class="name flex">
<span>def <span class="ident">valid</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/visym/vipy/blob/2ff555b8a23b3e6299d0a41de1f179f6005da60c/vipy/dataset.py#L436-L437" class="git-link">Browse git</a>
</summary>
<pre><code class="python">def valid(self):
    return self.filter(lambda v: v is not None)</code></pre>
</details>
</dd>
<dt id="vipy.dataset.Dataset.video_duration_in_seconds"><code class="name flex">
<span>def <span class="ident">video_duration_in_seconds</span></span>(<span>self, outfile=None, fontsize=6, max_duration=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Duration of activities</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/visym/vipy/blob/2ff555b8a23b3e6299d0a41de1f179f6005da60c/vipy/dataset.py#L740-L747" class="git-link">Browse git</a>
</summary>
<pre><code class="python">def video_duration_in_seconds(self, outfile=None, fontsize=6, max_duration=None):
    &#34;&#34;&#34;Duration of activities&#34;&#34;&#34;
    assert self._isvipy()
    d = {k:np.mean([d for (c,d) in D]) for (k,D) in groupbyasdict([(v.category(), v.duration()) for v in self.list()], lambda x: x[0]).items()}
    if outfile is not None:
        max_duration = max(d.values()) if max_duration is None else max_duration
        vipy.metrics.histogram([min(x, max_duration) for x in d.values()], d.keys(), outfile=outfile, ylabel=&#39;Duration (seconds)&#39;, fontsize=fontsize)            
    return d</code></pre>
</details>
</dd>
<dt id="vipy.dataset.Dataset.video_montage"><code class="name flex">
<span>def <span class="ident">video_montage</span></span>(<span>self, outfile, gridrows, gridcols, mindim=64, bycategory=False, category=None, annotate=True, trackcrop=False, transpose=False, max_duration=None, framerate=30, fontsize=8)</span>
</code></dt>
<dd>
<div class="desc"><p>30x50 activity montage, each 64x64 elements.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>outfile</code></strong></dt>
<dd>[str] The name of the outfile for the video.
Must have a valid video extension. </dd>
<dt><strong><code>gridrows</code></strong></dt>
<dd>[int, None]
The number of rows to include in the montage.
If None, infer from other args</dd>
<dt><strong><code>gridcols</code></strong></dt>
<dd>[int] The number of columns in the montage</dd>
<dt><strong><code>mindim</code></strong></dt>
<dd>[int] The square size of each video in the montage</dd>
<dt><strong><code>bycategory</code></strong></dt>
<dd>[bool]
Make the video such that each row is a category </dd>
<dt><strong><code>category</code></strong></dt>
<dd>[str, list] Make the video so that every element is of category.
May be a list of more than one categories</dd>
<dt><strong><code>annotate</code></strong></dt>
<dd>[bool] If true, include boxes and captions for objects and activities</dd>
<dt><strong><code>trackcrop</code></strong></dt>
<dd>[bool] If true, center the video elements on the tracks with dilation factor 1.5</dd>
<dt><strong><code>transpose</code></strong></dt>
<dd>[bool] If true, organize categories columnwise, but still return a montage of size (gridrows, gridcols)</dd>
<dt><strong><code>max_duration</code></strong></dt>
<dd>[float] If not None, then set a maximum duration in seconds for elements in the video.
If None, then the max duration is the duration of the longest element.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>A clone of the dataset containing the selected videos for the montage, ordered rowwise in the montage</p>
<div class="admonition notes">
<p class="admonition-title">Notes</p>
<ul>
<li>If a category does not contain the required number of elements for bycategory, it is removed prior to visualization</li>
<li>Elements are looped if they exit prior to the end of the longest video (or max_duration)</li>
</ul>
</div></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/visym/vipy/blob/2ff555b8a23b3e6299d0a41de1f179f6005da60c/vipy/dataset.py#L892-L948" class="git-link">Browse git</a>
</summary>
<pre><code class="python">def video_montage(self, outfile, gridrows, gridcols, mindim=64, bycategory=False, category=None, annotate=True, trackcrop=False, transpose=False, max_duration=None, framerate=30, fontsize=8):
    &#34;&#34;&#34;30x50 activity montage, each 64x64 elements.

    Args:
        outfile: [str] The name of the outfile for the video.  Must have a valid video extension. 
        gridrows: [int, None]  The number of rows to include in the montage.  If None, infer from other args
        gridcols: [int] The number of columns in the montage
        mindim: [int] The square size of each video in the montage
        bycategory: [bool]  Make the video such that each row is a category 
        category: [str, list] Make the video so that every element is of category.  May be a list of more than one categories
        annotate: [bool] If true, include boxes and captions for objects and activities
        trackcrop: [bool] If true, center the video elements on the tracks with dilation factor 1.5
        transpose: [bool] If true, organize categories columnwise, but still return a montage of size (gridrows, gridcols)
        max_duration: [float] If not None, then set a maximum duration in seconds for elements in the video.  If None, then the max duration is the duration of the longest element.

    Returns:
        A clone of the dataset containing the selected videos for the montage, ordered rowwise in the montage

    .. notes::  
        - If a category does not contain the required number of elements for bycategory, it is removed prior to visualization
        - Elements are looped if they exit prior to the end of the longest video (or max_duration)
    &#34;&#34;&#34;
    assert self._is_vipy_video()
    assert vipy.util.isvideo(outfile)
    assert gridrows is None or (isinstance(gridrows, int) and gridrows &gt;= 1)
    assert gridcols is None or (isinstance(gridcols, int) and gridcols &gt;= 1)
    assert isinstance(mindim, int) and mindim &gt;= 1
    assert category is None or isinstance(category, str)

    D = self.clone()
    if bycategory:
        (num_categories, num_elements) = (gridrows, gridcols) if not transpose else (gridcols, gridrows)
        assert num_elements is not None
        requested_categories = sorted(D.classlist()) if (num_categories is None) else sorted(D.classlist())[0:num_categories]             
        categories = [c for c in requested_categories if D.count()[c] &gt;= num_elements]  # filter those categories that do not have enough
        if set(categories) != set(requested_categories):
            warnings.warn(&#39;[vipy.dataset.video_montage]: removing &#34;%s&#34; without at least %d examples&#39; % (str(set(requested_categories).difference(set(categories))), num_elements))
        vidlist = sorted(D.filter(lambda v: v.category() in categories).take_per_category(num_elements).tolist(), key=lambda v: v.category())
        vidlist = vidlist if not transpose else [vidlist[k] for k in np.array(range(0, len(vidlist))).reshape( (len(categories), num_elements) ).transpose().flatten().tolist()] 
        (gridrows, gridcols) = (len(categories), num_elements) if not transpose else (num_elements, len(categories))
        assert len(vidlist) == gridrows*gridcols

    elif category is not None:
        vidlist = D.filter(lambda v: v.category() in vipy.util.tolist(category)).take(gridrows*gridcols, canload=True).tolist()            
    elif len(D) != gridrows*gridcols:
        vidlist = D.take(gridrows*gridcols, canload=True).tolist()
    else:
        vidlist = D.tolist()

    vidlist = [v.framerate(framerate) for v in vidlist]  # resample to common framerate (this may result in jittery tracks
    montage = Dataset(vidlist, id=&#39;video_montage&#39;).clone()  # for output
    vidlist = [v.trackcrop(dilate=1.5, maxsquare=True) if (v.trackbox() is not None) else v for v in vidlist] if trackcrop else vidlist  # may be None, if so return the video
    vidlist = [v.mindim(mindim) for v in vidlist]  # before annotate for common font size
    vidlist = [vipy.video.Video.cast(v) for v in vidlist] if not annotate else [v.annotate(verbose=False, fontsize=fontsize) for v in vidlist]  # pre-annotate
        
    vipy.visualize.videomontage(vidlist, mindim, mindim, gridrows=gridrows, gridcols=gridcols, framerate=framerate, max_duration=max_duration).saveas(outfile)
    return montage        </code></pre>
</details>
</dd>
<dt id="vipy.dataset.Dataset.zip"><code class="name flex">
<span>def <span class="ident">zip</span></span>(<span>self, other, sortkey=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Zip two datasets.
Equivalent to zip(self, other).</p>
<pre><code class="language-python">for (d1,d2) in D1.zip(D2, sortkey=lambda v: v.instanceid()):
    pass

for (d1, d2) in zip(D1, D2):
    pass
</code></pre>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>other</code></strong></dt>
<dd>[<code>vipy.dataset.Dataset</code>] </dd>
<dt><strong><code>sortkey</code></strong></dt>
<dd>[lambda] sort both datasets using the provided sortkey lambda.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>Generator for the tuple sequence ( (self[0], other[0]), (self[1], other[1]), &hellip; )</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/visym/vipy/blob/2ff555b8a23b3e6299d0a41de1f179f6005da60c/vipy/dataset.py#L950-L972" class="git-link">Browse git</a>
</summary>
<pre><code class="python">def zip(self, other, sortkey=None):
    &#34;&#34;&#34;Zip two datasets.  Equivalent to zip(self, other).

    ```python
    for (d1,d2) in D1.zip(D2, sortkey=lambda v: v.instanceid()):
        pass
    
    for (d1, d2) in zip(D1, D2):
        pass
    ```

    Args:
        other: [`vipy.dataset.Dataset`] 
        sortkey: [lambda] sort both datasets using the provided sortkey lambda.
    
    Returns:
        Generator for the tuple sequence ( (self[0], other[0]), (self[1], other[1]), ... )
    &#34;&#34;&#34; 
    assert isinstance(other, Dataset)
    assert len(self) == len(other)

    for (vi, vj) in zip(self.sort(sortkey), other.sort(sortkey)):
        yield (vi, vj)</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<header>
<a class="homelink" rel="home" title="VIPY" href="https://github.com/visym/vipy/">
<img src="https://www.visym.com/labs/images/visym_logo_black_notext.png" alt="" width="60">
</a>
<h1 style="font-size:200%;"><b>VIPY:</b> Visual Dataset Transformation</h1>
</header>
<form>
<input id="lunr-search" name="q" placeholder=" Search ..." aria-label="Search"
disabled minlength="2">
</form>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/tingle/0.15.3/tingle.min.css" integrity="sha512-j1u8eUJ4f23xPPxwOrLUPQaCD2dwzNqqmDDcWS4deWsMv2ohLqmXXuP3hU7g8TyzbMSakP/mMqoNBYWj8AEIFg==" crossorigin>
<script src="https://cdnjs.cloudflare.com/ajax/libs/tingle/0.15.3/tingle.min.js" integrity="sha512-plGUER9JkeEWPPqQBE4sdLqBoQug5Ap+BCGMc7bJ8BXkm+VVj6QzkpBz5Yv2yPkkq+cqg9IpkBaGCas6uDbW8g==" crossorigin></script>
<style>
.modal-dialog iframe {
width: 100vw;
height: calc(100vh - 80px);
}
@media screen and (min-width: 700px) {
.modal-dialog iframe {
width: 70vw;
height: 80vh;
}
}
.modal-dialog .tingle-modal-box {width: auto;}
.modal-dialog .tingle-modal-box__content {padding: 0;}
</style>
<script>
const input = document.getElementById('lunr-search');
input.disabled = false;
input.form.addEventListener('submit', (ev) => {
ev.preventDefault();
const url = new URL(window.location);
url.searchParams.set('q', input.value);
history.replaceState({}, null, url.toString());
search(input.value);
});
const query = new URL(window.location).searchParams.get('q');
if (query)
search(query);
function search(query) {
const url = './doc-search.html#' + encodeURIComponent(query);
new tingle.modal({
cssClass: ['modal-dialog'],
onClose: () => {
const url = new URL(window.location);
url.searchParams.delete('q');
history.replaceState({}, null, url.toString());
setTimeout(() => input.focus(), 100);
}
}).setContent('<iframe src="' + url + '"></iframe>').open();
}
</script>
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="vipy" href="index.html">vipy</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="vipy.dataset.Dataset" href="#vipy.dataset.Dataset">Dataset</a></code></h4>
<ul class="">
<li><code><a title="vipy.dataset.Dataset.annotate" href="#vipy.dataset.Dataset.annotate">annotate</a></code></li>
<li><code><a title="vipy.dataset.Dataset.archive" href="#vipy.dataset.Dataset.archive">archive</a></code></li>
<li><code><a title="vipy.dataset.Dataset.augment" href="#vipy.dataset.Dataset.augment">augment</a></code></li>
<li><code><a title="vipy.dataset.Dataset.boxsize" href="#vipy.dataset.Dataset.boxsize">boxsize</a></code></li>
<li><code><a title="vipy.dataset.Dataset.boxsize_by_category" href="#vipy.dataset.Dataset.boxsize_by_category">boxsize_by_category</a></code></li>
<li><code><a title="vipy.dataset.Dataset.boxsize_histogram" href="#vipy.dataset.Dataset.boxsize_histogram">boxsize_histogram</a></code></li>
<li><code><a title="vipy.dataset.Dataset.categories" href="#vipy.dataset.Dataset.categories">categories</a></code></li>
<li><code><a title="vipy.dataset.Dataset.chunk" href="#vipy.dataset.Dataset.chunk">chunk</a></code></li>
<li><code><a title="vipy.dataset.Dataset.class_to_index" href="#vipy.dataset.Dataset.class_to_index">class_to_index</a></code></li>
<li><code><a title="vipy.dataset.Dataset.classes" href="#vipy.dataset.Dataset.classes">classes</a></code></li>
<li><code><a title="vipy.dataset.Dataset.classlist" href="#vipy.dataset.Dataset.classlist">classlist</a></code></li>
<li><code><a title="vipy.dataset.Dataset.clone" href="#vipy.dataset.Dataset.clone">clone</a></code></li>
<li><code><a title="vipy.dataset.Dataset.count" href="#vipy.dataset.Dataset.count">count</a></code></li>
<li><code><a title="vipy.dataset.Dataset.countby" href="#vipy.dataset.Dataset.countby">countby</a></code></li>
<li><code><a title="vipy.dataset.Dataset.dedupe" href="#vipy.dataset.Dataset.dedupe">dedupe</a></code></li>
<li><code><a title="vipy.dataset.Dataset.density" href="#vipy.dataset.Dataset.density">density</a></code></li>
<li><code><a title="vipy.dataset.Dataset.difference" href="#vipy.dataset.Dataset.difference">difference</a></code></li>
<li><code><a title="vipy.dataset.Dataset.duration_in_frames" href="#vipy.dataset.Dataset.duration_in_frames">duration_in_frames</a></code></li>
<li><code><a title="vipy.dataset.Dataset.duration_in_seconds" href="#vipy.dataset.Dataset.duration_in_seconds">duration_in_seconds</a></code></li>
<li><code><a title="vipy.dataset.Dataset.filter" href="#vipy.dataset.Dataset.filter">filter</a></code></li>
<li><code><a title="vipy.dataset.Dataset.flatmap" href="#vipy.dataset.Dataset.flatmap">flatmap</a></code></li>
<li><code><a title="vipy.dataset.Dataset.flatten" href="#vipy.dataset.Dataset.flatten">flatten</a></code></li>
<li><code><a title="vipy.dataset.Dataset.framerate" href="#vipy.dataset.Dataset.framerate">framerate</a></code></li>
<li><code><a title="vipy.dataset.Dataset.frequency" href="#vipy.dataset.Dataset.frequency">frequency</a></code></li>
<li><code><a title="vipy.dataset.Dataset.from_json" href="#vipy.dataset.Dataset.from_json">from_json</a></code></li>
<li><code><a title="vipy.dataset.Dataset.has" href="#vipy.dataset.Dataset.has">has</a></code></li>
<li><code><a title="vipy.dataset.Dataset.histogram" href="#vipy.dataset.Dataset.histogram">histogram</a></code></li>
<li><code><a title="vipy.dataset.Dataset.id" href="#vipy.dataset.Dataset.id">id</a></code></li>
<li><code><a title="vipy.dataset.Dataset.index_to_class" href="#vipy.dataset.Dataset.index_to_class">index_to_class</a></code></li>
<li><code><a title="vipy.dataset.Dataset.inverse_frequency_weight" href="#vipy.dataset.Dataset.inverse_frequency_weight">inverse_frequency_weight</a></code></li>
<li><code><a title="vipy.dataset.Dataset.istype" href="#vipy.dataset.Dataset.istype">istype</a></code></li>
<li><code><a title="vipy.dataset.Dataset.json" href="#vipy.dataset.Dataset.json">json</a></code></li>
<li><code><a title="vipy.dataset.Dataset.jsondir" href="#vipy.dataset.Dataset.jsondir">jsondir</a></code></li>
<li><code><a title="vipy.dataset.Dataset.label_to_index" href="#vipy.dataset.Dataset.label_to_index">label_to_index</a></code></li>
<li><code><a title="vipy.dataset.Dataset.list" href="#vipy.dataset.Dataset.list">list</a></code></li>
<li><code><a title="vipy.dataset.Dataset.load" href="#vipy.dataset.Dataset.load">load</a></code></li>
<li><code><a title="vipy.dataset.Dataset.localmap" href="#vipy.dataset.Dataset.localmap">localmap</a></code></li>
<li><code><a title="vipy.dataset.Dataset.map" href="#vipy.dataset.Dataset.map">map</a></code></li>
<li><code><a title="vipy.dataset.Dataset.merge" href="#vipy.dataset.Dataset.merge">merge</a></code></li>
<li><code><a title="vipy.dataset.Dataset.multilabel_inverse_frequency_weight" href="#vipy.dataset.Dataset.multilabel_inverse_frequency_weight">multilabel_inverse_frequency_weight</a></code></li>
<li><code><a title="vipy.dataset.Dataset.num_categories" href="#vipy.dataset.Dataset.num_categories">num_categories</a></code></li>
<li><code><a title="vipy.dataset.Dataset.num_classes" href="#vipy.dataset.Dataset.num_classes">num_classes</a></code></li>
<li><code><a title="vipy.dataset.Dataset.num_labels" href="#vipy.dataset.Dataset.num_labels">num_labels</a></code></li>
<li><code><a title="vipy.dataset.Dataset.percentage" href="#vipy.dataset.Dataset.percentage">percentage</a></code></li>
<li><code><a title="vipy.dataset.Dataset.powerset" href="#vipy.dataset.Dataset.powerset">powerset</a></code></li>
<li><code><a title="vipy.dataset.Dataset.powerset_to_index" href="#vipy.dataset.Dataset.powerset_to_index">powerset_to_index</a></code></li>
<li><code><a title="vipy.dataset.Dataset.replace" href="#vipy.dataset.Dataset.replace">replace</a></code></li>
<li><code><a title="vipy.dataset.Dataset.save" href="#vipy.dataset.Dataset.save">save</a></code></li>
<li><code><a title="vipy.dataset.Dataset.shuffle" href="#vipy.dataset.Dataset.shuffle">shuffle</a></code></li>
<li><code><a title="vipy.dataset.Dataset.sort" href="#vipy.dataset.Dataset.sort">sort</a></code></li>
<li><code><a title="vipy.dataset.Dataset.split" href="#vipy.dataset.Dataset.split">split</a></code></li>
<li><code><a title="vipy.dataset.Dataset.synonym" href="#vipy.dataset.Dataset.synonym">synonym</a></code></li>
<li><code><a title="vipy.dataset.Dataset.take" href="#vipy.dataset.Dataset.take">take</a></code></li>
<li><code><a title="vipy.dataset.Dataset.take_per_category" href="#vipy.dataset.Dataset.take_per_category">take_per_category</a></code></li>
<li><code><a title="vipy.dataset.Dataset.takefilter" href="#vipy.dataset.Dataset.takefilter">takefilter</a></code></li>
<li><code><a title="vipy.dataset.Dataset.takelist" href="#vipy.dataset.Dataset.takelist">takelist</a></code></li>
<li><code><a title="vipy.dataset.Dataset.to_torch" href="#vipy.dataset.Dataset.to_torch">to_torch</a></code></li>
<li><code><a title="vipy.dataset.Dataset.to_torch_tensordir" href="#vipy.dataset.Dataset.to_torch_tensordir">to_torch_tensordir</a></code></li>
<li><code><a title="vipy.dataset.Dataset.tocsv" href="#vipy.dataset.Dataset.tocsv">tocsv</a></code></li>
<li><code><a title="vipy.dataset.Dataset.tohtml" href="#vipy.dataset.Dataset.tohtml">tohtml</a></code></li>
<li><code><a title="vipy.dataset.Dataset.tojsondir" href="#vipy.dataset.Dataset.tojsondir">tojsondir</a></code></li>
<li><code><a title="vipy.dataset.Dataset.tolist" href="#vipy.dataset.Dataset.tolist">tolist</a></code></li>
<li><code><a title="vipy.dataset.Dataset.union" href="#vipy.dataset.Dataset.union">union</a></code></li>
<li><code><a title="vipy.dataset.Dataset.valid" href="#vipy.dataset.Dataset.valid">valid</a></code></li>
<li><code><a title="vipy.dataset.Dataset.video_duration_in_seconds" href="#vipy.dataset.Dataset.video_duration_in_seconds">video_duration_in_seconds</a></code></li>
<li><code><a title="vipy.dataset.Dataset.video_montage" href="#vipy.dataset.Dataset.video_montage">video_montage</a></code></li>
<li><code><a title="vipy.dataset.Dataset.zip" href="#vipy.dataset.Dataset.zip">zip</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>