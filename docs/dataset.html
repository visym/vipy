<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.11.6" />
<meta name="google-site-verification" content="aB8LkQegj94_TJPdrcJm2ldIRWyXY82Jp24Gtkdgyn0" />
<title>vipy.dataset API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}#lunr-search{width:100%;font-size:1em;padding:6px 9px 5px 9px;border:1px solid silver}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>vipy.dataset</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/visym/vipy/blob/095a3bb922809af0de26f3bbf8f0fb8f9c4157a4/vipy/dataset.py#L1-L928" class="git-link">Browse git</a>
</summary>
<pre><code class="python">import os
import numpy as np
from vipy.util import findpkl, toextension, filepath, filebase, jsonlist, ishtml, ispkl, filetail, temphtml, env, cache
from vipy.util import listpkl, listext, templike, tempdir, remkdir, tolist, fileext, writelist, tempcsv, to_iterable
from vipy.util import newpathroot, listjson, extlist, filefull, tempdir, groupbyasdict, try_import, shufflelist, catcher
from vipy.util import is_email_address, findjson, findimages, isimageurl, countby, chunkgen, chunkgenbysize, dividelist, chunklist
from vipy.util import findvideos, truncate_string
from vipy.globals import log
import random
import shutil
import copy 
import gc 
import itertools
from pathlib import Path
import functools
import concurrent.futures as cf
import vipy.parallel


class Dataset():
    &#34;&#34;&#34;vipy.dataset.Dataset() class
    
    Common class to manipulate large sets of objects in parallel

    Args:
        - dataset [list, tuple, set, obj]: a python built-in type that supports indexing or a generic object that supports indexing and has a length
        - id [str]: an optional id of this dataset, which provides a descriptive name of the dataset
        - loader [callable]: a callable loader that will construct the object from a raw data element in dataset.  This is useful for custom deerialization or on demand transformations

    Datasets can be indexed, shuffled, iterated, minibatched, sorted, sampled, partitioned.
    Datasets constructed of vipy objects are lazy loaded, delaying loading pixels until they are needed

    ```python
    (trainset, valset, testset) = vipy.dataset.registry(&#39;mnist&#39;)

    (trainset, valset) = trainset.partition(0.9, 0.1)
    categories = trainset.set(lambda im: im.category())
    smaller = testset.take(1024)
    preprocessed = smaller.map(lambda im: im.resize(32, 32).gain(1/256))
    
    for b in preprocessed.minibatch(128):
        print(b)

    # visualize the dataset 
    (trainset, valset, testset) = vipy.dataset.registry(&#39;pascal_voc_2007&#39;)
    for im in trainset:
        im.mindim(1024).show().print(sleep=1).close()
    
    ```

    Datasets can be constructed from directories of json files or image files (`vipy.dataset.Dataset.from_directory`)
    Datasets can be constructed from a single json file containing a list of objects (`vipy.dataset.Dataset.from_json`)
    
    ..note:: that if a lambda function is provided as loader then this dataset is not serializable.  Use self.load() then serialize
    &#34;&#34;&#34;

    __slots__ = (&#39;_id&#39;, &#39;_ds&#39;, &#39;_idx&#39;, &#39;_loader&#39;, &#39;_type&#39;)
    def __init__(self, dataset, id=None, loader=None):
        assert loader is None or callable(loader)
        
        self._id = id
        self._ds = dataset if not isinstance(dataset, Dataset) else dataset._ds
        self._idx = None if not isinstance(dataset, Dataset) else dataset._idx   # random access on-demand
        self._loader = loader if not isinstance(dataset, Dataset) else dataset._loader  # not serializable if lambda is provided

        try:
            self._type = str(type(self._loader(dataset[0]) if self._loader else dataset[0]))  # peek at first element, cached
        except:
            self._type = None


    @classmethod
    def from_directory(cls, indir, filetype=&#39;json&#39;, id=None):
        &#34;&#34;&#34;Recursively search indir for filetype, construct a dataset from all discovered files of that type&#34;&#34;&#34;
        if filetype == &#39;json&#39;:
            return cls([x for f in findjson(indir) for x in to_iterable(vipy.load(f))], id=id)
        elif filetype.lower() in [&#39;jpg&#39;,&#39;jpeg&#39;,&#39;images&#39;]:
            return cls([vipy.image.Image(filename=f) for f in findimages(indir)], id=id)            
        elif filetype.lower() in [&#39;mp4&#39;,&#39;videos&#39;]:
            return cls([vipy.image.Video(filename=f) for f in findvideos(indir)], id=id)            
        else:
            raise ValueError(&#39;unsupported file type &#34;%s&#34;&#39; % filetype)

    @classmethod
    def from_image_urls(cls, urls, id=None):
        &#34;&#34;&#34;Construct a dataset from a list of image URLs&#34;&#34;&#34;
        return cls([vipy.image.Image(url=url) for url in to_iterable(urls) if isimageurl(url)], id=id)
        
    @classmethod
    def from_json(cls, jsonfile, id=None):
        return cls([x for x in to_iterable(vipy.load(jsonfile))], id=id)

    @classmethod
    def cast(cls, obj):
        return cls(obj) if not isinstance(obj, Dataset) else obj
    
    def __repr__(self):
        fields = [&#39;id=%s&#39; % truncate_string(self.id(), maxlen=80)] if self.id() else []
        fields += [&#39;len=%d&#39; % self.len()] if self.len() is not None else []
        fields += [&#39;type=%s&#39; % self._type] if self._type else []
        return str(&#39;&lt;vipy.dataset.Dataset: %s&gt;&#39; % &#39;, &#39;.join(fields))

    def __iter__(self):            
        if self.is_streaming():
            for x in self._ds:  # iterable access (faster)
                yield self._loader(x) if self._loader is not None else x                 
        else:
            for k in range(len(self)):
                yield self[k]   # random access (slower)                


    def __getitem__(self, k):
        assert self.len() is not None, &#34;dataset does not support indexing&#34;
        
        idx = self.index()  # convert to random access on demand
        if isinstance(k, (int, np.uint64)):
            assert abs(k) &lt; len(idx), &#34;invalid index&#34;
            x = self._ds[idx[int(k)]]
            x = self._loader(x) if self._loader is not None else x
            return x
        elif isinstance(k, slice):
            X = [self._ds[k] for k in idx[k.start:k.stop:k.step]]
            X = [self._loader(x) for x in X] if self._loader is not None else X
            return X
        else:
            raise ValueError(&#39;invalid slice &#34;%s&#34;&#39; % type(k))            

    def raw(self):
        &#34;&#34;&#34;Return a view of this dataset without the loader&#34;&#34;&#34;
        return Dataset(self._ds, loader=None)
    
    def is_streaming(self):
        return self._idx is None

    def len(self):
        return len(self._idx) if self._idx is not None else (len(self._ds) if hasattr(self._ds, &#39;__len__&#39;) else None)

    def __len__(self):
        len = self.len()
        if len is None:
            raise ValueError(&#39;dataset has no length&#39;)
        return len
    
    def __or__(self, other):
        assert isinstance(other, Dataset)
        return Union(self, other, id=self.id())
    
    def id(self, new_id=None):
        &#34;&#34;&#34;Change the dataset ID to the provided ID, or return it if None&#34;&#34;&#34;
        if new_id is not None:
            self._id = new_id
            return self
        return self._id

    def index(self, index=None, strict=False):
        &#34;&#34;&#34;Update the index, useful for filtering of large datasets&#34;&#34;&#34;
        if index is not None:
            assert not strict or index is None or (len(index)&gt;0 and len(index)&lt;=len(self) and max(index)&lt;len(self) and min(index)&gt;=0)            
            self._idx = index
            return self
        if self._idx is None:
            self._idx = list(range(len(self._ds)))  # on-demand index, only if underlying dataset has known length
        return self._idx
    
        
    def clone(self, deep=False):
        &#34;&#34;&#34;Return a copy of the dataset object&#34;&#34;&#34;
        if not deep:
            return copy.copy(self) 
        else:
            return copy.deepcopy(self)
    
    def shuffle(self, shuffler=None):
        &#34;&#34;&#34;Permute elements in this dataset uniformly at random in place using the optimal shuffling strategy for the dataset structure to maximize performance.
           This method will use either Dataset.streaming_shuffler (for iterable datasets) or Dataset.uniform_shuffler (for random access datasets)
        &#34;&#34;&#34;
        assert shuffler is None or callable(shuffler)
        shuffler = shuffler if shuffler is not None else (Dataset.streaming_shuffler if self.is_streaming() else Dataset.uniform_shuffler)
        return shuffler(self)

    def repeat(self, n):
        &#34;&#34;&#34;Repeat the dataset n times.  If n=0, the dataset is unchanged, if n=1 the dataset is doubled in length, etc.&#34;&#34;&#34;
        assert n&gt;=0
        return self.index( self.index()*(n+1) )
    
    def tuple(self, mapper=None, flatten=False, reducer=None):
        &#34;&#34;&#34;Return the dataset as a tuple, applying the optional mapper lambda on each element, applying optional flattener on sequences returned by mapper, and applying the optional reducer lambda on the final tuple, return a generator&#34;&#34;&#34;
        assert mapper is None or callable(mapper)
        assert reducer is None or callable(reducer)
        mapped = self.map(mapper) if mapper else self
        flattened = (y for x in mapped for y in x) if flatten else (x for x in mapped)
        reduced = reducer(flattened) if reducer else flattened
        return reduced

    def list(self, mapper=None, flatten=False):
        &#34;&#34;&#34;Return a tuple as a list, loading into memory&#34;&#34;&#34;
        return self.tuple(mapper, flatten, reducer=list)

    def set(self, mapper=None, flatten=False):
        &#34;&#34;&#34;Return the dataset as a set.  Mapper must be a lambda function that returns a hashable type&#34;&#34;&#34;
        return self.tuple(mapper=mapper, reducer=set, flatten=flatten)        

    def all(self, mapper):
        return self.tuple(mapper=mapper, reducer=all)
    
    def frequency(self, f):
        &#34;&#34;&#34;Frequency counts for which lambda returns the same value.  For example f=lambda im: im.category() returns a dictionary of category names and counts in this category&#34;&#34;&#34;
        return countby(self.tuple(mapper=f))

    def balanced(self, f):
        &#34;&#34;&#34;Is the dataset balanced (e.g. the frequencies returned from the lambda f are all the same)?&#34;&#34;&#34;
        return len(set(self.frequency(f).values())) == 1
    
    def count(self, f):
        &#34;&#34;&#34;Counts for each element for which lamba returns true.  
        
        Args:
            f: [lambda] if provided, count the number of elements that return true.  

        Returns:
            A length of elements that satisfy f(v) = True [if f is not None]
        &#34;&#34;&#34;
        return len(self.tuple(f, reducer=lambda X: [x for x in X if x is True]))

    def countby(self, f):
        return self.frequency(f)
    
    def filter(self, f):
        &#34;&#34;&#34;In place filter with lambda function f, keeping those elements obj in-place where f(obj) evaluates true.  Callable should return bool&#34;&#34;&#34;
        assert callable(f)
        return self.index( [i for (b,i) in zip(self.localmap(f), self.index()) if b] )
    
    def take(self, n, inplace=False):
        &#34;&#34;&#34;Randomly Take n elements from the dataset, and return a dataset (in-place or cloned). If n is greater than the size of the dataset, sample with replacement, if n is less than the size of the dataset, sample without replacement&#34;&#34;&#34;
        assert isinstance(n, int) and n&gt;0
        D = self.clone() if not inplace else self
        return D.index(list((random.sample if n&lt;= len(self) else random.choices)(D.index(), k=n)) )


    def groupby(self, f):
        &#34;&#34;&#34;Group the dataset according to the callable f, returning dictionary of grouped datasets.&#34;&#34;&#34;
        assert callable(f)        
        return {k:self.clone().index([x[1] for x in v]).id(&#39;%s:%s&#39; % (self.id(),str(k))) for (k,v) in itertools.groupby(enumerate(self.sort(f).index()), lambda x: f(self[x[0]]))}

    def takeby(self, f, n):
        &#34;&#34;&#34;Filter the dataset according to the callable f, take n from each group and return a dataset.  Callable should return bool.  If n==1, return a singleton&#34;&#34;&#34;
        d = self.clone().filter(f)
        return d.take(n) if n&gt;1 else d.takeone()

    def takelist(self, n):
        &#34;&#34;&#34;Take n elements and return list.  The elements are loaded and not cloned.&#34;&#34;&#34;
        return self.take(n).list()

    def takeone(self):
        &#34;&#34;&#34;Randomly take one element from the dataset and return a singleton&#34;&#34;&#34;
        return self[random.randint(0, len(self)-1)]

    def sample(self):
        &#34;&#34;&#34;Return a single element sampled uniformly at random&#34;&#34;&#34;
        return self.takeone()
    
    def take_fraction(self, p, inplace=False):
        &#34;&#34;&#34;Randomly take a percentage of the dataset, returning a clone or in-place&#34;&#34;&#34;
        assert p&gt;=0 and p&lt;=1, &#34;invalid fraction &#39;%s&#39;&#34; % p
        return self.take(n=int(len(self)*p), inplace=inplace)

    def inverse_frequency(self, f):
        &#34;&#34;&#34;Return the inverse frequency of elements grouped by the callable f.  Returns a dictionary of the callable output to inverse frequency &#34;&#34;&#34;
        attributes = self.set(f)
        frequency = self.frequency(f)
        return {a:(1/len(attributes))*(len(self)/frequency[a]) for a in attributes}  # (normalized) inverse frequency weight
    
    def load(self):
        &#34;&#34;&#34;Cache the entire dataset into memory&#34;&#34;&#34;
        return Dataset([x for x in self], id=self.id())
    
    def chunk(self, n):
        &#34;&#34;&#34;Yield n chunks as list.  Last chunk will be ragged.&#34;&#34;&#34;
        for (k,c) in enumerate(chunkgen(self, n)):
            yield list(c)

    def batch(self, n):
        &#34;&#34;&#34;Yield batches of size n as datasets.  Last batch will be ragged.  Batches are not loaded.  Batches have appended id equal to the zero-indexed batch order&#34;&#34;&#34;
        for (k,b) in enumerate(chunkgenbysize(self, n)):  
            yield Dataset(b).id(&#39;%s:%d&#39; % (self.id() if self.id() else &#39;&#39;, k))
                                
    def minibatch(self, n, ragged=True, loader=None, bufsize=1024, accepter=None, preprocessor=None):
        &#34;&#34;&#34;Yield preprocessed minibatches of size n of this dataset.

        To yield chunks of this dataset, suitable for minibatch training/testing

        ```python
        D = vipy.dataset.Dataset(...)
        for b in D.minibatch(n):
           print(b)
        ```
        
        To perform minibatch image downloading in parallel across four processes with the context manager:

        ```python
        D = vipy.dataset.registry(&#39;yfcc100m_url:train&#39;).take(128)
        with vipy.globals.parallel(4):
            for b in D.minibatch(16, loader=vipy.image.Transform.download, accepter=lambda im: im.is_downloaded()):
                print(b)  # complete minibatch that passed accepter
        ```

        Args:
            n [int]: The size of the minibatch
            ragged [bool]: If ragged=true, then the last chunk will be ragged with len(chunk)&lt;n, else skipped
            bufsize [int]:  The size of the buffer used in parallel processing of elements.  Useful for parallel loading
            accepter [callable]:  A callable that returns true|false on an element, where only elements that return true are included in the minibatch.  useful for parallel loading of elements that may fail to download
            loader [callable]: A callable that is applied to every element of the dataset.  Useful for parallel loading

        Returns:        
            Iterator over `vipy.dataset.Dataset` elements of length n.  Minibatches will be yielded loaded and preprocessed (processing done concurrently if vipy.parallel.executor() is initialized)

        ..note:: The distributed iterator appends the minibatch index to the minibatch.id().  
        ..note:: If there exists a vipy.parallel.exeuctor(), then loading and preprocessing will be performed concurrently

        &#34;&#34;&#34;
        for (k,b) in enumerate(chunkgenbysize(vipy.parallel.iter(self, mapper=loader, bufsize=max(bufsize,n), accepter=accepter), n)): 
            if ragged or len(b) == n:
                yield Dataset.cast(b).id(&#39;%s:%d&#39; % (self.id() if self.id() else &#39;&#39;, k))                    
                    
                        
    def shift(self, m):
        &#34;&#34;&#34;Circular shift the dataset m elements to the left, so that self[k+m] == self.shift(m)[k].  Circular shift for boundary handling so that self.shift(m)[-1] == self[m-1]&#34;&#34;&#34;
        return self.clone().index(self.index()[m:] + self.index()[0:m])

    def slice(self, start=0, stop=-1, step=1):
        &#34;&#34;&#34;Slice the dataset to contain elements defined by slice(start, stop, step)&#34;&#34;&#34;
        return self.clone().index(self.index()[start:stop:step])
        
    def truncate(self, m):
        &#34;&#34;&#34;Truncate the dataset to contain the first m elements only&#34;&#34;&#34;
        return self.slice(stop=m)
    
    def pipeline(self, n, m, ragged=True, prepad=True, postpad=True):
        &#34;&#34;&#34;Yield pipelined minibatches of size n with pipeline length m.

        A pipelined minibatch is a tuple (head, tail) such that (head, tail) are minibatches at different indexes in the dataset.  
        Head corresponds to the current minibatch and tail corresponds to the minibatch left shifted by (m-1) minibatches.

        This structure is useful for yielding datasets for pipelined training where head contains the minibatch that will complete pipeline training on this iteration, and tail contains the 
        next minibatch to be inserted into the pipeline on this iteration.
        
        ```python
        D = vipy.dataset.Dataset(...)
        for (head, tail) in D.pipeline(n, m, prepad=False, postpad=False):
            assert head == D[0:m]
            assert tail == D[n*(m-1): n*(m-1)+n]

        Args:
            n [int]: The size of each minibatch
            m [int]:  The pipeline length in minibatches
            ragged [bool]: If ragged=true, then the last chunk will be ragged with len(chunk)&lt;n, else skipped
            prepad: If true, yield (head, tail) == (None, batch) when filling the pipeline
            postpad: If true, yield (head, tail) == (batch, None) when flushing the pipeline
        
        Returns:        
            Iterator over tuples (head,tail) of `vipy.dataset.Dataset` elements of length n where tail is left shifted by n*(m-1) elements. 
        
        .. note::  The distributed iterator is not order preserving over minibatches and yields minibatches as completed, however the tuple (head, tail) is order preserving within the pipeline
        .. note:: If there exists a vipy.parallel.executor(), then loading and preprocessing will be performed concurrently
        
        &#34;&#34;&#34;
        pipeline = [] 
        for (k,b) in enumerate(self.minibatch(n, ragged=ragged)):  # not order preserving
            pipeline.append(b)  # order preserving within pipeline                        
            if k &lt; m-1:
                if prepad:
                    yield( (None, b) )  
            else:
                yield( (pipeline.pop(0), b) )  # yield deque-like (minibatch, shifted minibatch) tuples
        for p in pipeline:
            if postpad:
                yield( (p, None) )


    def chunks(self, sizes):
        &#34;&#34;&#34;Partition the dataset into chunks of size given by the tuple in partitions, and give the dataset suffix if provided&#34;&#34;&#34;
        assert sum(sizes) == len(self)

        i = 0
        datasets = []
        for n in sizes:
            datasets.append(self.clone().index(self.index()[i:i+n]))
            i += n
        return datasets
        
    def partition(self, trainfraction=0.9, valfraction=0.1, testfraction=0, trainsuffix=&#39;:train&#39;, valsuffix=&#39;:val&#39;, testsuffix=&#39;:test&#39;):
        &#34;&#34;&#34;Partition the dataset into the requested (train,val,test) fractions.  

        Args:
            trainfraction [float]: fraction of dataset for training set
            valfraction [float]: fraction of dataset for validation set
            testfraction [float]: fraction of dataset for test set
            trainsuffix: If not None, append this string the to trainset ID
            valsuffix: If not None, append this string the to valset ID
            testsuffix: If not None, append this string the to testset ID        
        
        Returns:        
            (trainset, valset, testset) such that trainset is the first trainfraction of the dataset.  

        .. note:: This does not permute the dataset.  To randomize split, shuffle dataset first

        &#34;&#34;&#34;
        assert trainfraction &gt;=0 and trainfraction &lt;= 1, &#34;invalid training set fraction &#39;%f&#39;&#34; % trainfraction
        assert valfraction &gt;=0 and valfraction &lt;= 1, &#34;invalid validation set fraction &#39;%f&#39;&#34; % valfraction
        assert testfraction &gt;=0 and testfraction &lt;= 1, &#34;invalid test set fraction &#39;%f&#39;&#34; % testfraction
        assert abs(trainfraction + valfraction + testfraction - 1) &lt; 1E-6, &#34;fractions must sum to one&#34;
        
        idx = self.index()
        (testidx, validx, trainidx) = dividelist(idx, (testfraction, valfraction, trainfraction))
            
        trainset = self.clone().index(trainidx)
        if trainsuffix and trainset.id():
            trainset.id(trainset.id() + trainsuffix)
        
        valset = self.clone().index(validx)
        if valsuffix and valset.id():
            valset.id(valset.id() + valsuffix)
        
        testset = self.clone().index(testidx)
        if testsuffix and testset.id():
            testset.id(testset.id() + testsuffix)
                
        return (trainset,valset,testset) if testfraction!=0 else (trainset, valset)

    def split(self, size):
        &#34;&#34;&#34;Split the dataset into two datasets, one of length size, the other of length len(self)-size&#34;&#34;&#34;
        assert isinstance(size, int) and size&gt;=0 and size&lt;len(self)
        return self.partition(size/len(self), (len(self)-size)/len(self), 0, &#39;&#39;, &#39;&#39;, &#39;&#39;)

    def even_split(self):
        &#34;&#34;&#34;Split the dataset into two datasets, each half the size of the dataset.  If the dataset length is odd, then one element will be dropped&#34;&#34;&#34;
        return self.chunks((len(self)//2, len(self)//2, len(self)%2))[0:2]
        
    def streaming_map(self, mapper, accepter=None, bufsize=1024):
        &#34;&#34;&#34;Returns a generator that will apply the mapper and yield only those elements that return True from the accepter.  Performs the map in parallel if used in the vipy.globals.parallel context manager&#34;&#34;&#34;
        return vipy.parallel.iter(self, mapper=mapper, accepter=accepter, bufsize=bufsize)
        
    def map(self, f_map, strict=True, oneway=False, ordered=False):        
        &#34;&#34;&#34;Parallel map.

        To perform this in parallel across four threads:

        ```python
        D = vipy.dataset.Dataset(...)
        with vipy.globals.parallel(4):
            D = D.map(lambda v: ...)
        ```

        Args:
            f_map: [lambda] The lambda function to apply in parallel to all elements in the dataset.  This must return a JSON serializable object (or set oneway=True)
            strict: [bool] If true, raise exception on distributed map failures, otherwise the map will return only those that succeeded
            oneway: [bool] If true, do not pass back results unless exception.  This is useful for distributed processing
        
        Returns:
            A `vipy.dataset.Dataset` containing the elements f_map(v).  This operation is order preserving if ordered=True.

        .. note:: 
            - This method uses dask distributed and `vipy.batch.Batch` operations
            - Due to chunking, all error handling is caught by this method.  Use `vipy.batch.Batch` to leverage dask distributed futures error handling.
            - Operations must be chunked and serialized because each dask task comes with overhead, and lots of small tasks violates best practices
            - Serialized results are deserialized by the client and returned a a new dataset
        &#34;&#34;&#34;
        assert f_map is None or callable(f_map), &#34;invalid map function&#34;

        # Identity
        if f_map is None:
            return self        

        # Parallel map 
        elif vipy.globals.cf() is not None:
            # This will fail on multiprocessing if dataset contains a loader lambda, or any element in the dataset contains a loader.  Use distributed instead
            assert ordered == False, &#34;not order preserving, use localmap()&#34;
            return Dataset(tuple(vipy.parallel.map(f_map, self)), id=self.id()) 
                                              
        # Distributed map
        elif vipy.globals.dask() is not None:
            from vipy.batch import Batch   # requires pip install vipy[all]                
            f_serialize = lambda x: x
            f_deserialize = lambda x: x
            f_oneway = lambda x, oneway=oneway: x if not x[0] or not oneway else (x[0], None)
            f_catcher = lambda f, *args, **kwargs: catcher(f, *args, **kwargs)  # catch exceptions when executing lambda, return (True, result) or (False, exception)
            f = lambda x, f_serializer=f_serialize, f_deserializer=f_deserialize, f_map=f_map, f_catcher=f_catcher, f_oneway=f_oneway: f_serializer(f_oneway(f_catcher(f_map, f_deserializer(x))))  # with closure capture
            
            S = [f_serialize(v) for v in self]  # local load, preprocess and serialize
            B = Batch(chunklist(S, 128), strict=False, warnme=False, minscatter=128)
            S = B.map(lambda X,f=f: [f(x) for x in X]).result()  # distributed, chunked, with caught exceptions, may return empty list
            V = [f_deserialize(x) for s in S for x in s]  # Local deserialization and chunk flattening
            
            # Error handling
            (good, bad) = ([r for (b,r) in V if b], [r for (b,r) in V if not b])  # catcher returns (True, result) or (False, exception string)
            if len(bad)&gt;0:
                log.warning(&#39;Exceptions in distributed processing:\n%s\n\n[vipy.dataset.Dataset.map]: %d/%d items failed&#39; % (str(bad), len(bad), len(self)))
                if strict:
                    raise ValueError(&#39;exceptions in distributed processing&#39;)
            return Dataset(good, id=self.id()) if not oneway else None

        # Local map
        else:
            return self.localmap(f_map)
        
    def localmap(self, f):
        &#34;&#34;&#34;A map performed without any parallel processing&#34;&#34;&#34;
        return Dataset([f(x) for x in self], id=self.id())  # triggers load into memory        

    def zip(self, iter):
        &#34;&#34;&#34;Returns a new dataset constructed by applying the callable on elements from zip(self,iter)&#34;&#34;&#34;
        return Dataset(zip(self,iter))
    
    def sort(self, f):
        &#34;&#34;&#34;Sort the dataset in-place using the sortkey lambda function f

        To perform a sort of the dataset using some property of the instance, such as the object category (e.g. for vipy.image.ImageCategory) 

        ```python
        dataset.sort(lambda im: im.category())
        ```
        &#34;&#34;&#34;
        idx = self.index()
        return self.index( [idx[j] for (j,x) in sorted(zip(range(len(self)), self.tuple(f)), key=lambda x: x[1])] )

    
    @staticmethod
    def uniform_shuffler(D):
        &#34;&#34;&#34;A uniform shuffle on the dataset elements.  Iterable access will be slow due to random access&#34;&#34;&#34;
        idx = D.index()
        random.shuffle(idx)
        return D.index(idx)

    @staticmethod
    def streaming_shuffler(D):
        &#34;&#34;&#34;A uniform shuffle (approximation) on the dataset elements for iterable access only&#34;&#34;&#34;
        assert D._idx is None, &#34;streaming only&#34;
        
        try_import(&#39;datasets&#39;, &#39;datasets&#39;); from datasets import Dataset as HuggingfaceDataset;
        
        if isinstance(D._ds, (list, tuple)):
            D._ds = list(D._ds)
            random.shuffle(D._ds)  # in-place shuffle objects
                
        elif isinstance(D._ds, HuggingfaceDataset):
            # Special case: Arrow backed dataset            
            D._ds = D._ds.to_iterable_dataset()  # no random access
            D._ds.shuffle()  # approximate shuffling for IterableDataset is much more efficient for __iter__
        else:
            raise ValueError(&#39;shuffle error&#39;)
        return D
    
    @staticmethod
    def identity_shuffler(D):
        &#34;&#34;&#34;Shuffler that does nothing&#34;&#34;&#34;
        return D


class Paged(Dataset):
    &#34;&#34;&#34; Paged dataset.

    A paged dataset is a dataset of length N=M*P constructed from M archive files (the pages) each containing P elements (the pagesize).  
    The paged dataset must be constructed with tuples of (pagesize, filename).  
    The loader will fetch, load and cache the pages on demand using the loader, preserving the most recently used cachesize pages

    ```python
    D = vipy.dataset.Paged([(64, &#39;archive1.pkl&#39;), (64, &#39;archive2.pkl&#39;)], lambda x,y: ivy.load(y))
    ```

    .. note :: Shuffling this dataset is biased.  Shuffling will be performed to mix the indexes, but not uniformly at random.  The goal is to preserve data locality to minimize cache misses.
    &#34;&#34;&#34;
    
    def __init__(self, pagelist, loader, id=None, strict=True, index=None, cachesize=32):        
        super().__init__(dataset=pagelist,
                         id=id,
                         loader=loader).index(index if index else list(range(sum([p[0] for p in pagelist]))))

        assert callable(loader), &#34;page loader required&#34;
        assert not strict or len(set([x[0] for x in self._ds])) == 1  # pagesizes all the same 

        self._cachesize = cachesize
        self._pagecache = {}
        self._ds = list(self._ds)
        self._pagesize = self._ds[0][0]  # (pagesize, pklfile) tuples        

    def shuffle(self, shuffler=None):
        &#34;&#34;&#34;Permute elements while preserve page locality to minimize cache misses&#34;&#34;&#34;
        shuffler = shuffler if shuffler is not None else functools.partial(Paged.chunk_shuffler, chunksize=int(1.5*self._pagesize))
        return shuffler(self)
        
    def __getitem__(self, k):
        if isinstance(k, (int, np.uint64)):
            assert abs(k) &lt; len(self._idx), &#34;invalid index&#34;
            page = self._idx[int(k)] // self._pagesize
            if page not in self._pagecache:
                self._pagecache[page] = self._loader(*self._ds[page])  # load and cache new page
                if len(self._pagecache) &gt; self._cachesize:
                    self._pagecache.pop(list(self._pagecache.keys())[0])  # remove oldest
            x = self._pagecache[page][int(k) % self._pagesize]
            return x
        elif isinstance(k, slice):
            return [self[i] for i in range(len(self))[k.start if k.start else 0:k.stop if k.stop else len(self):k.step if k.step else 1]]  # expensive
        else:
            raise ValueError(&#39;invalid index type &#34;%s&#34;&#39; % type(k))            

    def flush(self):
        self._pagecache = {}
        return self

    @staticmethod
    def chunk_shuffler(D, chunker, chunksize=64):
        &#34;&#34;&#34;Split dataset into len(D)/chunksize non-overlapping chunks with some common property returned by chunker, shuffle chunk order and shuffle within chunks.  

           - If chunksize=1 then this is equivalent to uniform_shuffler
           - chunker must be a callable of some property that is used to group into chunks
            
        &#34;&#34;&#34;
        assert callable(chunker)
        return D.randomize().sort(chunker).index([i for I in shufflelist([shufflelist(I) for I in chunkgenbysize(D.index(), chunksize)]) for i in I])
    
    
class Union(Dataset):
    &#34;&#34;&#34;vipy.dataset.Union() class
    
    Common class to manipulate groups of vipy.dataset.Dataset objects in parallel

    Usage:
    
        &gt;&gt;&gt; cifar10 = vipy.dataset.registry(&#39;cifar10&#39;)
        &gt;&gt;&gt; mnist = vipy.dataset.registry(&#39;mnist&#39;)
        &gt;&gt;&gt; dataset = vipy.dataset.Union(mnist, cifar10)
        &gt;&gt;&gt; dataset = mnist | cifar10

    Args:
        Datasets 
    &#34;&#34;&#34;

    __slots__ = (&#39;_id&#39;, &#39;_ds&#39;, &#39;_idx&#39;, &#39;_loader&#39;, &#39;_type&#39;)    
    def __init__(self, *args, **kwargs):
        assert all(isinstance(d, (Dataset, )) for d in args), &#34;invalid datasets&#34;
        
        datasets = [d for d in args]  # order preserving
        assert all([isinstance(d, Dataset) for d in datasets]), &#34;Invalid datasets &#39;%s&#39;&#34; % str([type(d) for d in datasets])

        datasets = [j for i in datasets for j in (i.datasets() if isinstance(i, Union) else (i,))]  # flatten unions        
        self._ds = datasets
        self._idx = None
        self._id = kwargs[&#39;id&#39;] if &#39;id&#39; in kwargs else None

        self._loader = None  # individual datasets have loaders
        self._type = None

    def is_streaming(self):
        return self._idx is None and all(d.is_streaming() for d in self.datasets())

    def __len__(self):
        return sum(d.__len__() for d in self.datasets()) if self._idx is None else len(self._idx)
    
    def __iter__(self):
        if self.is_streaming():
            k = -1
            iter = [d.__iter__() for d in self.datasets()]  # round-robin

            for m in range(len(self.datasets())):
                try:
                    while True:
                        k = (k + 1) % len(iter)                                    
                        yield next(iter[k])  # assumes ordered
                except StopIteration:
                    iter.pop(k)
                    k -= 1
            
        else:
            self.index()  # force random access                    
            for (i,j) in self._idx:
                yield self._ds[i][j]  # random access (slower)                

    def __getitem__(self, k):
        self.index()  # force random access        
        if isinstance(k, (int, np.uint64)):
            assert abs(k) &lt; len(self._idx), &#34;invalid index&#34;
            (i,j) = self._idx[int(k)]            
            return self._ds[i][j]
        elif isinstance(k, slice):
            return [self._ds[i][j] for (i,j) in self._idx[k.start:k.stop:k.step]]
        else:
            raise ValueError(&#39;invalid index type &#34;%s&#34;&#39; % type(k))

    def __repr__(self):
        fields = [&#39;id=%s&#39; % truncate_string(self.id(), maxlen=64)] if self.id() else []
        fields += [&#39;len=%d&#39; % len(self)]
        fields += [&#39;union=%s&#39; % str(tuple([truncate_string(d.id(), maxlen=80) for d in self._ds]))]
        return str(&#39;&lt;vipy.dataset.Dataset: %s&gt;&#39; % (&#39;, &#39;.join(fields)))

    def index(self, index=None, strict=False):
        &#34;&#34;&#34;Update the index, useful for filtering of large datasets&#34;&#34;&#34;
        if index is not None:
            self._idx = index
            return self
        if self._idx is None:
            # Index on-demand: zipped (dataset index, element index) tuples, in round-robin dataset order [(0,0),(1,0),...,(0,n),(1,n),...]            
            lengths = [len(d) for d in self.datasets()]            
            self._idx = [c for r in [[(i,j) for i in range(len(self.datasets()))] for j in range(max(lengths))] for c in r if c[1]&lt;lengths[c[0]]]
        return self._idx
    
    def clone(self, deep=False):
        &#34;&#34;&#34;Return a copy of the dataset object&#34;&#34;&#34;
        D = super().clone(deep=deep)
        D._ds =  [d.clone(deep=deep) for d in D._ds]
        return D
    
    def datasets(self):
        &#34;&#34;&#34;Return the dataset union elements, useful for generating unions of unions&#34;&#34;&#34;
        return list(self._ds)

    def shuffle(self, shuffler=None):
        &#34;&#34;&#34;Permute elements in this dataset uniformly at random in place using the best shuffler for the dataset structure&#34;&#34;&#34;
        shuffler = shuffler if shuffler is not None else (Union.streaming_shuffler if self.is_streaming() else Dataset.uniform_shuffler)
        return shuffler(self)
    
    @staticmethod
    def streaming_shuffler(D):
        &#34;&#34;&#34;A uniform shuffle (approximation) on the dataset elements for iterable access only&#34;&#34;&#34;
        assert D._idx is None, &#34;iterable dataset only&#34;
        D._ds = [Dataset.streaming_shuffler(d) for d in D._ds]  # shuffle dataset shards
        random.shuffle(D._ds)  # shuffle union order
        return D
    

def registry(name=None, datadir=None, freeze=True, clean=False, download=False, split=&#39;train&#39;):
    &#34;&#34;&#34;Common entry point for loading datasets by name.

    Usage:
    
        &gt;&gt;&gt; trainset = vipy.dataset.registry(&#39;cifar10&#39;, split=&#39;train&#39;)             # return a training split
        &gt;&gt;&gt; valset = vipy.dataset.registry(&#39;cifar10:val&#39;, datadir=&#39;/tmp/cifar10&#39;)  # download to a custom location
        &gt;&gt;&gt; datasets = vipy.dataset.registry((&#39;cifar10:train&#39;,&#39;cifar100:train&#39;))   # return a union
        &gt;&gt;&gt; vipy.dataset.registry()                                                # print allowable datasets

    Args:
       name [str]: The string name for the dataset.  If tuple, return a `vipy.dataset.Union`.  If None, return the list of registered datasets.  Append name:train, name:val, name:test to output the requested split, or use the split keyword.
       datadir [str]: A path to a directory to store data.  Defaults to environment variable VIPY_DATASET_REGISTRY_HOME (then VIPY_CACHE if not found).  Also uses HF_HOME for huggingface datasets.  Datasets will be stored in datadir/name
       freeze [bool]:  If true, disable reference cycle counting for the loaded object (which will never contain cycles anyway) 
       clean [bool]: If true, force a redownload of the dataset to correct for partial download errors
       download [bool]: If true, force a redownload of the dataset to correct for partial download errors.  This is a synonym for clean=True
       split [str]: return &#39;train&#39;, &#39;val&#39; or &#39;test&#39; split.  If None, return (trainset, valset, testset) tuple

    Datasets:
       &#39;mnist&#39;,&#39;cifar10&#39;,&#39;cifar100&#39;,&#39;caltech101&#39;,&#39;caltech256&#39;,&#39;oxford_pets&#39;,&#39;sun397&#39;, &#39;food101&#39;,&#39;stanford_dogs&#39;,
       &#39;flickr30k&#39;,&#39;oxford_fgvc_aircraft&#39;,&#39;oxford_flowers_102&#39;,&#39;eurosat&#39;,&#39;d2d&#39;,&#39;ethzshapes&#39;,&#39;coil100&#39;,&#39;kthactions&#39;,
       &#39;yfcc100m&#39;,&#39;yfcc100m_url&#39;,&#39;tiny_imagenet&#39;,&#39;coyo300m&#39;,&#39;coyo700m&#39;,&#39;pascal_voc_2007&#39;,&#39;coco_2014&#39;, &#39;ava&#39;,
       &#39;activitynet&#39;, &#39;open_images_v7&#39;, &#39;imagenet&#39;, &#39;imagenet21k&#39;, &#39;visualgenome&#39; ,&#39;widerface&#39;,&#39;meva_kf1&#39;,
       &#39;objectnet&#39;,&#39;lfw&#39;,&#39;inaturalist_2021&#39;,&#39;kinetics&#39;,&#39;hmdb&#39;,&#39;places365&#39;,&#39;ucf101&#39;,&#39;lvis&#39;,&#39;kitti&#39;,
       &#39;imagenet_localization&#39;,&#39;laion2b&#39;,&#39;datacomp_1b&#39;,&#39;imagenet2014_det&#39;,&#39;imagenet_faces&#39;,&#39;youtubeBB&#39;,
       &#39;pip_370k&#39;,&#39;pip_175k&#39;,&#39;cap&#39;,&#39;cap_pad&#39;,&#39;cap_detection&#39;,&#39;tiny_virat&#39;

    Returns:
       (trainset, valset, testset) tuple where each is a `vipy.dataset.Dataset` or None, or a single split if name has a &#34;:SPLIT&#34; suffix or split kwarg provided
    &#34;&#34;&#34;
    
    import vipy.data

    datasets = (&#39;mnist&#39;,&#39;cifar10&#39;,&#39;cifar100&#39;,&#39;caltech101&#39;,&#39;caltech256&#39;,&#39;oxford_pets&#39;,&#39;sun397&#39;, &#39;stanford_dogs&#39;,&#39;coil100&#39;,
                &#39;flickr30k&#39;,&#39;oxford_fgvc_aircraft&#39;,&#39;oxford_flowers_102&#39;, &#39;food101&#39;, &#39;eurosat&#39;,&#39;d2d&#39;,&#39;ethzshapes&#39;,&#39;kthactions&#39;,
                &#39;yfcc100m&#39;,&#39;yfcc100m_url&#39;,&#39;tiny_imagenet&#39;,&#39;coyo300m&#39;,&#39;coyo700m&#39;,&#39;pascal_voc_2007&#39;,&#39;coco_2014&#39;, &#39;ava&#39;,
                &#39;activitynet&#39;,&#39;open_images_v7&#39;,&#39;imagenet&#39;,&#39;imagenet21k&#39;,&#39;visualgenome&#39;,&#39;widerface&#39;, &#39;youtubeBB&#39;,
                &#39;objectnet&#39;,&#39;lfw&#39;,&#39;inaturalist_2021&#39;,&#39;kinetics&#39;,&#39;hmdb&#39;,&#39;places365&#39;,&#39;ucf101&#39;,&#39;kitti&#39;,&#39;meva_kf1&#39;,
                &#39;lvis&#39;,&#39;imagenet_localization&#39;,&#39;laion2b&#39;,&#39;datacomp_1b&#39;,&#39;imagenet2014_det&#39;,&#39;imagenet_faces&#39;,
                &#39;pip_175k&#39;,&#39;pip_370k&#39;,&#39;cap&#39;,&#39;cap_pad&#39;,&#39;cap_detection&#39;,&#39;tiny_virat&#39;)  # Add to docstring too...
    
    if name is None:
        return tuple(sorted(datasets))
    if isinstance(name, (tuple, list)):
        assert all(n.startswith(datasets) for n in name)
        assert split is not None or all(&#39;:&#39; in n for n in name)
        return Union(*(registry(n, datadir=datadir, freeze=freeze, clean=clean, download=download, split=split) for n in name))    
    
    (name, split) = name.split(&#39;:&#39;,1) if name.count(&#39;:&#39;)&gt;0 else (name, split)
    if name not in datasets:
        raise ValueError(&#39;unknown dataset &#34;%s&#34; - choose from &#34;%s&#34;&#39; % (name, &#39;, &#39;.join(sorted(datasets))))
    if split not in [None, &#39;train&#39;, &#39;test&#39;, &#39;val&#39;]:
        raise ValueError(&#39;unknown split &#34;%s&#34; - choose from &#34;%s&#34;&#39; % (split, &#39;, &#39;.join([str(None), &#39;train&#39;, &#39;test&#39;, &#39;val&#39;])))

    datadir = remkdir(datadir if datadir is not None else (env(&#39;VIPY_DATASET_REGISTRY_HOME&#39;) if &#39;VIPY_DATASET_REGISTRY_HOME&#39; in env() else cache()))
    namedir = Path(datadir)/name    
    if (clean or download) and name in datasets and os.path.exists(namedir):
        log.info(&#39;Removing cached dataset &#34;%s&#34;&#39; % namedir)
        shutil.rmtree(namedir)  # delete cached subtree to force redownload ...
        
    if freeze:
        gc.disable()
        
    (trainset, valset, testset) = (None, None, None)    
    if name == &#39;mnist&#39;:
        (trainset, testset) = vipy.data.hf.mnist()        
    elif name == &#39;cifar10&#39;:
        (trainset, testset) = vipy.data.hf.cifar10()        
    elif name == &#39;cifar100&#39;:
        (trainset, testset) = vipy.data.hf.cifar100()        
    elif name == &#39;caltech101&#39;:
        trainset = vipy.data.caltech101.Caltech101(namedir)        
    elif name == &#39;caltech256&#39;:
        trainset = vipy.data.caltech256.Caltech256(namedir)
    elif name == &#39;oxford_pets&#39;:
        (trainset, testset) = vipy.data.hf.oxford_pets()
    elif name == &#39;sun397&#39;:
        (trainset, valset, testset) = vipy.data.hf.sun397()
    elif name == &#39;stanford_dogs&#39;:
        trainset = vipy.data.stanford_dogs.StanfordDogs(namedir)
    elif name == &#39;food101&#39;:
        trainset = vipy.data.food101.Food101(namedir)
    elif name == &#39;eurosat&#39;:
        trainset = vipy.data.eurosat.EuroSAT(namedir)
    elif name == &#39;d2d&#39;:
        trainset = vipy.data.d2d.D2D(namedir)
    elif name == &#39;coil100&#39;:
        trainset = vipy.data.coil100.COIL100(namedir)
    elif name == &#39;kthactions&#39;:
        (trainset, testset) = vipy.data.kthactions.KTHActions(namedir).split()
    elif name == &#39;ethzshapes&#39;:
        trainset = vipy.data.ethzshapes.ETHZShapes(namedir)        
    elif name == &#39;flickr30k&#39;:
        trainset = vipy.data.hf.flickr30k()
    elif name == &#39;oxford_fgvc_aircraft&#39;:
        trainset = vipy.data.hf.oxford_fgvc_aircraft()
    elif name == &#39;oxford_flowers_102&#39;:
        trainset = vipy.data.oxford_flowers_102.Flowers102(namedir)
    elif name == &#39;yfcc100m&#39;:
        (trainset, _, valset, _) = vipy.data.hf.yfcc100m()  
    elif name == &#39;yfcc100m_url&#39;:
        (_, trainset, _, valset) = vipy.data.hf.yfcc100m()  
    elif name == &#39;tiny_imagenet&#39;:
        (trainset, valset) = vipy.data.hf.tiny_imagenet()
    elif name == &#39;coyo300m&#39;:
        trainset = vipy.data.hf.coyo300m()
    elif name == &#39;coyo700m&#39;:
        trainset = vipy.data.hf.coyo700m()
    elif name == &#39;datacomp_1b&#39;:
        trainset = vipy.data.hf.datacomp_1b()
    elif name == &#39;pascal_voc_2007&#39;:
        (trainset, valset, testset) = vipy.data.hf.pascal_voc_2007()
    elif name == &#39;coco_2014&#39;:
        trainset = vipy.data.coco.COCO_2014(namedir)
    elif name == &#39;ava&#39;:
        ava = vipy.data.ava.AVA(namedir)
        (trainset, valset) = (ava.trainset(), ava.valset())
    elif name == &#39;activitynet&#39;:
        activitynet = vipy.data.activitynet.ActivityNet(namedir)  # ActivityNet 200
        (trainset, valset, testset) = (activitynet.trainset(), activitynet.valset(), activitynet.testset())
    elif name == &#39;open_images_v7&#39;:
        trainset = vipy.data.openimages.open_images_v7(namedir)
    elif name == &#39;imagenet&#39;:
        imagenet = vipy.data.imagenet.Imagenet2012(namedir)
        (trainset, valset) = (imagenet.classification_trainset(), imagenet.classification_valset())
    elif name == &#39;imagenet_faces&#39;:
        trainset = vipy.data.imagenet.Imagenet2012(Path(datadir)/&#39;imagenet&#39;).faces()
    elif name == &#39;imagenet21k&#39;:
        trainset = vipy.data.imagenet.Imagenet21K(namedir)
    elif name == &#39;visualgenome&#39;:
        trainset = vipy.data.visualgenome.VisualGenome(namedir)  # visualgenome-1.4
    elif name == &#39;widerface&#39;:
        trainset = vipy.data.widerface.WiderFace(namedir, split=&#39;train&#39;)
        valset = vipy.data.widerface.WiderFace(namedir, split=&#39;val&#39;)
        testset = vipy.data.widerface.WiderFace(namedir, split=&#39;test&#39;)                                      
    elif name == &#39;objectnet&#39;:
        assert split is None or split == &#39;test&#39;, &#34;objectnet is a test set&#34;
        testset = vipy.data.objectnet.Objectnet(namedir)
    elif name == &#39;lfw&#39;:
        trainset = vipy.data.lfw.LFW(namedir)
    elif name == &#39;inaturalist_2021&#39;:
        dataset = vipy.data.inaturalist.iNaturalist2021(namedir)
        (trainset, valset) = (dataset.trainset(), dataset.valset())
    elif name == &#39;kinetics&#39;:
        dataset = vipy.data.kinetics.Kinetics700(namedir)  # Kinetics700
        (trainset, valset, testset) = (dataset.trainset(), dataset.valset(), dataset.testset())
    elif name == &#39;hmdb&#39;:
        trainset = vipy.dataset.Dataset(vipy.data.hmdb.HMDB(namedir).dataset(), id=&#39;hmdb&#39;)
    elif name == &#39;places365&#39;:
        places = vipy.data.places.Places365(namedir)
        (trainset, valset) = (places.trainset(), places.valset())
    elif name == &#39;ucf101&#39;:
        trainset = vipy.data.ucf101.UCF101(namedir)
    elif name == &#39;kitti&#39;:
        trainset = vipy.data.kitti.KITTI(namedir, split=&#39;train&#39;)
        testset = vipy.data.kitti.KITTI(namedir, split=&#39;test&#39;)                                      
    elif name == &#39;lvis&#39;:
        lvis = vipy.data.lvis.LVIS(namedir)
        (trainset, valset) = (lvis.trainset(), lvis.valset())
    elif name == &#39;imagenet_localization&#39;:
        trainset = vipy.data.imagenet.Imagenet2012(Path(datadir)/&#39;imagenet&#39;).localization_trainset()
    elif name == &#39;imagenet2014_det&#39;:
        imagenet2014_det = vipy.data.imagenet.Imagenet2014_DET(namedir)
        (trainset, valset, testset) = (imagenet2014_det.trainset(), imagenet2014_det.valset(), imagenet2014_det.testset())
    elif name == &#39;laion2b&#39;:
        trainset = vipy.data.hf.laion2b()
    elif name == &#39;youtubeBB&#39;:
        trainset = vipy.data.youtubeBB.YoutubeBB(namedir)
    elif name == &#39;meva_kf1&#39;:
        trainset = vipy.data.meva.KF1(namedir).dataset()  # consider using &#34;with vipy.globals.multiprocessing(pct=0.5):&#34;
    elif name == &#39;pip_175k&#39;:
        trainset = vipy.data.pip.PIP_175k(namedir)
    elif name == &#39;pip_370k&#39;:
        trainset = vipy.data.pip.PIP_370k_stabilized(namedir)
    elif name == &#39;cap&#39;:
        trainset = vipy.data.cap.CAP_classification_clip(namedir)
    elif name == &#39;cap_pad&#39;:
        trainset = vipy.data.cap.CAP_classification_pad(namedir)        
    elif name == &#39;cap_detection&#39;:
        trainset = vipy.data.cap.CAP_detection(namedir)
    elif name == &#39;tiny_virat&#39;:
        dataset = vipy.data.tiny_virat.TinyVIRAT(namedir)
        (trainset, valset, testset) = (dataset.trainset(), dataset.valset(), dataset.testset())
    else:
        raise ValueError(&#39;unknown dataset &#34;%s&#34; - choose from &#34;%s&#34;&#39; % (name, &#39;, &#39;.join(sorted(datasets))))
    
    if freeze:
        gc.enable()
        gc.collect()
        gc.freeze()  # python-3.7

    if split == &#39;train&#39;:
        return trainset
    elif split == &#39;val&#39;:
        return valset
    elif split == &#39;test&#39;:
        return testset
    else:
        return (trainset, valset, testset)</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="vipy.dataset.registry"><code class="name flex">
<span>def <span class="ident">registry</span></span>(<span>name=None, datadir=None, freeze=True, clean=False, download=False, split='train')</span>
</code></dt>
<dd>
<div class="desc"><p>Common entry point for loading datasets by name.</p>
<h2 id="usage">Usage</h2>
<pre><code class="language-python-repl">&gt;&gt;&gt; trainset = vipy.dataset.registry('cifar10', split='train')             # return a training split
&gt;&gt;&gt; valset = vipy.dataset.registry('cifar10:val', datadir='/tmp/cifar10')  # download to a custom location
&gt;&gt;&gt; datasets = vipy.dataset.registry(('cifar10:train','cifar100:train'))   # return a union
&gt;&gt;&gt; vipy.dataset.registry()                                                # print allowable datasets
</code></pre>
<h2 id="args">Args</h2>
<p>name [str]: The string name for the dataset.
If tuple, return a <code><a title="vipy.dataset.Union" href="#vipy.dataset.Union">Union</a></code>.
If None, return the list of registered datasets.
Append name:train, name:val, name:test to output the requested split, or use the split keyword.
datadir [str]: A path to a directory to store data.
Defaults to environment variable VIPY_DATASET_REGISTRY_HOME (then VIPY_CACHE if not found).
Also uses HF_HOME for huggingface datasets.
Datasets will be stored in datadir/name
freeze [bool]:
If true, disable reference cycle counting for the loaded object (which will never contain cycles anyway)
clean [bool]: If true, force a redownload of the dataset to correct for partial download errors
download [bool]: If true, force a redownload of the dataset to correct for partial download errors.
This is a synonym for clean=True
split [str]: return 'train', 'val' or 'test' split.
If None, return (trainset, valset, testset) tuple</p>
<h2 id="datasets">Datasets</h2>
<p>'mnist','cifar10','cifar100','caltech101','caltech256','oxford_pets','sun397', 'food101','stanford_dogs',
'flickr30k','oxford_fgvc_aircraft','oxford_flowers_102','eurosat','d2d','ethzshapes','coil100','kthactions',
'yfcc100m','yfcc100m_url','tiny_imagenet','coyo300m','coyo700m','pascal_voc_2007','coco_2014', 'ava',
'activitynet', 'open_images_v7', 'imagenet', 'imagenet21k', 'visualgenome' ,'widerface','meva_kf1',
'objectnet','lfw','inaturalist_2021','kinetics','hmdb','places365','ucf101','lvis','kitti',
'imagenet_localization','laion2b','datacomp_1b','imagenet2014_det','imagenet_faces','youtubeBB',
'pip_370k','pip_175k','cap','cap_pad','cap_detection','tiny_virat'</p>
<h2 id="returns">Returns</h2>
<p>(trainset, valset, testset) tuple where each is a <code><a title="vipy.dataset.Dataset" href="#vipy.dataset.Dataset">Dataset</a></code> or None, or a single split if name has a ":SPLIT" suffix or split kwarg provided</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/visym/vipy/blob/095a3bb922809af0de26f3bbf8f0fb8f9c4157a4/vipy/dataset.py#L730-L928" class="git-link">Browse git</a>
</summary>
<pre><code class="python">def registry(name=None, datadir=None, freeze=True, clean=False, download=False, split=&#39;train&#39;):
    &#34;&#34;&#34;Common entry point for loading datasets by name.

    Usage:
    
        &gt;&gt;&gt; trainset = vipy.dataset.registry(&#39;cifar10&#39;, split=&#39;train&#39;)             # return a training split
        &gt;&gt;&gt; valset = vipy.dataset.registry(&#39;cifar10:val&#39;, datadir=&#39;/tmp/cifar10&#39;)  # download to a custom location
        &gt;&gt;&gt; datasets = vipy.dataset.registry((&#39;cifar10:train&#39;,&#39;cifar100:train&#39;))   # return a union
        &gt;&gt;&gt; vipy.dataset.registry()                                                # print allowable datasets

    Args:
       name [str]: The string name for the dataset.  If tuple, return a `vipy.dataset.Union`.  If None, return the list of registered datasets.  Append name:train, name:val, name:test to output the requested split, or use the split keyword.
       datadir [str]: A path to a directory to store data.  Defaults to environment variable VIPY_DATASET_REGISTRY_HOME (then VIPY_CACHE if not found).  Also uses HF_HOME for huggingface datasets.  Datasets will be stored in datadir/name
       freeze [bool]:  If true, disable reference cycle counting for the loaded object (which will never contain cycles anyway) 
       clean [bool]: If true, force a redownload of the dataset to correct for partial download errors
       download [bool]: If true, force a redownload of the dataset to correct for partial download errors.  This is a synonym for clean=True
       split [str]: return &#39;train&#39;, &#39;val&#39; or &#39;test&#39; split.  If None, return (trainset, valset, testset) tuple

    Datasets:
       &#39;mnist&#39;,&#39;cifar10&#39;,&#39;cifar100&#39;,&#39;caltech101&#39;,&#39;caltech256&#39;,&#39;oxford_pets&#39;,&#39;sun397&#39;, &#39;food101&#39;,&#39;stanford_dogs&#39;,
       &#39;flickr30k&#39;,&#39;oxford_fgvc_aircraft&#39;,&#39;oxford_flowers_102&#39;,&#39;eurosat&#39;,&#39;d2d&#39;,&#39;ethzshapes&#39;,&#39;coil100&#39;,&#39;kthactions&#39;,
       &#39;yfcc100m&#39;,&#39;yfcc100m_url&#39;,&#39;tiny_imagenet&#39;,&#39;coyo300m&#39;,&#39;coyo700m&#39;,&#39;pascal_voc_2007&#39;,&#39;coco_2014&#39;, &#39;ava&#39;,
       &#39;activitynet&#39;, &#39;open_images_v7&#39;, &#39;imagenet&#39;, &#39;imagenet21k&#39;, &#39;visualgenome&#39; ,&#39;widerface&#39;,&#39;meva_kf1&#39;,
       &#39;objectnet&#39;,&#39;lfw&#39;,&#39;inaturalist_2021&#39;,&#39;kinetics&#39;,&#39;hmdb&#39;,&#39;places365&#39;,&#39;ucf101&#39;,&#39;lvis&#39;,&#39;kitti&#39;,
       &#39;imagenet_localization&#39;,&#39;laion2b&#39;,&#39;datacomp_1b&#39;,&#39;imagenet2014_det&#39;,&#39;imagenet_faces&#39;,&#39;youtubeBB&#39;,
       &#39;pip_370k&#39;,&#39;pip_175k&#39;,&#39;cap&#39;,&#39;cap_pad&#39;,&#39;cap_detection&#39;,&#39;tiny_virat&#39;

    Returns:
       (trainset, valset, testset) tuple where each is a `vipy.dataset.Dataset` or None, or a single split if name has a &#34;:SPLIT&#34; suffix or split kwarg provided
    &#34;&#34;&#34;
    
    import vipy.data

    datasets = (&#39;mnist&#39;,&#39;cifar10&#39;,&#39;cifar100&#39;,&#39;caltech101&#39;,&#39;caltech256&#39;,&#39;oxford_pets&#39;,&#39;sun397&#39;, &#39;stanford_dogs&#39;,&#39;coil100&#39;,
                &#39;flickr30k&#39;,&#39;oxford_fgvc_aircraft&#39;,&#39;oxford_flowers_102&#39;, &#39;food101&#39;, &#39;eurosat&#39;,&#39;d2d&#39;,&#39;ethzshapes&#39;,&#39;kthactions&#39;,
                &#39;yfcc100m&#39;,&#39;yfcc100m_url&#39;,&#39;tiny_imagenet&#39;,&#39;coyo300m&#39;,&#39;coyo700m&#39;,&#39;pascal_voc_2007&#39;,&#39;coco_2014&#39;, &#39;ava&#39;,
                &#39;activitynet&#39;,&#39;open_images_v7&#39;,&#39;imagenet&#39;,&#39;imagenet21k&#39;,&#39;visualgenome&#39;,&#39;widerface&#39;, &#39;youtubeBB&#39;,
                &#39;objectnet&#39;,&#39;lfw&#39;,&#39;inaturalist_2021&#39;,&#39;kinetics&#39;,&#39;hmdb&#39;,&#39;places365&#39;,&#39;ucf101&#39;,&#39;kitti&#39;,&#39;meva_kf1&#39;,
                &#39;lvis&#39;,&#39;imagenet_localization&#39;,&#39;laion2b&#39;,&#39;datacomp_1b&#39;,&#39;imagenet2014_det&#39;,&#39;imagenet_faces&#39;,
                &#39;pip_175k&#39;,&#39;pip_370k&#39;,&#39;cap&#39;,&#39;cap_pad&#39;,&#39;cap_detection&#39;,&#39;tiny_virat&#39;)  # Add to docstring too...
    
    if name is None:
        return tuple(sorted(datasets))
    if isinstance(name, (tuple, list)):
        assert all(n.startswith(datasets) for n in name)
        assert split is not None or all(&#39;:&#39; in n for n in name)
        return Union(*(registry(n, datadir=datadir, freeze=freeze, clean=clean, download=download, split=split) for n in name))    
    
    (name, split) = name.split(&#39;:&#39;,1) if name.count(&#39;:&#39;)&gt;0 else (name, split)
    if name not in datasets:
        raise ValueError(&#39;unknown dataset &#34;%s&#34; - choose from &#34;%s&#34;&#39; % (name, &#39;, &#39;.join(sorted(datasets))))
    if split not in [None, &#39;train&#39;, &#39;test&#39;, &#39;val&#39;]:
        raise ValueError(&#39;unknown split &#34;%s&#34; - choose from &#34;%s&#34;&#39; % (split, &#39;, &#39;.join([str(None), &#39;train&#39;, &#39;test&#39;, &#39;val&#39;])))

    datadir = remkdir(datadir if datadir is not None else (env(&#39;VIPY_DATASET_REGISTRY_HOME&#39;) if &#39;VIPY_DATASET_REGISTRY_HOME&#39; in env() else cache()))
    namedir = Path(datadir)/name    
    if (clean or download) and name in datasets and os.path.exists(namedir):
        log.info(&#39;Removing cached dataset &#34;%s&#34;&#39; % namedir)
        shutil.rmtree(namedir)  # delete cached subtree to force redownload ...
        
    if freeze:
        gc.disable()
        
    (trainset, valset, testset) = (None, None, None)    
    if name == &#39;mnist&#39;:
        (trainset, testset) = vipy.data.hf.mnist()        
    elif name == &#39;cifar10&#39;:
        (trainset, testset) = vipy.data.hf.cifar10()        
    elif name == &#39;cifar100&#39;:
        (trainset, testset) = vipy.data.hf.cifar100()        
    elif name == &#39;caltech101&#39;:
        trainset = vipy.data.caltech101.Caltech101(namedir)        
    elif name == &#39;caltech256&#39;:
        trainset = vipy.data.caltech256.Caltech256(namedir)
    elif name == &#39;oxford_pets&#39;:
        (trainset, testset) = vipy.data.hf.oxford_pets()
    elif name == &#39;sun397&#39;:
        (trainset, valset, testset) = vipy.data.hf.sun397()
    elif name == &#39;stanford_dogs&#39;:
        trainset = vipy.data.stanford_dogs.StanfordDogs(namedir)
    elif name == &#39;food101&#39;:
        trainset = vipy.data.food101.Food101(namedir)
    elif name == &#39;eurosat&#39;:
        trainset = vipy.data.eurosat.EuroSAT(namedir)
    elif name == &#39;d2d&#39;:
        trainset = vipy.data.d2d.D2D(namedir)
    elif name == &#39;coil100&#39;:
        trainset = vipy.data.coil100.COIL100(namedir)
    elif name == &#39;kthactions&#39;:
        (trainset, testset) = vipy.data.kthactions.KTHActions(namedir).split()
    elif name == &#39;ethzshapes&#39;:
        trainset = vipy.data.ethzshapes.ETHZShapes(namedir)        
    elif name == &#39;flickr30k&#39;:
        trainset = vipy.data.hf.flickr30k()
    elif name == &#39;oxford_fgvc_aircraft&#39;:
        trainset = vipy.data.hf.oxford_fgvc_aircraft()
    elif name == &#39;oxford_flowers_102&#39;:
        trainset = vipy.data.oxford_flowers_102.Flowers102(namedir)
    elif name == &#39;yfcc100m&#39;:
        (trainset, _, valset, _) = vipy.data.hf.yfcc100m()  
    elif name == &#39;yfcc100m_url&#39;:
        (_, trainset, _, valset) = vipy.data.hf.yfcc100m()  
    elif name == &#39;tiny_imagenet&#39;:
        (trainset, valset) = vipy.data.hf.tiny_imagenet()
    elif name == &#39;coyo300m&#39;:
        trainset = vipy.data.hf.coyo300m()
    elif name == &#39;coyo700m&#39;:
        trainset = vipy.data.hf.coyo700m()
    elif name == &#39;datacomp_1b&#39;:
        trainset = vipy.data.hf.datacomp_1b()
    elif name == &#39;pascal_voc_2007&#39;:
        (trainset, valset, testset) = vipy.data.hf.pascal_voc_2007()
    elif name == &#39;coco_2014&#39;:
        trainset = vipy.data.coco.COCO_2014(namedir)
    elif name == &#39;ava&#39;:
        ava = vipy.data.ava.AVA(namedir)
        (trainset, valset) = (ava.trainset(), ava.valset())
    elif name == &#39;activitynet&#39;:
        activitynet = vipy.data.activitynet.ActivityNet(namedir)  # ActivityNet 200
        (trainset, valset, testset) = (activitynet.trainset(), activitynet.valset(), activitynet.testset())
    elif name == &#39;open_images_v7&#39;:
        trainset = vipy.data.openimages.open_images_v7(namedir)
    elif name == &#39;imagenet&#39;:
        imagenet = vipy.data.imagenet.Imagenet2012(namedir)
        (trainset, valset) = (imagenet.classification_trainset(), imagenet.classification_valset())
    elif name == &#39;imagenet_faces&#39;:
        trainset = vipy.data.imagenet.Imagenet2012(Path(datadir)/&#39;imagenet&#39;).faces()
    elif name == &#39;imagenet21k&#39;:
        trainset = vipy.data.imagenet.Imagenet21K(namedir)
    elif name == &#39;visualgenome&#39;:
        trainset = vipy.data.visualgenome.VisualGenome(namedir)  # visualgenome-1.4
    elif name == &#39;widerface&#39;:
        trainset = vipy.data.widerface.WiderFace(namedir, split=&#39;train&#39;)
        valset = vipy.data.widerface.WiderFace(namedir, split=&#39;val&#39;)
        testset = vipy.data.widerface.WiderFace(namedir, split=&#39;test&#39;)                                      
    elif name == &#39;objectnet&#39;:
        assert split is None or split == &#39;test&#39;, &#34;objectnet is a test set&#34;
        testset = vipy.data.objectnet.Objectnet(namedir)
    elif name == &#39;lfw&#39;:
        trainset = vipy.data.lfw.LFW(namedir)
    elif name == &#39;inaturalist_2021&#39;:
        dataset = vipy.data.inaturalist.iNaturalist2021(namedir)
        (trainset, valset) = (dataset.trainset(), dataset.valset())
    elif name == &#39;kinetics&#39;:
        dataset = vipy.data.kinetics.Kinetics700(namedir)  # Kinetics700
        (trainset, valset, testset) = (dataset.trainset(), dataset.valset(), dataset.testset())
    elif name == &#39;hmdb&#39;:
        trainset = vipy.dataset.Dataset(vipy.data.hmdb.HMDB(namedir).dataset(), id=&#39;hmdb&#39;)
    elif name == &#39;places365&#39;:
        places = vipy.data.places.Places365(namedir)
        (trainset, valset) = (places.trainset(), places.valset())
    elif name == &#39;ucf101&#39;:
        trainset = vipy.data.ucf101.UCF101(namedir)
    elif name == &#39;kitti&#39;:
        trainset = vipy.data.kitti.KITTI(namedir, split=&#39;train&#39;)
        testset = vipy.data.kitti.KITTI(namedir, split=&#39;test&#39;)                                      
    elif name == &#39;lvis&#39;:
        lvis = vipy.data.lvis.LVIS(namedir)
        (trainset, valset) = (lvis.trainset(), lvis.valset())
    elif name == &#39;imagenet_localization&#39;:
        trainset = vipy.data.imagenet.Imagenet2012(Path(datadir)/&#39;imagenet&#39;).localization_trainset()
    elif name == &#39;imagenet2014_det&#39;:
        imagenet2014_det = vipy.data.imagenet.Imagenet2014_DET(namedir)
        (trainset, valset, testset) = (imagenet2014_det.trainset(), imagenet2014_det.valset(), imagenet2014_det.testset())
    elif name == &#39;laion2b&#39;:
        trainset = vipy.data.hf.laion2b()
    elif name == &#39;youtubeBB&#39;:
        trainset = vipy.data.youtubeBB.YoutubeBB(namedir)
    elif name == &#39;meva_kf1&#39;:
        trainset = vipy.data.meva.KF1(namedir).dataset()  # consider using &#34;with vipy.globals.multiprocessing(pct=0.5):&#34;
    elif name == &#39;pip_175k&#39;:
        trainset = vipy.data.pip.PIP_175k(namedir)
    elif name == &#39;pip_370k&#39;:
        trainset = vipy.data.pip.PIP_370k_stabilized(namedir)
    elif name == &#39;cap&#39;:
        trainset = vipy.data.cap.CAP_classification_clip(namedir)
    elif name == &#39;cap_pad&#39;:
        trainset = vipy.data.cap.CAP_classification_pad(namedir)        
    elif name == &#39;cap_detection&#39;:
        trainset = vipy.data.cap.CAP_detection(namedir)
    elif name == &#39;tiny_virat&#39;:
        dataset = vipy.data.tiny_virat.TinyVIRAT(namedir)
        (trainset, valset, testset) = (dataset.trainset(), dataset.valset(), dataset.testset())
    else:
        raise ValueError(&#39;unknown dataset &#34;%s&#34; - choose from &#34;%s&#34;&#39; % (name, &#39;, &#39;.join(sorted(datasets))))
    
    if freeze:
        gc.enable()
        gc.collect()
        gc.freeze()  # python-3.7

    if split == &#39;train&#39;:
        return trainset
    elif split == &#39;val&#39;:
        return valset
    elif split == &#39;test&#39;:
        return testset
    else:
        return (trainset, valset, testset)</code></pre>
</details>
</dd>
</dl>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="vipy.dataset.Dataset"><code class="flex name class">
<span>class <span class="ident">Dataset</span></span>
<span>(</span><span>dataset, id=None, loader=None)</span>
</code></dt>
<dd>
<div class="desc"><p>vipy.dataset.Dataset() class</p>
<p>Common class to manipulate large sets of objects in parallel</p>
<h2 id="args">Args</h2>
<ul>
<li>dataset [list, tuple, set, obj]: a python built-in type that supports indexing or a generic object that supports indexing and has a length</li>
<li>id [str]: an optional id of this dataset, which provides a descriptive name of the dataset</li>
<li>loader [callable]: a callable loader that will construct the object from a raw data element in dataset.
This is useful for custom deerialization or on demand transformations
Datasets can be indexed, shuffled, iterated, minibatched, sorted, sampled, partitioned.
Datasets constructed of vipy objects are lazy loaded, delaying loading pixels until they are needed</li>
</ul>
<pre><code class="language-python">(trainset, valset, testset) = vipy.dataset.registry('mnist')

(trainset, valset) = trainset.partition(0.9, 0.1)
categories = trainset.set(lambda im: im.category())
smaller = testset.take(1024)
preprocessed = smaller.map(lambda im: im.resize(32, 32).gain(1/256))

for b in preprocessed.minibatch(128):
    print(b)

# visualize the dataset 
(trainset, valset, testset) = vipy.dataset.registry('pascal_voc_2007')
for im in trainset:
    im.mindim(1024).show().print(sleep=1).close()

</code></pre>
<p>Datasets can be constructed from directories of json files or image files (<code><a title="vipy.dataset.Dataset.from_directory" href="#vipy.dataset.Dataset.from_directory">Dataset.from_directory()</a></code>)
Datasets can be constructed from a single json file containing a list of objects (<code><a title="vipy.dataset.Dataset.from_json" href="#vipy.dataset.Dataset.from_json">Dataset.from_json()</a></code>)</p>
<div class="admonition note">
<p class="admonition-title">Note:&ensp;that if a lambda function is provided as loader then this dataset is not serializable.
Use self.load() then serialize</p>
</div></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/visym/vipy/blob/095a3bb922809af0de26f3bbf8f0fb8f9c4157a4/vipy/dataset.py#L20-L556" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class Dataset():
    &#34;&#34;&#34;vipy.dataset.Dataset() class
    
    Common class to manipulate large sets of objects in parallel

    Args:
        - dataset [list, tuple, set, obj]: a python built-in type that supports indexing or a generic object that supports indexing and has a length
        - id [str]: an optional id of this dataset, which provides a descriptive name of the dataset
        - loader [callable]: a callable loader that will construct the object from a raw data element in dataset.  This is useful for custom deerialization or on demand transformations

    Datasets can be indexed, shuffled, iterated, minibatched, sorted, sampled, partitioned.
    Datasets constructed of vipy objects are lazy loaded, delaying loading pixels until they are needed

    ```python
    (trainset, valset, testset) = vipy.dataset.registry(&#39;mnist&#39;)

    (trainset, valset) = trainset.partition(0.9, 0.1)
    categories = trainset.set(lambda im: im.category())
    smaller = testset.take(1024)
    preprocessed = smaller.map(lambda im: im.resize(32, 32).gain(1/256))
    
    for b in preprocessed.minibatch(128):
        print(b)

    # visualize the dataset 
    (trainset, valset, testset) = vipy.dataset.registry(&#39;pascal_voc_2007&#39;)
    for im in trainset:
        im.mindim(1024).show().print(sleep=1).close()
    
    ```

    Datasets can be constructed from directories of json files or image files (`vipy.dataset.Dataset.from_directory`)
    Datasets can be constructed from a single json file containing a list of objects (`vipy.dataset.Dataset.from_json`)
    
    ..note:: that if a lambda function is provided as loader then this dataset is not serializable.  Use self.load() then serialize
    &#34;&#34;&#34;

    __slots__ = (&#39;_id&#39;, &#39;_ds&#39;, &#39;_idx&#39;, &#39;_loader&#39;, &#39;_type&#39;)
    def __init__(self, dataset, id=None, loader=None):
        assert loader is None or callable(loader)
        
        self._id = id
        self._ds = dataset if not isinstance(dataset, Dataset) else dataset._ds
        self._idx = None if not isinstance(dataset, Dataset) else dataset._idx   # random access on-demand
        self._loader = loader if not isinstance(dataset, Dataset) else dataset._loader  # not serializable if lambda is provided

        try:
            self._type = str(type(self._loader(dataset[0]) if self._loader else dataset[0]))  # peek at first element, cached
        except:
            self._type = None


    @classmethod
    def from_directory(cls, indir, filetype=&#39;json&#39;, id=None):
        &#34;&#34;&#34;Recursively search indir for filetype, construct a dataset from all discovered files of that type&#34;&#34;&#34;
        if filetype == &#39;json&#39;:
            return cls([x for f in findjson(indir) for x in to_iterable(vipy.load(f))], id=id)
        elif filetype.lower() in [&#39;jpg&#39;,&#39;jpeg&#39;,&#39;images&#39;]:
            return cls([vipy.image.Image(filename=f) for f in findimages(indir)], id=id)            
        elif filetype.lower() in [&#39;mp4&#39;,&#39;videos&#39;]:
            return cls([vipy.image.Video(filename=f) for f in findvideos(indir)], id=id)            
        else:
            raise ValueError(&#39;unsupported file type &#34;%s&#34;&#39; % filetype)

    @classmethod
    def from_image_urls(cls, urls, id=None):
        &#34;&#34;&#34;Construct a dataset from a list of image URLs&#34;&#34;&#34;
        return cls([vipy.image.Image(url=url) for url in to_iterable(urls) if isimageurl(url)], id=id)
        
    @classmethod
    def from_json(cls, jsonfile, id=None):
        return cls([x for x in to_iterable(vipy.load(jsonfile))], id=id)

    @classmethod
    def cast(cls, obj):
        return cls(obj) if not isinstance(obj, Dataset) else obj
    
    def __repr__(self):
        fields = [&#39;id=%s&#39; % truncate_string(self.id(), maxlen=80)] if self.id() else []
        fields += [&#39;len=%d&#39; % self.len()] if self.len() is not None else []
        fields += [&#39;type=%s&#39; % self._type] if self._type else []
        return str(&#39;&lt;vipy.dataset.Dataset: %s&gt;&#39; % &#39;, &#39;.join(fields))

    def __iter__(self):            
        if self.is_streaming():
            for x in self._ds:  # iterable access (faster)
                yield self._loader(x) if self._loader is not None else x                 
        else:
            for k in range(len(self)):
                yield self[k]   # random access (slower)                


    def __getitem__(self, k):
        assert self.len() is not None, &#34;dataset does not support indexing&#34;
        
        idx = self.index()  # convert to random access on demand
        if isinstance(k, (int, np.uint64)):
            assert abs(k) &lt; len(idx), &#34;invalid index&#34;
            x = self._ds[idx[int(k)]]
            x = self._loader(x) if self._loader is not None else x
            return x
        elif isinstance(k, slice):
            X = [self._ds[k] for k in idx[k.start:k.stop:k.step]]
            X = [self._loader(x) for x in X] if self._loader is not None else X
            return X
        else:
            raise ValueError(&#39;invalid slice &#34;%s&#34;&#39; % type(k))            

    def raw(self):
        &#34;&#34;&#34;Return a view of this dataset without the loader&#34;&#34;&#34;
        return Dataset(self._ds, loader=None)
    
    def is_streaming(self):
        return self._idx is None

    def len(self):
        return len(self._idx) if self._idx is not None else (len(self._ds) if hasattr(self._ds, &#39;__len__&#39;) else None)

    def __len__(self):
        len = self.len()
        if len is None:
            raise ValueError(&#39;dataset has no length&#39;)
        return len
    
    def __or__(self, other):
        assert isinstance(other, Dataset)
        return Union(self, other, id=self.id())
    
    def id(self, new_id=None):
        &#34;&#34;&#34;Change the dataset ID to the provided ID, or return it if None&#34;&#34;&#34;
        if new_id is not None:
            self._id = new_id
            return self
        return self._id

    def index(self, index=None, strict=False):
        &#34;&#34;&#34;Update the index, useful for filtering of large datasets&#34;&#34;&#34;
        if index is not None:
            assert not strict or index is None or (len(index)&gt;0 and len(index)&lt;=len(self) and max(index)&lt;len(self) and min(index)&gt;=0)            
            self._idx = index
            return self
        if self._idx is None:
            self._idx = list(range(len(self._ds)))  # on-demand index, only if underlying dataset has known length
        return self._idx
    
        
    def clone(self, deep=False):
        &#34;&#34;&#34;Return a copy of the dataset object&#34;&#34;&#34;
        if not deep:
            return copy.copy(self) 
        else:
            return copy.deepcopy(self)
    
    def shuffle(self, shuffler=None):
        &#34;&#34;&#34;Permute elements in this dataset uniformly at random in place using the optimal shuffling strategy for the dataset structure to maximize performance.
           This method will use either Dataset.streaming_shuffler (for iterable datasets) or Dataset.uniform_shuffler (for random access datasets)
        &#34;&#34;&#34;
        assert shuffler is None or callable(shuffler)
        shuffler = shuffler if shuffler is not None else (Dataset.streaming_shuffler if self.is_streaming() else Dataset.uniform_shuffler)
        return shuffler(self)

    def repeat(self, n):
        &#34;&#34;&#34;Repeat the dataset n times.  If n=0, the dataset is unchanged, if n=1 the dataset is doubled in length, etc.&#34;&#34;&#34;
        assert n&gt;=0
        return self.index( self.index()*(n+1) )
    
    def tuple(self, mapper=None, flatten=False, reducer=None):
        &#34;&#34;&#34;Return the dataset as a tuple, applying the optional mapper lambda on each element, applying optional flattener on sequences returned by mapper, and applying the optional reducer lambda on the final tuple, return a generator&#34;&#34;&#34;
        assert mapper is None or callable(mapper)
        assert reducer is None or callable(reducer)
        mapped = self.map(mapper) if mapper else self
        flattened = (y for x in mapped for y in x) if flatten else (x for x in mapped)
        reduced = reducer(flattened) if reducer else flattened
        return reduced

    def list(self, mapper=None, flatten=False):
        &#34;&#34;&#34;Return a tuple as a list, loading into memory&#34;&#34;&#34;
        return self.tuple(mapper, flatten, reducer=list)

    def set(self, mapper=None, flatten=False):
        &#34;&#34;&#34;Return the dataset as a set.  Mapper must be a lambda function that returns a hashable type&#34;&#34;&#34;
        return self.tuple(mapper=mapper, reducer=set, flatten=flatten)        

    def all(self, mapper):
        return self.tuple(mapper=mapper, reducer=all)
    
    def frequency(self, f):
        &#34;&#34;&#34;Frequency counts for which lambda returns the same value.  For example f=lambda im: im.category() returns a dictionary of category names and counts in this category&#34;&#34;&#34;
        return countby(self.tuple(mapper=f))

    def balanced(self, f):
        &#34;&#34;&#34;Is the dataset balanced (e.g. the frequencies returned from the lambda f are all the same)?&#34;&#34;&#34;
        return len(set(self.frequency(f).values())) == 1
    
    def count(self, f):
        &#34;&#34;&#34;Counts for each element for which lamba returns true.  
        
        Args:
            f: [lambda] if provided, count the number of elements that return true.  

        Returns:
            A length of elements that satisfy f(v) = True [if f is not None]
        &#34;&#34;&#34;
        return len(self.tuple(f, reducer=lambda X: [x for x in X if x is True]))

    def countby(self, f):
        return self.frequency(f)
    
    def filter(self, f):
        &#34;&#34;&#34;In place filter with lambda function f, keeping those elements obj in-place where f(obj) evaluates true.  Callable should return bool&#34;&#34;&#34;
        assert callable(f)
        return self.index( [i for (b,i) in zip(self.localmap(f), self.index()) if b] )
    
    def take(self, n, inplace=False):
        &#34;&#34;&#34;Randomly Take n elements from the dataset, and return a dataset (in-place or cloned). If n is greater than the size of the dataset, sample with replacement, if n is less than the size of the dataset, sample without replacement&#34;&#34;&#34;
        assert isinstance(n, int) and n&gt;0
        D = self.clone() if not inplace else self
        return D.index(list((random.sample if n&lt;= len(self) else random.choices)(D.index(), k=n)) )


    def groupby(self, f):
        &#34;&#34;&#34;Group the dataset according to the callable f, returning dictionary of grouped datasets.&#34;&#34;&#34;
        assert callable(f)        
        return {k:self.clone().index([x[1] for x in v]).id(&#39;%s:%s&#39; % (self.id(),str(k))) for (k,v) in itertools.groupby(enumerate(self.sort(f).index()), lambda x: f(self[x[0]]))}

    def takeby(self, f, n):
        &#34;&#34;&#34;Filter the dataset according to the callable f, take n from each group and return a dataset.  Callable should return bool.  If n==1, return a singleton&#34;&#34;&#34;
        d = self.clone().filter(f)
        return d.take(n) if n&gt;1 else d.takeone()

    def takelist(self, n):
        &#34;&#34;&#34;Take n elements and return list.  The elements are loaded and not cloned.&#34;&#34;&#34;
        return self.take(n).list()

    def takeone(self):
        &#34;&#34;&#34;Randomly take one element from the dataset and return a singleton&#34;&#34;&#34;
        return self[random.randint(0, len(self)-1)]

    def sample(self):
        &#34;&#34;&#34;Return a single element sampled uniformly at random&#34;&#34;&#34;
        return self.takeone()
    
    def take_fraction(self, p, inplace=False):
        &#34;&#34;&#34;Randomly take a percentage of the dataset, returning a clone or in-place&#34;&#34;&#34;
        assert p&gt;=0 and p&lt;=1, &#34;invalid fraction &#39;%s&#39;&#34; % p
        return self.take(n=int(len(self)*p), inplace=inplace)

    def inverse_frequency(self, f):
        &#34;&#34;&#34;Return the inverse frequency of elements grouped by the callable f.  Returns a dictionary of the callable output to inverse frequency &#34;&#34;&#34;
        attributes = self.set(f)
        frequency = self.frequency(f)
        return {a:(1/len(attributes))*(len(self)/frequency[a]) for a in attributes}  # (normalized) inverse frequency weight
    
    def load(self):
        &#34;&#34;&#34;Cache the entire dataset into memory&#34;&#34;&#34;
        return Dataset([x for x in self], id=self.id())
    
    def chunk(self, n):
        &#34;&#34;&#34;Yield n chunks as list.  Last chunk will be ragged.&#34;&#34;&#34;
        for (k,c) in enumerate(chunkgen(self, n)):
            yield list(c)

    def batch(self, n):
        &#34;&#34;&#34;Yield batches of size n as datasets.  Last batch will be ragged.  Batches are not loaded.  Batches have appended id equal to the zero-indexed batch order&#34;&#34;&#34;
        for (k,b) in enumerate(chunkgenbysize(self, n)):  
            yield Dataset(b).id(&#39;%s:%d&#39; % (self.id() if self.id() else &#39;&#39;, k))
                                
    def minibatch(self, n, ragged=True, loader=None, bufsize=1024, accepter=None, preprocessor=None):
        &#34;&#34;&#34;Yield preprocessed minibatches of size n of this dataset.

        To yield chunks of this dataset, suitable for minibatch training/testing

        ```python
        D = vipy.dataset.Dataset(...)
        for b in D.minibatch(n):
           print(b)
        ```
        
        To perform minibatch image downloading in parallel across four processes with the context manager:

        ```python
        D = vipy.dataset.registry(&#39;yfcc100m_url:train&#39;).take(128)
        with vipy.globals.parallel(4):
            for b in D.minibatch(16, loader=vipy.image.Transform.download, accepter=lambda im: im.is_downloaded()):
                print(b)  # complete minibatch that passed accepter
        ```

        Args:
            n [int]: The size of the minibatch
            ragged [bool]: If ragged=true, then the last chunk will be ragged with len(chunk)&lt;n, else skipped
            bufsize [int]:  The size of the buffer used in parallel processing of elements.  Useful for parallel loading
            accepter [callable]:  A callable that returns true|false on an element, where only elements that return true are included in the minibatch.  useful for parallel loading of elements that may fail to download
            loader [callable]: A callable that is applied to every element of the dataset.  Useful for parallel loading

        Returns:        
            Iterator over `vipy.dataset.Dataset` elements of length n.  Minibatches will be yielded loaded and preprocessed (processing done concurrently if vipy.parallel.executor() is initialized)

        ..note:: The distributed iterator appends the minibatch index to the minibatch.id().  
        ..note:: If there exists a vipy.parallel.exeuctor(), then loading and preprocessing will be performed concurrently

        &#34;&#34;&#34;
        for (k,b) in enumerate(chunkgenbysize(vipy.parallel.iter(self, mapper=loader, bufsize=max(bufsize,n), accepter=accepter), n)): 
            if ragged or len(b) == n:
                yield Dataset.cast(b).id(&#39;%s:%d&#39; % (self.id() if self.id() else &#39;&#39;, k))                    
                    
                        
    def shift(self, m):
        &#34;&#34;&#34;Circular shift the dataset m elements to the left, so that self[k+m] == self.shift(m)[k].  Circular shift for boundary handling so that self.shift(m)[-1] == self[m-1]&#34;&#34;&#34;
        return self.clone().index(self.index()[m:] + self.index()[0:m])

    def slice(self, start=0, stop=-1, step=1):
        &#34;&#34;&#34;Slice the dataset to contain elements defined by slice(start, stop, step)&#34;&#34;&#34;
        return self.clone().index(self.index()[start:stop:step])
        
    def truncate(self, m):
        &#34;&#34;&#34;Truncate the dataset to contain the first m elements only&#34;&#34;&#34;
        return self.slice(stop=m)
    
    def pipeline(self, n, m, ragged=True, prepad=True, postpad=True):
        &#34;&#34;&#34;Yield pipelined minibatches of size n with pipeline length m.

        A pipelined minibatch is a tuple (head, tail) such that (head, tail) are minibatches at different indexes in the dataset.  
        Head corresponds to the current minibatch and tail corresponds to the minibatch left shifted by (m-1) minibatches.

        This structure is useful for yielding datasets for pipelined training where head contains the minibatch that will complete pipeline training on this iteration, and tail contains the 
        next minibatch to be inserted into the pipeline on this iteration.
        
        ```python
        D = vipy.dataset.Dataset(...)
        for (head, tail) in D.pipeline(n, m, prepad=False, postpad=False):
            assert head == D[0:m]
            assert tail == D[n*(m-1): n*(m-1)+n]

        Args:
            n [int]: The size of each minibatch
            m [int]:  The pipeline length in minibatches
            ragged [bool]: If ragged=true, then the last chunk will be ragged with len(chunk)&lt;n, else skipped
            prepad: If true, yield (head, tail) == (None, batch) when filling the pipeline
            postpad: If true, yield (head, tail) == (batch, None) when flushing the pipeline
        
        Returns:        
            Iterator over tuples (head,tail) of `vipy.dataset.Dataset` elements of length n where tail is left shifted by n*(m-1) elements. 
        
        .. note::  The distributed iterator is not order preserving over minibatches and yields minibatches as completed, however the tuple (head, tail) is order preserving within the pipeline
        .. note:: If there exists a vipy.parallel.executor(), then loading and preprocessing will be performed concurrently
        
        &#34;&#34;&#34;
        pipeline = [] 
        for (k,b) in enumerate(self.minibatch(n, ragged=ragged)):  # not order preserving
            pipeline.append(b)  # order preserving within pipeline                        
            if k &lt; m-1:
                if prepad:
                    yield( (None, b) )  
            else:
                yield( (pipeline.pop(0), b) )  # yield deque-like (minibatch, shifted minibatch) tuples
        for p in pipeline:
            if postpad:
                yield( (p, None) )


    def chunks(self, sizes):
        &#34;&#34;&#34;Partition the dataset into chunks of size given by the tuple in partitions, and give the dataset suffix if provided&#34;&#34;&#34;
        assert sum(sizes) == len(self)

        i = 0
        datasets = []
        for n in sizes:
            datasets.append(self.clone().index(self.index()[i:i+n]))
            i += n
        return datasets
        
    def partition(self, trainfraction=0.9, valfraction=0.1, testfraction=0, trainsuffix=&#39;:train&#39;, valsuffix=&#39;:val&#39;, testsuffix=&#39;:test&#39;):
        &#34;&#34;&#34;Partition the dataset into the requested (train,val,test) fractions.  

        Args:
            trainfraction [float]: fraction of dataset for training set
            valfraction [float]: fraction of dataset for validation set
            testfraction [float]: fraction of dataset for test set
            trainsuffix: If not None, append this string the to trainset ID
            valsuffix: If not None, append this string the to valset ID
            testsuffix: If not None, append this string the to testset ID        
        
        Returns:        
            (trainset, valset, testset) such that trainset is the first trainfraction of the dataset.  

        .. note:: This does not permute the dataset.  To randomize split, shuffle dataset first

        &#34;&#34;&#34;
        assert trainfraction &gt;=0 and trainfraction &lt;= 1, &#34;invalid training set fraction &#39;%f&#39;&#34; % trainfraction
        assert valfraction &gt;=0 and valfraction &lt;= 1, &#34;invalid validation set fraction &#39;%f&#39;&#34; % valfraction
        assert testfraction &gt;=0 and testfraction &lt;= 1, &#34;invalid test set fraction &#39;%f&#39;&#34; % testfraction
        assert abs(trainfraction + valfraction + testfraction - 1) &lt; 1E-6, &#34;fractions must sum to one&#34;
        
        idx = self.index()
        (testidx, validx, trainidx) = dividelist(idx, (testfraction, valfraction, trainfraction))
            
        trainset = self.clone().index(trainidx)
        if trainsuffix and trainset.id():
            trainset.id(trainset.id() + trainsuffix)
        
        valset = self.clone().index(validx)
        if valsuffix and valset.id():
            valset.id(valset.id() + valsuffix)
        
        testset = self.clone().index(testidx)
        if testsuffix and testset.id():
            testset.id(testset.id() + testsuffix)
                
        return (trainset,valset,testset) if testfraction!=0 else (trainset, valset)

    def split(self, size):
        &#34;&#34;&#34;Split the dataset into two datasets, one of length size, the other of length len(self)-size&#34;&#34;&#34;
        assert isinstance(size, int) and size&gt;=0 and size&lt;len(self)
        return self.partition(size/len(self), (len(self)-size)/len(self), 0, &#39;&#39;, &#39;&#39;, &#39;&#39;)

    def even_split(self):
        &#34;&#34;&#34;Split the dataset into two datasets, each half the size of the dataset.  If the dataset length is odd, then one element will be dropped&#34;&#34;&#34;
        return self.chunks((len(self)//2, len(self)//2, len(self)%2))[0:2]
        
    def streaming_map(self, mapper, accepter=None, bufsize=1024):
        &#34;&#34;&#34;Returns a generator that will apply the mapper and yield only those elements that return True from the accepter.  Performs the map in parallel if used in the vipy.globals.parallel context manager&#34;&#34;&#34;
        return vipy.parallel.iter(self, mapper=mapper, accepter=accepter, bufsize=bufsize)
        
    def map(self, f_map, strict=True, oneway=False, ordered=False):        
        &#34;&#34;&#34;Parallel map.

        To perform this in parallel across four threads:

        ```python
        D = vipy.dataset.Dataset(...)
        with vipy.globals.parallel(4):
            D = D.map(lambda v: ...)
        ```

        Args:
            f_map: [lambda] The lambda function to apply in parallel to all elements in the dataset.  This must return a JSON serializable object (or set oneway=True)
            strict: [bool] If true, raise exception on distributed map failures, otherwise the map will return only those that succeeded
            oneway: [bool] If true, do not pass back results unless exception.  This is useful for distributed processing
        
        Returns:
            A `vipy.dataset.Dataset` containing the elements f_map(v).  This operation is order preserving if ordered=True.

        .. note:: 
            - This method uses dask distributed and `vipy.batch.Batch` operations
            - Due to chunking, all error handling is caught by this method.  Use `vipy.batch.Batch` to leverage dask distributed futures error handling.
            - Operations must be chunked and serialized because each dask task comes with overhead, and lots of small tasks violates best practices
            - Serialized results are deserialized by the client and returned a a new dataset
        &#34;&#34;&#34;
        assert f_map is None or callable(f_map), &#34;invalid map function&#34;

        # Identity
        if f_map is None:
            return self        

        # Parallel map 
        elif vipy.globals.cf() is not None:
            # This will fail on multiprocessing if dataset contains a loader lambda, or any element in the dataset contains a loader.  Use distributed instead
            assert ordered == False, &#34;not order preserving, use localmap()&#34;
            return Dataset(tuple(vipy.parallel.map(f_map, self)), id=self.id()) 
                                              
        # Distributed map
        elif vipy.globals.dask() is not None:
            from vipy.batch import Batch   # requires pip install vipy[all]                
            f_serialize = lambda x: x
            f_deserialize = lambda x: x
            f_oneway = lambda x, oneway=oneway: x if not x[0] or not oneway else (x[0], None)
            f_catcher = lambda f, *args, **kwargs: catcher(f, *args, **kwargs)  # catch exceptions when executing lambda, return (True, result) or (False, exception)
            f = lambda x, f_serializer=f_serialize, f_deserializer=f_deserialize, f_map=f_map, f_catcher=f_catcher, f_oneway=f_oneway: f_serializer(f_oneway(f_catcher(f_map, f_deserializer(x))))  # with closure capture
            
            S = [f_serialize(v) for v in self]  # local load, preprocess and serialize
            B = Batch(chunklist(S, 128), strict=False, warnme=False, minscatter=128)
            S = B.map(lambda X,f=f: [f(x) for x in X]).result()  # distributed, chunked, with caught exceptions, may return empty list
            V = [f_deserialize(x) for s in S for x in s]  # Local deserialization and chunk flattening
            
            # Error handling
            (good, bad) = ([r for (b,r) in V if b], [r for (b,r) in V if not b])  # catcher returns (True, result) or (False, exception string)
            if len(bad)&gt;0:
                log.warning(&#39;Exceptions in distributed processing:\n%s\n\n[vipy.dataset.Dataset.map]: %d/%d items failed&#39; % (str(bad), len(bad), len(self)))
                if strict:
                    raise ValueError(&#39;exceptions in distributed processing&#39;)
            return Dataset(good, id=self.id()) if not oneway else None

        # Local map
        else:
            return self.localmap(f_map)
        
    def localmap(self, f):
        &#34;&#34;&#34;A map performed without any parallel processing&#34;&#34;&#34;
        return Dataset([f(x) for x in self], id=self.id())  # triggers load into memory        

    def zip(self, iter):
        &#34;&#34;&#34;Returns a new dataset constructed by applying the callable on elements from zip(self,iter)&#34;&#34;&#34;
        return Dataset(zip(self,iter))
    
    def sort(self, f):
        &#34;&#34;&#34;Sort the dataset in-place using the sortkey lambda function f

        To perform a sort of the dataset using some property of the instance, such as the object category (e.g. for vipy.image.ImageCategory) 

        ```python
        dataset.sort(lambda im: im.category())
        ```
        &#34;&#34;&#34;
        idx = self.index()
        return self.index( [idx[j] for (j,x) in sorted(zip(range(len(self)), self.tuple(f)), key=lambda x: x[1])] )

    
    @staticmethod
    def uniform_shuffler(D):
        &#34;&#34;&#34;A uniform shuffle on the dataset elements.  Iterable access will be slow due to random access&#34;&#34;&#34;
        idx = D.index()
        random.shuffle(idx)
        return D.index(idx)

    @staticmethod
    def streaming_shuffler(D):
        &#34;&#34;&#34;A uniform shuffle (approximation) on the dataset elements for iterable access only&#34;&#34;&#34;
        assert D._idx is None, &#34;streaming only&#34;
        
        try_import(&#39;datasets&#39;, &#39;datasets&#39;); from datasets import Dataset as HuggingfaceDataset;
        
        if isinstance(D._ds, (list, tuple)):
            D._ds = list(D._ds)
            random.shuffle(D._ds)  # in-place shuffle objects
                
        elif isinstance(D._ds, HuggingfaceDataset):
            # Special case: Arrow backed dataset            
            D._ds = D._ds.to_iterable_dataset()  # no random access
            D._ds.shuffle()  # approximate shuffling for IterableDataset is much more efficient for __iter__
        else:
            raise ValueError(&#39;shuffle error&#39;)
        return D
    
    @staticmethod
    def identity_shuffler(D):
        &#34;&#34;&#34;Shuffler that does nothing&#34;&#34;&#34;
        return D</code></pre>
</details>
<h3>Subclasses</h3>
<ul class="hlist">
<li><a title="vipy.data.caltech101.Caltech101" href="data/caltech101.html#vipy.data.caltech101.Caltech101">Caltech101</a></li>
<li><a title="vipy.data.caltech256.Caltech256" href="data/caltech256.html#vipy.data.caltech256.Caltech256">Caltech256</a></li>
<li><a title="vipy.data.cap.CAP_classification_clip" href="data/cap.html#vipy.data.cap.CAP_classification_clip">CAP_classification_clip</a></li>
<li><a title="vipy.data.cap.CAP_classification_pad" href="data/cap.html#vipy.data.cap.CAP_classification_pad">CAP_classification_pad</a></li>
<li><a title="vipy.data.cap.CAP_detection" href="data/cap.html#vipy.data.cap.CAP_detection">CAP_detection</a></li>
<li><a title="vipy.data.cc12m.CC12M" href="data/cc12m.html#vipy.data.cc12m.CC12M">CC12M</a></li>
<li><a title="vipy.data.celebA.CelebA" href="data/celebA.html#vipy.data.celebA.CelebA">CelebA</a></li>
<li><a title="vipy.data.coco.COCO_2014" href="data/coco.html#vipy.data.coco.COCO_2014">COCO_2014</a></li>
<li><a title="vipy.data.coil100.COIL100" href="data/coil100.html#vipy.data.coil100.COIL100">COIL100</a></li>
<li><a title="vipy.data.d2d.D2D" href="data/d2d.html#vipy.data.d2d.D2D">D2D</a></li>
<li><a title="vipy.data.ethzshapes.ETHZShapes" href="data/ethzshapes.html#vipy.data.ethzshapes.ETHZShapes">ETHZShapes</a></li>
<li><a title="vipy.data.eurosat.EuroSAT" href="data/eurosat.html#vipy.data.eurosat.EuroSAT">EuroSAT</a></li>
<li><a title="vipy.data.food101.Food101" href="data/food101.html#vipy.data.food101.Food101">Food101</a></li>
<li><a title="vipy.data.food2k.Food2k" href="data/food2k.html#vipy.data.food2k.Food2k">Food2k</a></li>
<li><a title="vipy.data.imagenet.Imagenet21K" href="data/imagenet.html#vipy.data.imagenet.Imagenet21K">Imagenet21K</a></li>
<li><a title="vipy.data.imagenet.Imagenet21K_Resized" href="data/imagenet.html#vipy.data.imagenet.Imagenet21K_Resized">Imagenet21K_Resized</a></li>
<li><a title="vipy.data.inaturalist.iNaturalist2021" href="data/inaturalist.html#vipy.data.inaturalist.iNaturalist2021">iNaturalist2021</a></li>
<li><a title="vipy.data.kitti.KITTI" href="data/kitti.html#vipy.data.kitti.KITTI">KITTI</a></li>
<li><a title="vipy.data.lfw.LFW" href="data/lfw.html#vipy.data.lfw.LFW">LFW</a></li>
<li><a title="vipy.data.mit67.MIT67" href="data/mit67.html#vipy.data.mit67.MIT67">MIT67</a></li>
<li><a title="vipy.data.objectnet.Objectnet" href="data/objectnet.html#vipy.data.objectnet.Objectnet">Objectnet</a></li>
<li><a title="vipy.data.oxford_flowers_102.Flowers102" href="data/oxford_flowers_102.html#vipy.data.oxford_flowers_102.Flowers102">Flowers102</a></li>
<li><a title="vipy.data.pip.PIP_175k" href="data/pip.html#vipy.data.pip.PIP_175k">PIP_175k</a></li>
<li><a title="vipy.data.pip.PIP_370k_stabilized" href="data/pip.html#vipy.data.pip.PIP_370k_stabilized">PIP_370k_stabilized</a></li>
<li><a title="vipy.data.stanford_cars.StanfordCars" href="data/stanford_cars.html#vipy.data.stanford_cars.StanfordCars">StanfordCars</a></li>
<li><a title="vipy.data.stanford_dogs.StanfordDogs" href="data/stanford_dogs.html#vipy.data.stanford_dogs.StanfordDogs">StanfordDogs</a></li>
<li><a title="vipy.data.ucf101.UCF101" href="data/ucf101.html#vipy.data.ucf101.UCF101">UCF101</a></li>
<li><a title="vipy.data.visualgenome.VisualGenome" href="data/visualgenome.html#vipy.data.visualgenome.VisualGenome">VisualGenome</a></li>
<li><a title="vipy.data.widerface.WiderFace" href="data/widerface.html#vipy.data.widerface.WiderFace">WiderFace</a></li>
<li><a title="vipy.data.youtubeBB.YoutubeBB" href="data/youtubeBB.html#vipy.data.youtubeBB.YoutubeBB">YoutubeBB</a></li>
<li><a title="vipy.dataset.Paged" href="#vipy.dataset.Paged">Paged</a></li>
<li><a title="vipy.dataset.Union" href="#vipy.dataset.Union">Union</a></li>
</ul>
<h3>Static methods</h3>
<dl>
<dt id="vipy.dataset.Dataset.cast"><code class="name flex">
<span>def <span class="ident">cast</span></span>(<span>obj)</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="vipy.dataset.Dataset.from_directory"><code class="name flex">
<span>def <span class="ident">from_directory</span></span>(<span>indir, filetype='json', id=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Recursively search indir for filetype, construct a dataset from all discovered files of that type</p></div>
</dd>
<dt id="vipy.dataset.Dataset.from_image_urls"><code class="name flex">
<span>def <span class="ident">from_image_urls</span></span>(<span>urls, id=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Construct a dataset from a list of image URLs</p></div>
</dd>
<dt id="vipy.dataset.Dataset.from_json"><code class="name flex">
<span>def <span class="ident">from_json</span></span>(<span>jsonfile, id=None)</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="vipy.dataset.Dataset.identity_shuffler"><code class="name flex">
<span>def <span class="ident">identity_shuffler</span></span>(<span>D)</span>
</code></dt>
<dd>
<div class="desc"><p>Shuffler that does nothing</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/visym/vipy/blob/095a3bb922809af0de26f3bbf8f0fb8f9c4157a4/vipy/dataset.py#L553-L556" class="git-link">Browse git</a>
</summary>
<pre><code class="python">@staticmethod
def identity_shuffler(D):
    &#34;&#34;&#34;Shuffler that does nothing&#34;&#34;&#34;
    return D</code></pre>
</details>
</dd>
<dt id="vipy.dataset.Dataset.streaming_shuffler"><code class="name flex">
<span>def <span class="ident">streaming_shuffler</span></span>(<span>D)</span>
</code></dt>
<dd>
<div class="desc"><p>A uniform shuffle (approximation) on the dataset elements for iterable access only</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/visym/vipy/blob/095a3bb922809af0de26f3bbf8f0fb8f9c4157a4/vipy/dataset.py#L534-L551" class="git-link">Browse git</a>
</summary>
<pre><code class="python">@staticmethod
def streaming_shuffler(D):
    &#34;&#34;&#34;A uniform shuffle (approximation) on the dataset elements for iterable access only&#34;&#34;&#34;
    assert D._idx is None, &#34;streaming only&#34;
    
    try_import(&#39;datasets&#39;, &#39;datasets&#39;); from datasets import Dataset as HuggingfaceDataset;
    
    if isinstance(D._ds, (list, tuple)):
        D._ds = list(D._ds)
        random.shuffle(D._ds)  # in-place shuffle objects
            
    elif isinstance(D._ds, HuggingfaceDataset):
        # Special case: Arrow backed dataset            
        D._ds = D._ds.to_iterable_dataset()  # no random access
        D._ds.shuffle()  # approximate shuffling for IterableDataset is much more efficient for __iter__
    else:
        raise ValueError(&#39;shuffle error&#39;)
    return D</code></pre>
</details>
</dd>
<dt id="vipy.dataset.Dataset.uniform_shuffler"><code class="name flex">
<span>def <span class="ident">uniform_shuffler</span></span>(<span>D)</span>
</code></dt>
<dd>
<div class="desc"><p>A uniform shuffle on the dataset elements.
Iterable access will be slow due to random access</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/visym/vipy/blob/095a3bb922809af0de26f3bbf8f0fb8f9c4157a4/vipy/dataset.py#L527-L532" class="git-link">Browse git</a>
</summary>
<pre><code class="python">@staticmethod
def uniform_shuffler(D):
    &#34;&#34;&#34;A uniform shuffle on the dataset elements.  Iterable access will be slow due to random access&#34;&#34;&#34;
    idx = D.index()
    random.shuffle(idx)
    return D.index(idx)</code></pre>
</details>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="vipy.dataset.Dataset.all"><code class="name flex">
<span>def <span class="ident">all</span></span>(<span>self, mapper)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/visym/vipy/blob/095a3bb922809af0de26f3bbf8f0fb8f9c4157a4/vipy/dataset.py#L203-L204" class="git-link">Browse git</a>
</summary>
<pre><code class="python">def all(self, mapper):
    return self.tuple(mapper=mapper, reducer=all)</code></pre>
</details>
</dd>
<dt id="vipy.dataset.Dataset.balanced"><code class="name flex">
<span>def <span class="ident">balanced</span></span>(<span>self, f)</span>
</code></dt>
<dd>
<div class="desc"><p>Is the dataset balanced (e.g. the frequencies returned from the lambda f are all the same)?</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/visym/vipy/blob/095a3bb922809af0de26f3bbf8f0fb8f9c4157a4/vipy/dataset.py#L210-L212" class="git-link">Browse git</a>
</summary>
<pre><code class="python">def balanced(self, f):
    &#34;&#34;&#34;Is the dataset balanced (e.g. the frequencies returned from the lambda f are all the same)?&#34;&#34;&#34;
    return len(set(self.frequency(f).values())) == 1</code></pre>
</details>
</dd>
<dt id="vipy.dataset.Dataset.batch"><code class="name flex">
<span>def <span class="ident">batch</span></span>(<span>self, n)</span>
</code></dt>
<dd>
<div class="desc"><p>Yield batches of size n as datasets.
Last batch will be ragged.
Batches are not loaded.
Batches have appended id equal to the zero-indexed batch order</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/visym/vipy/blob/095a3bb922809af0de26f3bbf8f0fb8f9c4157a4/vipy/dataset.py#L282-L285" class="git-link">Browse git</a>
</summary>
<pre><code class="python">def batch(self, n):
    &#34;&#34;&#34;Yield batches of size n as datasets.  Last batch will be ragged.  Batches are not loaded.  Batches have appended id equal to the zero-indexed batch order&#34;&#34;&#34;
    for (k,b) in enumerate(chunkgenbysize(self, n)):  
        yield Dataset(b).id(&#39;%s:%d&#39; % (self.id() if self.id() else &#39;&#39;, k))</code></pre>
</details>
</dd>
<dt id="vipy.dataset.Dataset.chunk"><code class="name flex">
<span>def <span class="ident">chunk</span></span>(<span>self, n)</span>
</code></dt>
<dd>
<div class="desc"><p>Yield n chunks as list.
Last chunk will be ragged.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/visym/vipy/blob/095a3bb922809af0de26f3bbf8f0fb8f9c4157a4/vipy/dataset.py#L277-L280" class="git-link">Browse git</a>
</summary>
<pre><code class="python">def chunk(self, n):
    &#34;&#34;&#34;Yield n chunks as list.  Last chunk will be ragged.&#34;&#34;&#34;
    for (k,c) in enumerate(chunkgen(self, n)):
        yield list(c)</code></pre>
</details>
</dd>
<dt id="vipy.dataset.Dataset.chunks"><code class="name flex">
<span>def <span class="ident">chunks</span></span>(<span>self, sizes)</span>
</code></dt>
<dd>
<div class="desc"><p>Partition the dataset into chunks of size given by the tuple in partitions, and give the dataset suffix if provided</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/visym/vipy/blob/095a3bb922809af0de26f3bbf8f0fb8f9c4157a4/vipy/dataset.py#L380-L389" class="git-link">Browse git</a>
</summary>
<pre><code class="python">def chunks(self, sizes):
    &#34;&#34;&#34;Partition the dataset into chunks of size given by the tuple in partitions, and give the dataset suffix if provided&#34;&#34;&#34;
    assert sum(sizes) == len(self)

    i = 0
    datasets = []
    for n in sizes:
        datasets.append(self.clone().index(self.index()[i:i+n]))
        i += n
    return datasets</code></pre>
</details>
</dd>
<dt id="vipy.dataset.Dataset.clone"><code class="name flex">
<span>def <span class="ident">clone</span></span>(<span>self, deep=False)</span>
</code></dt>
<dd>
<div class="desc"><p>Return a copy of the dataset object</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/visym/vipy/blob/095a3bb922809af0de26f3bbf8f0fb8f9c4157a4/vipy/dataset.py#L166-L171" class="git-link">Browse git</a>
</summary>
<pre><code class="python">def clone(self, deep=False):
    &#34;&#34;&#34;Return a copy of the dataset object&#34;&#34;&#34;
    if not deep:
        return copy.copy(self) 
    else:
        return copy.deepcopy(self)</code></pre>
</details>
</dd>
<dt id="vipy.dataset.Dataset.count"><code class="name flex">
<span>def <span class="ident">count</span></span>(<span>self, f)</span>
</code></dt>
<dd>
<div class="desc"><p>Counts for each element for which lamba returns true.
</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>f</code></strong></dt>
<dd>[lambda] if provided, count the number of elements that return true.
</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>A length of elements that satisfy f(v) = True [if f is not None]</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/visym/vipy/blob/095a3bb922809af0de26f3bbf8f0fb8f9c4157a4/vipy/dataset.py#L214-L223" class="git-link">Browse git</a>
</summary>
<pre><code class="python">def count(self, f):
    &#34;&#34;&#34;Counts for each element for which lamba returns true.  
    
    Args:
        f: [lambda] if provided, count the number of elements that return true.  

    Returns:
        A length of elements that satisfy f(v) = True [if f is not None]
    &#34;&#34;&#34;
    return len(self.tuple(f, reducer=lambda X: [x for x in X if x is True]))</code></pre>
</details>
</dd>
<dt id="vipy.dataset.Dataset.countby"><code class="name flex">
<span>def <span class="ident">countby</span></span>(<span>self, f)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/visym/vipy/blob/095a3bb922809af0de26f3bbf8f0fb8f9c4157a4/vipy/dataset.py#L225-L226" class="git-link">Browse git</a>
</summary>
<pre><code class="python">def countby(self, f):
    return self.frequency(f)</code></pre>
</details>
</dd>
<dt id="vipy.dataset.Dataset.even_split"><code class="name flex">
<span>def <span class="ident">even_split</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Split the dataset into two datasets, each half the size of the dataset.
If the dataset length is odd, then one element will be dropped</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/visym/vipy/blob/095a3bb922809af0de26f3bbf8f0fb8f9c4157a4/vipy/dataset.py#L435-L437" class="git-link">Browse git</a>
</summary>
<pre><code class="python">def even_split(self):
    &#34;&#34;&#34;Split the dataset into two datasets, each half the size of the dataset.  If the dataset length is odd, then one element will be dropped&#34;&#34;&#34;
    return self.chunks((len(self)//2, len(self)//2, len(self)%2))[0:2]</code></pre>
</details>
</dd>
<dt id="vipy.dataset.Dataset.filter"><code class="name flex">
<span>def <span class="ident">filter</span></span>(<span>self, f)</span>
</code></dt>
<dd>
<div class="desc"><p>In place filter with lambda function f, keeping those elements obj in-place where f(obj) evaluates true.
Callable should return bool</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/visym/vipy/blob/095a3bb922809af0de26f3bbf8f0fb8f9c4157a4/vipy/dataset.py#L228-L231" class="git-link">Browse git</a>
</summary>
<pre><code class="python">def filter(self, f):
    &#34;&#34;&#34;In place filter with lambda function f, keeping those elements obj in-place where f(obj) evaluates true.  Callable should return bool&#34;&#34;&#34;
    assert callable(f)
    return self.index( [i for (b,i) in zip(self.localmap(f), self.index()) if b] )</code></pre>
</details>
</dd>
<dt id="vipy.dataset.Dataset.frequency"><code class="name flex">
<span>def <span class="ident">frequency</span></span>(<span>self, f)</span>
</code></dt>
<dd>
<div class="desc"><p>Frequency counts for which lambda returns the same value.
For example f=lambda im: im.category() returns a dictionary of category names and counts in this category</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/visym/vipy/blob/095a3bb922809af0de26f3bbf8f0fb8f9c4157a4/vipy/dataset.py#L206-L208" class="git-link">Browse git</a>
</summary>
<pre><code class="python">def frequency(self, f):
    &#34;&#34;&#34;Frequency counts for which lambda returns the same value.  For example f=lambda im: im.category() returns a dictionary of category names and counts in this category&#34;&#34;&#34;
    return countby(self.tuple(mapper=f))</code></pre>
</details>
</dd>
<dt id="vipy.dataset.Dataset.groupby"><code class="name flex">
<span>def <span class="ident">groupby</span></span>(<span>self, f)</span>
</code></dt>
<dd>
<div class="desc"><p>Group the dataset according to the callable f, returning dictionary of grouped datasets.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/visym/vipy/blob/095a3bb922809af0de26f3bbf8f0fb8f9c4157a4/vipy/dataset.py#L240-L243" class="git-link">Browse git</a>
</summary>
<pre><code class="python">def groupby(self, f):
    &#34;&#34;&#34;Group the dataset according to the callable f, returning dictionary of grouped datasets.&#34;&#34;&#34;
    assert callable(f)        
    return {k:self.clone().index([x[1] for x in v]).id(&#39;%s:%s&#39; % (self.id(),str(k))) for (k,v) in itertools.groupby(enumerate(self.sort(f).index()), lambda x: f(self[x[0]]))}</code></pre>
</details>
</dd>
<dt id="vipy.dataset.Dataset.id"><code class="name flex">
<span>def <span class="ident">id</span></span>(<span>self, new_id=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Change the dataset ID to the provided ID, or return it if None</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/visym/vipy/blob/095a3bb922809af0de26f3bbf8f0fb8f9c4157a4/vipy/dataset.py#L148-L153" class="git-link">Browse git</a>
</summary>
<pre><code class="python">def id(self, new_id=None):
    &#34;&#34;&#34;Change the dataset ID to the provided ID, or return it if None&#34;&#34;&#34;
    if new_id is not None:
        self._id = new_id
        return self
    return self._id</code></pre>
</details>
</dd>
<dt id="vipy.dataset.Dataset.index"><code class="name flex">
<span>def <span class="ident">index</span></span>(<span>self, index=None, strict=False)</span>
</code></dt>
<dd>
<div class="desc"><p>Update the index, useful for filtering of large datasets</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/visym/vipy/blob/095a3bb922809af0de26f3bbf8f0fb8f9c4157a4/vipy/dataset.py#L155-L163" class="git-link">Browse git</a>
</summary>
<pre><code class="python">def index(self, index=None, strict=False):
    &#34;&#34;&#34;Update the index, useful for filtering of large datasets&#34;&#34;&#34;
    if index is not None:
        assert not strict or index is None or (len(index)&gt;0 and len(index)&lt;=len(self) and max(index)&lt;len(self) and min(index)&gt;=0)            
        self._idx = index
        return self
    if self._idx is None:
        self._idx = list(range(len(self._ds)))  # on-demand index, only if underlying dataset has known length
    return self._idx</code></pre>
</details>
</dd>
<dt id="vipy.dataset.Dataset.inverse_frequency"><code class="name flex">
<span>def <span class="ident">inverse_frequency</span></span>(<span>self, f)</span>
</code></dt>
<dd>
<div class="desc"><p>Return the inverse frequency of elements grouped by the callable f.
Returns a dictionary of the callable output to inverse frequency</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/visym/vipy/blob/095a3bb922809af0de26f3bbf8f0fb8f9c4157a4/vipy/dataset.py#L267-L271" class="git-link">Browse git</a>
</summary>
<pre><code class="python">def inverse_frequency(self, f):
    &#34;&#34;&#34;Return the inverse frequency of elements grouped by the callable f.  Returns a dictionary of the callable output to inverse frequency &#34;&#34;&#34;
    attributes = self.set(f)
    frequency = self.frequency(f)
    return {a:(1/len(attributes))*(len(self)/frequency[a]) for a in attributes}  # (normalized) inverse frequency weight</code></pre>
</details>
</dd>
<dt id="vipy.dataset.Dataset.is_streaming"><code class="name flex">
<span>def <span class="ident">is_streaming</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/visym/vipy/blob/095a3bb922809af0de26f3bbf8f0fb8f9c4157a4/vipy/dataset.py#L132-L133" class="git-link">Browse git</a>
</summary>
<pre><code class="python">def is_streaming(self):
    return self._idx is None</code></pre>
</details>
</dd>
<dt id="vipy.dataset.Dataset.len"><code class="name flex">
<span>def <span class="ident">len</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/visym/vipy/blob/095a3bb922809af0de26f3bbf8f0fb8f9c4157a4/vipy/dataset.py#L135-L136" class="git-link">Browse git</a>
</summary>
<pre><code class="python">def len(self):
    return len(self._idx) if self._idx is not None else (len(self._ds) if hasattr(self._ds, &#39;__len__&#39;) else None)</code></pre>
</details>
</dd>
<dt id="vipy.dataset.Dataset.list"><code class="name flex">
<span>def <span class="ident">list</span></span>(<span>self, mapper=None, flatten=False)</span>
</code></dt>
<dd>
<div class="desc"><p>Return a tuple as a list, loading into memory</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/visym/vipy/blob/095a3bb922809af0de26f3bbf8f0fb8f9c4157a4/vipy/dataset.py#L195-L197" class="git-link">Browse git</a>
</summary>
<pre><code class="python">def list(self, mapper=None, flatten=False):
    &#34;&#34;&#34;Return a tuple as a list, loading into memory&#34;&#34;&#34;
    return self.tuple(mapper, flatten, reducer=list)</code></pre>
</details>
</dd>
<dt id="vipy.dataset.Dataset.load"><code class="name flex">
<span>def <span class="ident">load</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Cache the entire dataset into memory</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/visym/vipy/blob/095a3bb922809af0de26f3bbf8f0fb8f9c4157a4/vipy/dataset.py#L273-L275" class="git-link">Browse git</a>
</summary>
<pre><code class="python">def load(self):
    &#34;&#34;&#34;Cache the entire dataset into memory&#34;&#34;&#34;
    return Dataset([x for x in self], id=self.id())</code></pre>
</details>
</dd>
<dt id="vipy.dataset.Dataset.localmap"><code class="name flex">
<span>def <span class="ident">localmap</span></span>(<span>self, f)</span>
</code></dt>
<dd>
<div class="desc"><p>A map performed without any parallel processing</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/visym/vipy/blob/095a3bb922809af0de26f3bbf8f0fb8f9c4157a4/vipy/dataset.py#L506-L508" class="git-link">Browse git</a>
</summary>
<pre><code class="python">def localmap(self, f):
    &#34;&#34;&#34;A map performed without any parallel processing&#34;&#34;&#34;
    return Dataset([f(x) for x in self], id=self.id())  # triggers load into memory        </code></pre>
</details>
</dd>
<dt id="vipy.dataset.Dataset.map"><code class="name flex">
<span>def <span class="ident">map</span></span>(<span>self, f_map, strict=True, oneway=False, ordered=False)</span>
</code></dt>
<dd>
<div class="desc"><p>Parallel map.</p>
<p>To perform this in parallel across four threads:</p>
<pre><code class="language-python">D = vipy.dataset.Dataset(...)
with vipy.globals.parallel(4):
    D = D.map(lambda v: ...)
</code></pre>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>f_map</code></strong></dt>
<dd>[lambda] The lambda function to apply in parallel to all elements in the dataset.
This must return a JSON serializable object (or set oneway=True)</dd>
<dt><strong><code>strict</code></strong></dt>
<dd>[bool] If true, raise exception on distributed map failures, otherwise the map will return only those that succeeded</dd>
<dt><strong><code>oneway</code></strong></dt>
<dd>[bool] If true, do not pass back results unless exception.
This is useful for distributed processing</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>A <code><a title="vipy.dataset.Dataset" href="#vipy.dataset.Dataset">Dataset</a></code> containing the elements f_map(v).
This operation is order preserving if ordered=True.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<ul>
<li>This method uses dask distributed and <code><a title="vipy.batch.Batch" href="batch.html#vipy.batch.Batch">Batch</a></code> operations</li>
<li>Due to chunking, all error handling is caught by this method.
Use <code><a title="vipy.batch.Batch" href="batch.html#vipy.batch.Batch">Batch</a></code> to leverage dask distributed futures error handling.</li>
<li>Operations must be chunked and serialized because each dask task comes with overhead, and lots of small tasks violates best practices</li>
<li>Serialized results are deserialized by the client and returned a a new dataset</li>
</ul>
</div></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/visym/vipy/blob/095a3bb922809af0de26f3bbf8f0fb8f9c4157a4/vipy/dataset.py#L443-L504" class="git-link">Browse git</a>
</summary>
<pre><code class="python">def map(self, f_map, strict=True, oneway=False, ordered=False):        
    &#34;&#34;&#34;Parallel map.

    To perform this in parallel across four threads:

    ```python
    D = vipy.dataset.Dataset(...)
    with vipy.globals.parallel(4):
        D = D.map(lambda v: ...)
    ```

    Args:
        f_map: [lambda] The lambda function to apply in parallel to all elements in the dataset.  This must return a JSON serializable object (or set oneway=True)
        strict: [bool] If true, raise exception on distributed map failures, otherwise the map will return only those that succeeded
        oneway: [bool] If true, do not pass back results unless exception.  This is useful for distributed processing
    
    Returns:
        A `vipy.dataset.Dataset` containing the elements f_map(v).  This operation is order preserving if ordered=True.

    .. note:: 
        - This method uses dask distributed and `vipy.batch.Batch` operations
        - Due to chunking, all error handling is caught by this method.  Use `vipy.batch.Batch` to leverage dask distributed futures error handling.
        - Operations must be chunked and serialized because each dask task comes with overhead, and lots of small tasks violates best practices
        - Serialized results are deserialized by the client and returned a a new dataset
    &#34;&#34;&#34;
    assert f_map is None or callable(f_map), &#34;invalid map function&#34;

    # Identity
    if f_map is None:
        return self        

    # Parallel map 
    elif vipy.globals.cf() is not None:
        # This will fail on multiprocessing if dataset contains a loader lambda, or any element in the dataset contains a loader.  Use distributed instead
        assert ordered == False, &#34;not order preserving, use localmap()&#34;
        return Dataset(tuple(vipy.parallel.map(f_map, self)), id=self.id()) 
                                          
    # Distributed map
    elif vipy.globals.dask() is not None:
        from vipy.batch import Batch   # requires pip install vipy[all]                
        f_serialize = lambda x: x
        f_deserialize = lambda x: x
        f_oneway = lambda x, oneway=oneway: x if not x[0] or not oneway else (x[0], None)
        f_catcher = lambda f, *args, **kwargs: catcher(f, *args, **kwargs)  # catch exceptions when executing lambda, return (True, result) or (False, exception)
        f = lambda x, f_serializer=f_serialize, f_deserializer=f_deserialize, f_map=f_map, f_catcher=f_catcher, f_oneway=f_oneway: f_serializer(f_oneway(f_catcher(f_map, f_deserializer(x))))  # with closure capture
        
        S = [f_serialize(v) for v in self]  # local load, preprocess and serialize
        B = Batch(chunklist(S, 128), strict=False, warnme=False, minscatter=128)
        S = B.map(lambda X,f=f: [f(x) for x in X]).result()  # distributed, chunked, with caught exceptions, may return empty list
        V = [f_deserialize(x) for s in S for x in s]  # Local deserialization and chunk flattening
        
        # Error handling
        (good, bad) = ([r for (b,r) in V if b], [r for (b,r) in V if not b])  # catcher returns (True, result) or (False, exception string)
        if len(bad)&gt;0:
            log.warning(&#39;Exceptions in distributed processing:\n%s\n\n[vipy.dataset.Dataset.map]: %d/%d items failed&#39; % (str(bad), len(bad), len(self)))
            if strict:
                raise ValueError(&#39;exceptions in distributed processing&#39;)
        return Dataset(good, id=self.id()) if not oneway else None

    # Local map
    else:
        return self.localmap(f_map)</code></pre>
</details>
</dd>
<dt id="vipy.dataset.Dataset.minibatch"><code class="name flex">
<span>def <span class="ident">minibatch</span></span>(<span>self, n, ragged=True, loader=None, bufsize=1024, accepter=None, preprocessor=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Yield preprocessed minibatches of size n of this dataset.</p>
<p>To yield chunks of this dataset, suitable for minibatch training/testing</p>
<pre><code class="language-python">D = vipy.dataset.Dataset(...)
for b in D.minibatch(n):
   print(b)
</code></pre>
<p>To perform minibatch image downloading in parallel across four processes with the context manager:</p>
<pre><code class="language-python">D = vipy.dataset.registry('yfcc100m_url:train').take(128)
with vipy.globals.parallel(4):
    for b in D.minibatch(16, loader=vipy.image.Transform.download, accepter=lambda im: im.is_downloaded()):
        print(b)  # complete minibatch that passed accepter
</code></pre>
<h2 id="args">Args</h2>
<p>n [int]: The size of the minibatch
ragged [bool]: If ragged=true, then the last chunk will be ragged with len(chunk)&lt;n, else skipped
bufsize [int]:
The size of the buffer used in parallel processing of elements.
Useful for parallel loading
accepter [callable]:
A callable that returns true|false on an element, where only elements that return true are included in the minibatch.
useful for parallel loading of elements that may fail to download
loader [callable]: A callable that is applied to every element of the dataset.
Useful for parallel loading
Returns:
<br>
Iterator over <code><a title="vipy.dataset.Dataset" href="#vipy.dataset.Dataset">Dataset</a></code> elements of length n.
Minibatches will be yielded loaded and preprocessed (processing done concurrently if vipy.parallel.executor() is initialized)</p>
<div class="admonition note">
<p class="admonition-title">Note:&ensp;The distributed iterator appends the minibatch index to the minibatch.id().
</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note:&ensp;If there exists a vipy.parallel.exeuctor(), then loading and preprocessing will be performed concurrently</p>
</div></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/visym/vipy/blob/095a3bb922809af0de26f3bbf8f0fb8f9c4157a4/vipy/dataset.py#L287-L323" class="git-link">Browse git</a>
</summary>
<pre><code class="python">def minibatch(self, n, ragged=True, loader=None, bufsize=1024, accepter=None, preprocessor=None):
    &#34;&#34;&#34;Yield preprocessed minibatches of size n of this dataset.

    To yield chunks of this dataset, suitable for minibatch training/testing

    ```python
    D = vipy.dataset.Dataset(...)
    for b in D.minibatch(n):
       print(b)
    ```
    
    To perform minibatch image downloading in parallel across four processes with the context manager:

    ```python
    D = vipy.dataset.registry(&#39;yfcc100m_url:train&#39;).take(128)
    with vipy.globals.parallel(4):
        for b in D.minibatch(16, loader=vipy.image.Transform.download, accepter=lambda im: im.is_downloaded()):
            print(b)  # complete minibatch that passed accepter
    ```

    Args:
        n [int]: The size of the minibatch
        ragged [bool]: If ragged=true, then the last chunk will be ragged with len(chunk)&lt;n, else skipped
        bufsize [int]:  The size of the buffer used in parallel processing of elements.  Useful for parallel loading
        accepter [callable]:  A callable that returns true|false on an element, where only elements that return true are included in the minibatch.  useful for parallel loading of elements that may fail to download
        loader [callable]: A callable that is applied to every element of the dataset.  Useful for parallel loading

    Returns:        
        Iterator over `vipy.dataset.Dataset` elements of length n.  Minibatches will be yielded loaded and preprocessed (processing done concurrently if vipy.parallel.executor() is initialized)

    ..note:: The distributed iterator appends the minibatch index to the minibatch.id().  
    ..note:: If there exists a vipy.parallel.exeuctor(), then loading and preprocessing will be performed concurrently

    &#34;&#34;&#34;
    for (k,b) in enumerate(chunkgenbysize(vipy.parallel.iter(self, mapper=loader, bufsize=max(bufsize,n), accepter=accepter), n)): 
        if ragged or len(b) == n:
            yield Dataset.cast(b).id(&#39;%s:%d&#39; % (self.id() if self.id() else &#39;&#39;, k))                    </code></pre>
</details>
</dd>
<dt id="vipy.dataset.Dataset.partition"><code class="name flex">
<span>def <span class="ident">partition</span></span>(<span>self, trainfraction=0.9, valfraction=0.1, testfraction=0, trainsuffix=':train', valsuffix=':val', testsuffix=':test')</span>
</code></dt>
<dd>
<div class="desc"><p>Partition the dataset into the requested (train,val,test) fractions.
</p>
<h2 id="args">Args</h2>
<dl>
<dt>trainfraction [float]: fraction of dataset for training set</dt>
<dt>valfraction [float]: fraction of dataset for validation set</dt>
<dt>testfraction [float]: fraction of dataset for test set</dt>
<dt><strong><code>trainsuffix</code></strong></dt>
<dd>If not None, append this string the to trainset ID</dd>
<dt><strong><code>valsuffix</code></strong></dt>
<dd>If not None, append this string the to valset ID</dd>
<dt><strong><code>testsuffix</code></strong></dt>
<dd>If not None, append this string the to testset ID
</dd>
</dl>
<p>Returns:
<br>
(trainset, valset, testset) such that trainset is the first trainfraction of the dataset.
</p>
<div class="admonition note">
<p class="admonition-title">Note:&ensp;This does not permute the dataset.
To randomize split, shuffle dataset first</p>
</div></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/visym/vipy/blob/095a3bb922809af0de26f3bbf8f0fb8f9c4157a4/vipy/dataset.py#L391-L428" class="git-link">Browse git</a>
</summary>
<pre><code class="python">def partition(self, trainfraction=0.9, valfraction=0.1, testfraction=0, trainsuffix=&#39;:train&#39;, valsuffix=&#39;:val&#39;, testsuffix=&#39;:test&#39;):
    &#34;&#34;&#34;Partition the dataset into the requested (train,val,test) fractions.  

    Args:
        trainfraction [float]: fraction of dataset for training set
        valfraction [float]: fraction of dataset for validation set
        testfraction [float]: fraction of dataset for test set
        trainsuffix: If not None, append this string the to trainset ID
        valsuffix: If not None, append this string the to valset ID
        testsuffix: If not None, append this string the to testset ID        
    
    Returns:        
        (trainset, valset, testset) such that trainset is the first trainfraction of the dataset.  

    .. note:: This does not permute the dataset.  To randomize split, shuffle dataset first

    &#34;&#34;&#34;
    assert trainfraction &gt;=0 and trainfraction &lt;= 1, &#34;invalid training set fraction &#39;%f&#39;&#34; % trainfraction
    assert valfraction &gt;=0 and valfraction &lt;= 1, &#34;invalid validation set fraction &#39;%f&#39;&#34; % valfraction
    assert testfraction &gt;=0 and testfraction &lt;= 1, &#34;invalid test set fraction &#39;%f&#39;&#34; % testfraction
    assert abs(trainfraction + valfraction + testfraction - 1) &lt; 1E-6, &#34;fractions must sum to one&#34;
    
    idx = self.index()
    (testidx, validx, trainidx) = dividelist(idx, (testfraction, valfraction, trainfraction))
        
    trainset = self.clone().index(trainidx)
    if trainsuffix and trainset.id():
        trainset.id(trainset.id() + trainsuffix)
    
    valset = self.clone().index(validx)
    if valsuffix and valset.id():
        valset.id(valset.id() + valsuffix)
    
    testset = self.clone().index(testidx)
    if testsuffix and testset.id():
        testset.id(testset.id() + testsuffix)
            
    return (trainset,valset,testset) if testfraction!=0 else (trainset, valset)</code></pre>
</details>
</dd>
<dt id="vipy.dataset.Dataset.pipeline"><code class="name flex">
<span>def <span class="ident">pipeline</span></span>(<span>self, n, m, ragged=True, prepad=True, postpad=True)</span>
</code></dt>
<dd>
<div class="desc"><p>Yield pipelined minibatches of size n with pipeline length m.</p>
<p>A pipelined minibatch is a tuple (head, tail) such that (head, tail) are minibatches at different indexes in the dataset.<br>
Head corresponds to the current minibatch and tail corresponds to the minibatch left shifted by (m-1) minibatches.</p>
<p>This structure is useful for yielding datasets for pipelined training where head contains the minibatch that will complete pipeline training on this iteration, and tail contains the
next minibatch to be inserted into the pipeline on this iteration.</p>
<p>```python
D = vipy.dataset.Dataset(&hellip;)
for (head, tail) in D.pipeline(n, m, prepad=False, postpad=False):
assert head == D[0:m]
assert tail == D[n<em>(m-1): n</em>(m-1)+n]</p>
<h2 id="args">Args</h2>
<dl>
<dt>n [int]: The size of each minibatch</dt>
<dt>m [int]:
The pipeline length in minibatches</dt>
<dt>ragged [bool]: If ragged=true, then the last chunk will be ragged with len(chunk)&lt;n, else skipped</dt>
<dt><strong><code>prepad</code></strong></dt>
<dd>If true, yield (head, tail) == (None, batch) when filling the pipeline</dd>
<dt><strong><code>postpad</code></strong></dt>
<dd>If true, yield (head, tail) == (batch, None) when flushing the pipeline</dd>
</dl>
<p>Returns:
<br>
Iterator over tuples (head,tail) of <code><a title="vipy.dataset.Dataset" href="#vipy.dataset.Dataset">Dataset</a></code> elements of length n where tail is left shifted by n*(m-1) elements. </p>
<div class="admonition note">
<p class="admonition-title">Note:&ensp;The distributed iterator is not order preserving over minibatches and yields minibatches as completed, however the tuple (head, tail) is order preserving within the pipeline</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note:&ensp;If there exists a vipy.parallel.executor(), then loading and preprocessing will be performed concurrently</p>
</div></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/visym/vipy/blob/095a3bb922809af0de26f3bbf8f0fb8f9c4157a4/vipy/dataset.py#L338-L377" class="git-link">Browse git</a>
</summary>
<pre><code class="python">def pipeline(self, n, m, ragged=True, prepad=True, postpad=True):
    &#34;&#34;&#34;Yield pipelined minibatches of size n with pipeline length m.

    A pipelined minibatch is a tuple (head, tail) such that (head, tail) are minibatches at different indexes in the dataset.  
    Head corresponds to the current minibatch and tail corresponds to the minibatch left shifted by (m-1) minibatches.

    This structure is useful for yielding datasets for pipelined training where head contains the minibatch that will complete pipeline training on this iteration, and tail contains the 
    next minibatch to be inserted into the pipeline on this iteration.
    
    ```python
    D = vipy.dataset.Dataset(...)
    for (head, tail) in D.pipeline(n, m, prepad=False, postpad=False):
        assert head == D[0:m]
        assert tail == D[n*(m-1): n*(m-1)+n]

    Args:
        n [int]: The size of each minibatch
        m [int]:  The pipeline length in minibatches
        ragged [bool]: If ragged=true, then the last chunk will be ragged with len(chunk)&lt;n, else skipped
        prepad: If true, yield (head, tail) == (None, batch) when filling the pipeline
        postpad: If true, yield (head, tail) == (batch, None) when flushing the pipeline
    
    Returns:        
        Iterator over tuples (head,tail) of `vipy.dataset.Dataset` elements of length n where tail is left shifted by n*(m-1) elements. 
    
    .. note::  The distributed iterator is not order preserving over minibatches and yields minibatches as completed, however the tuple (head, tail) is order preserving within the pipeline
    .. note:: If there exists a vipy.parallel.executor(), then loading and preprocessing will be performed concurrently
    
    &#34;&#34;&#34;
    pipeline = [] 
    for (k,b) in enumerate(self.minibatch(n, ragged=ragged)):  # not order preserving
        pipeline.append(b)  # order preserving within pipeline                        
        if k &lt; m-1:
            if prepad:
                yield( (None, b) )  
        else:
            yield( (pipeline.pop(0), b) )  # yield deque-like (minibatch, shifted minibatch) tuples
    for p in pipeline:
        if postpad:
            yield( (p, None) )</code></pre>
</details>
</dd>
<dt id="vipy.dataset.Dataset.raw"><code class="name flex">
<span>def <span class="ident">raw</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Return a view of this dataset without the loader</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/visym/vipy/blob/095a3bb922809af0de26f3bbf8f0fb8f9c4157a4/vipy/dataset.py#L128-L130" class="git-link">Browse git</a>
</summary>
<pre><code class="python">def raw(self):
    &#34;&#34;&#34;Return a view of this dataset without the loader&#34;&#34;&#34;
    return Dataset(self._ds, loader=None)</code></pre>
</details>
</dd>
<dt id="vipy.dataset.Dataset.repeat"><code class="name flex">
<span>def <span class="ident">repeat</span></span>(<span>self, n)</span>
</code></dt>
<dd>
<div class="desc"><p>Repeat the dataset n times.
If n=0, the dataset is unchanged, if n=1 the dataset is doubled in length, etc.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/visym/vipy/blob/095a3bb922809af0de26f3bbf8f0fb8f9c4157a4/vipy/dataset.py#L181-L184" class="git-link">Browse git</a>
</summary>
<pre><code class="python">def repeat(self, n):
    &#34;&#34;&#34;Repeat the dataset n times.  If n=0, the dataset is unchanged, if n=1 the dataset is doubled in length, etc.&#34;&#34;&#34;
    assert n&gt;=0
    return self.index( self.index()*(n+1) )</code></pre>
</details>
</dd>
<dt id="vipy.dataset.Dataset.sample"><code class="name flex">
<span>def <span class="ident">sample</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Return a single element sampled uniformly at random</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/visym/vipy/blob/095a3bb922809af0de26f3bbf8f0fb8f9c4157a4/vipy/dataset.py#L258-L260" class="git-link">Browse git</a>
</summary>
<pre><code class="python">def sample(self):
    &#34;&#34;&#34;Return a single element sampled uniformly at random&#34;&#34;&#34;
    return self.takeone()</code></pre>
</details>
</dd>
<dt id="vipy.dataset.Dataset.set"><code class="name flex">
<span>def <span class="ident">set</span></span>(<span>self, mapper=None, flatten=False)</span>
</code></dt>
<dd>
<div class="desc"><p>Return the dataset as a set.
Mapper must be a lambda function that returns a hashable type</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/visym/vipy/blob/095a3bb922809af0de26f3bbf8f0fb8f9c4157a4/vipy/dataset.py#L199-L201" class="git-link">Browse git</a>
</summary>
<pre><code class="python">def set(self, mapper=None, flatten=False):
    &#34;&#34;&#34;Return the dataset as a set.  Mapper must be a lambda function that returns a hashable type&#34;&#34;&#34;
    return self.tuple(mapper=mapper, reducer=set, flatten=flatten)        </code></pre>
</details>
</dd>
<dt id="vipy.dataset.Dataset.shift"><code class="name flex">
<span>def <span class="ident">shift</span></span>(<span>self, m)</span>
</code></dt>
<dd>
<div class="desc"><p>Circular shift the dataset m elements to the left, so that self[k+m] == self.shift(m)[k].
Circular shift for boundary handling so that self.shift(m)[-1] == self[m-1]</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/visym/vipy/blob/095a3bb922809af0de26f3bbf8f0fb8f9c4157a4/vipy/dataset.py#L326-L328" class="git-link">Browse git</a>
</summary>
<pre><code class="python">def shift(self, m):
    &#34;&#34;&#34;Circular shift the dataset m elements to the left, so that self[k+m] == self.shift(m)[k].  Circular shift for boundary handling so that self.shift(m)[-1] == self[m-1]&#34;&#34;&#34;
    return self.clone().index(self.index()[m:] + self.index()[0:m])</code></pre>
</details>
</dd>
<dt id="vipy.dataset.Dataset.shuffle"><code class="name flex">
<span>def <span class="ident">shuffle</span></span>(<span>self, shuffler=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Permute elements in this dataset uniformly at random in place using the optimal shuffling strategy for the dataset structure to maximize performance.
This method will use either Dataset.streaming_shuffler (for iterable datasets) or Dataset.uniform_shuffler (for random access datasets)</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/visym/vipy/blob/095a3bb922809af0de26f3bbf8f0fb8f9c4157a4/vipy/dataset.py#L173-L179" class="git-link">Browse git</a>
</summary>
<pre><code class="python">def shuffle(self, shuffler=None):
    &#34;&#34;&#34;Permute elements in this dataset uniformly at random in place using the optimal shuffling strategy for the dataset structure to maximize performance.
       This method will use either Dataset.streaming_shuffler (for iterable datasets) or Dataset.uniform_shuffler (for random access datasets)
    &#34;&#34;&#34;
    assert shuffler is None or callable(shuffler)
    shuffler = shuffler if shuffler is not None else (Dataset.streaming_shuffler if self.is_streaming() else Dataset.uniform_shuffler)
    return shuffler(self)</code></pre>
</details>
</dd>
<dt id="vipy.dataset.Dataset.slice"><code class="name flex">
<span>def <span class="ident">slice</span></span>(<span>self, start=0, stop=-1, step=1)</span>
</code></dt>
<dd>
<div class="desc"><p>Slice the dataset to contain elements defined by slice(start, stop, step)</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/visym/vipy/blob/095a3bb922809af0de26f3bbf8f0fb8f9c4157a4/vipy/dataset.py#L330-L332" class="git-link">Browse git</a>
</summary>
<pre><code class="python">def slice(self, start=0, stop=-1, step=1):
    &#34;&#34;&#34;Slice the dataset to contain elements defined by slice(start, stop, step)&#34;&#34;&#34;
    return self.clone().index(self.index()[start:stop:step])</code></pre>
</details>
</dd>
<dt id="vipy.dataset.Dataset.sort"><code class="name flex">
<span>def <span class="ident">sort</span></span>(<span>self, f)</span>
</code></dt>
<dd>
<div class="desc"><p>Sort the dataset in-place using the sortkey lambda function f</p>
<p>To perform a sort of the dataset using some property of the instance, such as the object category (e.g. for vipy.image.ImageCategory) </p>
<pre><code class="language-python">dataset.sort(lambda im: im.category())
</code></pre></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/visym/vipy/blob/095a3bb922809af0de26f3bbf8f0fb8f9c4157a4/vipy/dataset.py#L514-L524" class="git-link">Browse git</a>
</summary>
<pre><code class="python">def sort(self, f):
    &#34;&#34;&#34;Sort the dataset in-place using the sortkey lambda function f

    To perform a sort of the dataset using some property of the instance, such as the object category (e.g. for vipy.image.ImageCategory) 

    ```python
    dataset.sort(lambda im: im.category())
    ```
    &#34;&#34;&#34;
    idx = self.index()
    return self.index( [idx[j] for (j,x) in sorted(zip(range(len(self)), self.tuple(f)), key=lambda x: x[1])] )</code></pre>
</details>
</dd>
<dt id="vipy.dataset.Dataset.split"><code class="name flex">
<span>def <span class="ident">split</span></span>(<span>self, size)</span>
</code></dt>
<dd>
<div class="desc"><p>Split the dataset into two datasets, one of length size, the other of length len(self)-size</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/visym/vipy/blob/095a3bb922809af0de26f3bbf8f0fb8f9c4157a4/vipy/dataset.py#L430-L433" class="git-link">Browse git</a>
</summary>
<pre><code class="python">def split(self, size):
    &#34;&#34;&#34;Split the dataset into two datasets, one of length size, the other of length len(self)-size&#34;&#34;&#34;
    assert isinstance(size, int) and size&gt;=0 and size&lt;len(self)
    return self.partition(size/len(self), (len(self)-size)/len(self), 0, &#39;&#39;, &#39;&#39;, &#39;&#39;)</code></pre>
</details>
</dd>
<dt id="vipy.dataset.Dataset.streaming_map"><code class="name flex">
<span>def <span class="ident">streaming_map</span></span>(<span>self, mapper, accepter=None, bufsize=1024)</span>
</code></dt>
<dd>
<div class="desc"><p>Returns a generator that will apply the mapper and yield only those elements that return True from the accepter.
Performs the map in parallel if used in the vipy.globals.parallel context manager</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/visym/vipy/blob/095a3bb922809af0de26f3bbf8f0fb8f9c4157a4/vipy/dataset.py#L439-L441" class="git-link">Browse git</a>
</summary>
<pre><code class="python">def streaming_map(self, mapper, accepter=None, bufsize=1024):
    &#34;&#34;&#34;Returns a generator that will apply the mapper and yield only those elements that return True from the accepter.  Performs the map in parallel if used in the vipy.globals.parallel context manager&#34;&#34;&#34;
    return vipy.parallel.iter(self, mapper=mapper, accepter=accepter, bufsize=bufsize)</code></pre>
</details>
</dd>
<dt id="vipy.dataset.Dataset.take"><code class="name flex">
<span>def <span class="ident">take</span></span>(<span>self, n, inplace=False)</span>
</code></dt>
<dd>
<div class="desc"><p>Randomly Take n elements from the dataset, and return a dataset (in-place or cloned). If n is greater than the size of the dataset, sample with replacement, if n is less than the size of the dataset, sample without replacement</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/visym/vipy/blob/095a3bb922809af0de26f3bbf8f0fb8f9c4157a4/vipy/dataset.py#L233-L237" class="git-link">Browse git</a>
</summary>
<pre><code class="python">def take(self, n, inplace=False):
    &#34;&#34;&#34;Randomly Take n elements from the dataset, and return a dataset (in-place or cloned). If n is greater than the size of the dataset, sample with replacement, if n is less than the size of the dataset, sample without replacement&#34;&#34;&#34;
    assert isinstance(n, int) and n&gt;0
    D = self.clone() if not inplace else self
    return D.index(list((random.sample if n&lt;= len(self) else random.choices)(D.index(), k=n)) )</code></pre>
</details>
</dd>
<dt id="vipy.dataset.Dataset.take_fraction"><code class="name flex">
<span>def <span class="ident">take_fraction</span></span>(<span>self, p, inplace=False)</span>
</code></dt>
<dd>
<div class="desc"><p>Randomly take a percentage of the dataset, returning a clone or in-place</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/visym/vipy/blob/095a3bb922809af0de26f3bbf8f0fb8f9c4157a4/vipy/dataset.py#L262-L265" class="git-link">Browse git</a>
</summary>
<pre><code class="python">def take_fraction(self, p, inplace=False):
    &#34;&#34;&#34;Randomly take a percentage of the dataset, returning a clone or in-place&#34;&#34;&#34;
    assert p&gt;=0 and p&lt;=1, &#34;invalid fraction &#39;%s&#39;&#34; % p
    return self.take(n=int(len(self)*p), inplace=inplace)</code></pre>
</details>
</dd>
<dt id="vipy.dataset.Dataset.takeby"><code class="name flex">
<span>def <span class="ident">takeby</span></span>(<span>self, f, n)</span>
</code></dt>
<dd>
<div class="desc"><p>Filter the dataset according to the callable f, take n from each group and return a dataset.
Callable should return bool.
If n==1, return a singleton</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/visym/vipy/blob/095a3bb922809af0de26f3bbf8f0fb8f9c4157a4/vipy/dataset.py#L245-L248" class="git-link">Browse git</a>
</summary>
<pre><code class="python">def takeby(self, f, n):
    &#34;&#34;&#34;Filter the dataset according to the callable f, take n from each group and return a dataset.  Callable should return bool.  If n==1, return a singleton&#34;&#34;&#34;
    d = self.clone().filter(f)
    return d.take(n) if n&gt;1 else d.takeone()</code></pre>
</details>
</dd>
<dt id="vipy.dataset.Dataset.takelist"><code class="name flex">
<span>def <span class="ident">takelist</span></span>(<span>self, n)</span>
</code></dt>
<dd>
<div class="desc"><p>Take n elements and return list.
The elements are loaded and not cloned.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/visym/vipy/blob/095a3bb922809af0de26f3bbf8f0fb8f9c4157a4/vipy/dataset.py#L250-L252" class="git-link">Browse git</a>
</summary>
<pre><code class="python">def takelist(self, n):
    &#34;&#34;&#34;Take n elements and return list.  The elements are loaded and not cloned.&#34;&#34;&#34;
    return self.take(n).list()</code></pre>
</details>
</dd>
<dt id="vipy.dataset.Dataset.takeone"><code class="name flex">
<span>def <span class="ident">takeone</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Randomly take one element from the dataset and return a singleton</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/visym/vipy/blob/095a3bb922809af0de26f3bbf8f0fb8f9c4157a4/vipy/dataset.py#L254-L256" class="git-link">Browse git</a>
</summary>
<pre><code class="python">def takeone(self):
    &#34;&#34;&#34;Randomly take one element from the dataset and return a singleton&#34;&#34;&#34;
    return self[random.randint(0, len(self)-1)]</code></pre>
</details>
</dd>
<dt id="vipy.dataset.Dataset.truncate"><code class="name flex">
<span>def <span class="ident">truncate</span></span>(<span>self, m)</span>
</code></dt>
<dd>
<div class="desc"><p>Truncate the dataset to contain the first m elements only</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/visym/vipy/blob/095a3bb922809af0de26f3bbf8f0fb8f9c4157a4/vipy/dataset.py#L334-L336" class="git-link">Browse git</a>
</summary>
<pre><code class="python">def truncate(self, m):
    &#34;&#34;&#34;Truncate the dataset to contain the first m elements only&#34;&#34;&#34;
    return self.slice(stop=m)</code></pre>
</details>
</dd>
<dt id="vipy.dataset.Dataset.tuple"><code class="name flex">
<span>def <span class="ident">tuple</span></span>(<span>self, mapper=None, flatten=False, reducer=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Return the dataset as a tuple, applying the optional mapper lambda on each element, applying optional flattener on sequences returned by mapper, and applying the optional reducer lambda on the final tuple, return a generator</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/visym/vipy/blob/095a3bb922809af0de26f3bbf8f0fb8f9c4157a4/vipy/dataset.py#L186-L193" class="git-link">Browse git</a>
</summary>
<pre><code class="python">def tuple(self, mapper=None, flatten=False, reducer=None):
    &#34;&#34;&#34;Return the dataset as a tuple, applying the optional mapper lambda on each element, applying optional flattener on sequences returned by mapper, and applying the optional reducer lambda on the final tuple, return a generator&#34;&#34;&#34;
    assert mapper is None or callable(mapper)
    assert reducer is None or callable(reducer)
    mapped = self.map(mapper) if mapper else self
    flattened = (y for x in mapped for y in x) if flatten else (x for x in mapped)
    reduced = reducer(flattened) if reducer else flattened
    return reduced</code></pre>
</details>
</dd>
<dt id="vipy.dataset.Dataset.zip"><code class="name flex">
<span>def <span class="ident">zip</span></span>(<span>self, iter)</span>
</code></dt>
<dd>
<div class="desc"><p>Returns a new dataset constructed by applying the callable on elements from zip(self,iter)</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/visym/vipy/blob/095a3bb922809af0de26f3bbf8f0fb8f9c4157a4/vipy/dataset.py#L510-L512" class="git-link">Browse git</a>
</summary>
<pre><code class="python">def zip(self, iter):
    &#34;&#34;&#34;Returns a new dataset constructed by applying the callable on elements from zip(self,iter)&#34;&#34;&#34;
    return Dataset(zip(self,iter))</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="vipy.dataset.Paged"><code class="flex name class">
<span>class <span class="ident">Paged</span></span>
<span>(</span><span>pagelist, loader, id=None, strict=True, index=None, cachesize=32)</span>
</code></dt>
<dd>
<div class="desc"><p>Paged dataset.</p>
<p>A paged dataset is a dataset of length N=M*P constructed from M archive files (the pages) each containing P elements (the pagesize).<br>
The paged dataset must be constructed with tuples of (pagesize, filename).<br>
The loader will fetch, load and cache the pages on demand using the loader, preserving the most recently used cachesize pages</p>
<pre><code class="language-python">D = vipy.dataset.Paged([(64, 'archive1.pkl'), (64, 'archive2.pkl')], lambda x,y: ivy.load(y))
</code></pre>
<p>.. note :: Shuffling this dataset is biased.
Shuffling will be performed to mix the indexes, but not uniformly at random.
The goal is to preserve data locality to minimize cache misses.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/visym/vipy/blob/095a3bb922809af0de26f3bbf8f0fb8f9c4157a4/vipy/dataset.py#L559-L619" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class Paged(Dataset):
    &#34;&#34;&#34; Paged dataset.

    A paged dataset is a dataset of length N=M*P constructed from M archive files (the pages) each containing P elements (the pagesize).  
    The paged dataset must be constructed with tuples of (pagesize, filename).  
    The loader will fetch, load and cache the pages on demand using the loader, preserving the most recently used cachesize pages

    ```python
    D = vipy.dataset.Paged([(64, &#39;archive1.pkl&#39;), (64, &#39;archive2.pkl&#39;)], lambda x,y: ivy.load(y))
    ```

    .. note :: Shuffling this dataset is biased.  Shuffling will be performed to mix the indexes, but not uniformly at random.  The goal is to preserve data locality to minimize cache misses.
    &#34;&#34;&#34;
    
    def __init__(self, pagelist, loader, id=None, strict=True, index=None, cachesize=32):        
        super().__init__(dataset=pagelist,
                         id=id,
                         loader=loader).index(index if index else list(range(sum([p[0] for p in pagelist]))))

        assert callable(loader), &#34;page loader required&#34;
        assert not strict or len(set([x[0] for x in self._ds])) == 1  # pagesizes all the same 

        self._cachesize = cachesize
        self._pagecache = {}
        self._ds = list(self._ds)
        self._pagesize = self._ds[0][0]  # (pagesize, pklfile) tuples        

    def shuffle(self, shuffler=None):
        &#34;&#34;&#34;Permute elements while preserve page locality to minimize cache misses&#34;&#34;&#34;
        shuffler = shuffler if shuffler is not None else functools.partial(Paged.chunk_shuffler, chunksize=int(1.5*self._pagesize))
        return shuffler(self)
        
    def __getitem__(self, k):
        if isinstance(k, (int, np.uint64)):
            assert abs(k) &lt; len(self._idx), &#34;invalid index&#34;
            page = self._idx[int(k)] // self._pagesize
            if page not in self._pagecache:
                self._pagecache[page] = self._loader(*self._ds[page])  # load and cache new page
                if len(self._pagecache) &gt; self._cachesize:
                    self._pagecache.pop(list(self._pagecache.keys())[0])  # remove oldest
            x = self._pagecache[page][int(k) % self._pagesize]
            return x
        elif isinstance(k, slice):
            return [self[i] for i in range(len(self))[k.start if k.start else 0:k.stop if k.stop else len(self):k.step if k.step else 1]]  # expensive
        else:
            raise ValueError(&#39;invalid index type &#34;%s&#34;&#39; % type(k))            

    def flush(self):
        self._pagecache = {}
        return self

    @staticmethod
    def chunk_shuffler(D, chunker, chunksize=64):
        &#34;&#34;&#34;Split dataset into len(D)/chunksize non-overlapping chunks with some common property returned by chunker, shuffle chunk order and shuffle within chunks.  

           - If chunksize=1 then this is equivalent to uniform_shuffler
           - chunker must be a callable of some property that is used to group into chunks
            
        &#34;&#34;&#34;
        assert callable(chunker)
        return D.randomize().sort(chunker).index([i for I in shufflelist([shufflelist(I) for I in chunkgenbysize(D.index(), chunksize)]) for i in I])</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="vipy.dataset.Dataset" href="#vipy.dataset.Dataset">Dataset</a></li>
</ul>
<h3>Static methods</h3>
<dl>
<dt id="vipy.dataset.Paged.chunk_shuffler"><code class="name flex">
<span>def <span class="ident">chunk_shuffler</span></span>(<span>D, chunker, chunksize=64)</span>
</code></dt>
<dd>
<div class="desc"><p>Split dataset into len(D)/chunksize non-overlapping chunks with some common property returned by chunker, shuffle chunk order and shuffle within chunks.
</p>
<ul>
<li>If chunksize=1 then this is equivalent to uniform_shuffler</li>
<li>chunker must be a callable of some property that is used to group into chunks</li>
</ul></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/visym/vipy/blob/095a3bb922809af0de26f3bbf8f0fb8f9c4157a4/vipy/dataset.py#L610-L619" class="git-link">Browse git</a>
</summary>
<pre><code class="python">@staticmethod
def chunk_shuffler(D, chunker, chunksize=64):
    &#34;&#34;&#34;Split dataset into len(D)/chunksize non-overlapping chunks with some common property returned by chunker, shuffle chunk order and shuffle within chunks.  

       - If chunksize=1 then this is equivalent to uniform_shuffler
       - chunker must be a callable of some property that is used to group into chunks
        
    &#34;&#34;&#34;
    assert callable(chunker)
    return D.randomize().sort(chunker).index([i for I in shufflelist([shufflelist(I) for I in chunkgenbysize(D.index(), chunksize)]) for i in I])</code></pre>
</details>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="vipy.dataset.Paged.flush"><code class="name flex">
<span>def <span class="ident">flush</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/visym/vipy/blob/095a3bb922809af0de26f3bbf8f0fb8f9c4157a4/vipy/dataset.py#L606-L608" class="git-link">Browse git</a>
</summary>
<pre><code class="python">def flush(self):
    self._pagecache = {}
    return self</code></pre>
</details>
</dd>
<dt id="vipy.dataset.Paged.shuffle"><code class="name flex">
<span>def <span class="ident">shuffle</span></span>(<span>self, shuffler=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Permute elements while preserve page locality to minimize cache misses</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/visym/vipy/blob/095a3bb922809af0de26f3bbf8f0fb8f9c4157a4/vipy/dataset.py#L586-L589" class="git-link">Browse git</a>
</summary>
<pre><code class="python">def shuffle(self, shuffler=None):
    &#34;&#34;&#34;Permute elements while preserve page locality to minimize cache misses&#34;&#34;&#34;
    shuffler = shuffler if shuffler is not None else functools.partial(Paged.chunk_shuffler, chunksize=int(1.5*self._pagesize))
    return shuffler(self)</code></pre>
</details>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="vipy.dataset.Dataset" href="#vipy.dataset.Dataset">Dataset</a></b></code>:
<ul class="hlist">
<li><code><a title="vipy.dataset.Dataset.balanced" href="#vipy.dataset.Dataset.balanced">balanced</a></code></li>
<li><code><a title="vipy.dataset.Dataset.batch" href="#vipy.dataset.Dataset.batch">batch</a></code></li>
<li><code><a title="vipy.dataset.Dataset.chunk" href="#vipy.dataset.Dataset.chunk">chunk</a></code></li>
<li><code><a title="vipy.dataset.Dataset.chunks" href="#vipy.dataset.Dataset.chunks">chunks</a></code></li>
<li><code><a title="vipy.dataset.Dataset.clone" href="#vipy.dataset.Dataset.clone">clone</a></code></li>
<li><code><a title="vipy.dataset.Dataset.count" href="#vipy.dataset.Dataset.count">count</a></code></li>
<li><code><a title="vipy.dataset.Dataset.even_split" href="#vipy.dataset.Dataset.even_split">even_split</a></code></li>
<li><code><a title="vipy.dataset.Dataset.filter" href="#vipy.dataset.Dataset.filter">filter</a></code></li>
<li><code><a title="vipy.dataset.Dataset.frequency" href="#vipy.dataset.Dataset.frequency">frequency</a></code></li>
<li><code><a title="vipy.dataset.Dataset.from_directory" href="#vipy.dataset.Dataset.from_directory">from_directory</a></code></li>
<li><code><a title="vipy.dataset.Dataset.from_image_urls" href="#vipy.dataset.Dataset.from_image_urls">from_image_urls</a></code></li>
<li><code><a title="vipy.dataset.Dataset.groupby" href="#vipy.dataset.Dataset.groupby">groupby</a></code></li>
<li><code><a title="vipy.dataset.Dataset.id" href="#vipy.dataset.Dataset.id">id</a></code></li>
<li><code><a title="vipy.dataset.Dataset.identity_shuffler" href="#vipy.dataset.Dataset.identity_shuffler">identity_shuffler</a></code></li>
<li><code><a title="vipy.dataset.Dataset.index" href="#vipy.dataset.Dataset.index">index</a></code></li>
<li><code><a title="vipy.dataset.Dataset.inverse_frequency" href="#vipy.dataset.Dataset.inverse_frequency">inverse_frequency</a></code></li>
<li><code><a title="vipy.dataset.Dataset.list" href="#vipy.dataset.Dataset.list">list</a></code></li>
<li><code><a title="vipy.dataset.Dataset.load" href="#vipy.dataset.Dataset.load">load</a></code></li>
<li><code><a title="vipy.dataset.Dataset.localmap" href="#vipy.dataset.Dataset.localmap">localmap</a></code></li>
<li><code><a title="vipy.dataset.Dataset.map" href="#vipy.dataset.Dataset.map">map</a></code></li>
<li><code><a title="vipy.dataset.Dataset.minibatch" href="#vipy.dataset.Dataset.minibatch">minibatch</a></code></li>
<li><code><a title="vipy.dataset.Dataset.partition" href="#vipy.dataset.Dataset.partition">partition</a></code></li>
<li><code><a title="vipy.dataset.Dataset.pipeline" href="#vipy.dataset.Dataset.pipeline">pipeline</a></code></li>
<li><code><a title="vipy.dataset.Dataset.raw" href="#vipy.dataset.Dataset.raw">raw</a></code></li>
<li><code><a title="vipy.dataset.Dataset.repeat" href="#vipy.dataset.Dataset.repeat">repeat</a></code></li>
<li><code><a title="vipy.dataset.Dataset.sample" href="#vipy.dataset.Dataset.sample">sample</a></code></li>
<li><code><a title="vipy.dataset.Dataset.set" href="#vipy.dataset.Dataset.set">set</a></code></li>
<li><code><a title="vipy.dataset.Dataset.shift" href="#vipy.dataset.Dataset.shift">shift</a></code></li>
<li><code><a title="vipy.dataset.Dataset.slice" href="#vipy.dataset.Dataset.slice">slice</a></code></li>
<li><code><a title="vipy.dataset.Dataset.sort" href="#vipy.dataset.Dataset.sort">sort</a></code></li>
<li><code><a title="vipy.dataset.Dataset.split" href="#vipy.dataset.Dataset.split">split</a></code></li>
<li><code><a title="vipy.dataset.Dataset.streaming_map" href="#vipy.dataset.Dataset.streaming_map">streaming_map</a></code></li>
<li><code><a title="vipy.dataset.Dataset.streaming_shuffler" href="#vipy.dataset.Dataset.streaming_shuffler">streaming_shuffler</a></code></li>
<li><code><a title="vipy.dataset.Dataset.take" href="#vipy.dataset.Dataset.take">take</a></code></li>
<li><code><a title="vipy.dataset.Dataset.take_fraction" href="#vipy.dataset.Dataset.take_fraction">take_fraction</a></code></li>
<li><code><a title="vipy.dataset.Dataset.takeby" href="#vipy.dataset.Dataset.takeby">takeby</a></code></li>
<li><code><a title="vipy.dataset.Dataset.takelist" href="#vipy.dataset.Dataset.takelist">takelist</a></code></li>
<li><code><a title="vipy.dataset.Dataset.takeone" href="#vipy.dataset.Dataset.takeone">takeone</a></code></li>
<li><code><a title="vipy.dataset.Dataset.truncate" href="#vipy.dataset.Dataset.truncate">truncate</a></code></li>
<li><code><a title="vipy.dataset.Dataset.tuple" href="#vipy.dataset.Dataset.tuple">tuple</a></code></li>
<li><code><a title="vipy.dataset.Dataset.uniform_shuffler" href="#vipy.dataset.Dataset.uniform_shuffler">uniform_shuffler</a></code></li>
<li><code><a title="vipy.dataset.Dataset.zip" href="#vipy.dataset.Dataset.zip">zip</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="vipy.dataset.Union"><code class="flex name class">
<span>class <span class="ident">Union</span></span>
<span>(</span><span>*args, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>vipy.dataset.Union() class</p>
<p>Common class to manipulate groups of vipy.dataset.Dataset objects in parallel</p>
<h2 id="usage">Usage</h2>
<pre><code class="language-python-repl">&gt;&gt;&gt; cifar10 = vipy.dataset.registry('cifar10')
&gt;&gt;&gt; mnist = vipy.dataset.registry('mnist')
&gt;&gt;&gt; dataset = vipy.dataset.Union(mnist, cifar10)
&gt;&gt;&gt; dataset = mnist | cifar10
</code></pre>
<h2 id="args">Args</h2>
<p>Datasets</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/visym/vipy/blob/095a3bb922809af0de26f3bbf8f0fb8f9c4157a4/vipy/dataset.py#L622-L727" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class Union(Dataset):
    &#34;&#34;&#34;vipy.dataset.Union() class
    
    Common class to manipulate groups of vipy.dataset.Dataset objects in parallel

    Usage:
    
        &gt;&gt;&gt; cifar10 = vipy.dataset.registry(&#39;cifar10&#39;)
        &gt;&gt;&gt; mnist = vipy.dataset.registry(&#39;mnist&#39;)
        &gt;&gt;&gt; dataset = vipy.dataset.Union(mnist, cifar10)
        &gt;&gt;&gt; dataset = mnist | cifar10

    Args:
        Datasets 
    &#34;&#34;&#34;

    __slots__ = (&#39;_id&#39;, &#39;_ds&#39;, &#39;_idx&#39;, &#39;_loader&#39;, &#39;_type&#39;)    
    def __init__(self, *args, **kwargs):
        assert all(isinstance(d, (Dataset, )) for d in args), &#34;invalid datasets&#34;
        
        datasets = [d for d in args]  # order preserving
        assert all([isinstance(d, Dataset) for d in datasets]), &#34;Invalid datasets &#39;%s&#39;&#34; % str([type(d) for d in datasets])

        datasets = [j for i in datasets for j in (i.datasets() if isinstance(i, Union) else (i,))]  # flatten unions        
        self._ds = datasets
        self._idx = None
        self._id = kwargs[&#39;id&#39;] if &#39;id&#39; in kwargs else None

        self._loader = None  # individual datasets have loaders
        self._type = None

    def is_streaming(self):
        return self._idx is None and all(d.is_streaming() for d in self.datasets())

    def __len__(self):
        return sum(d.__len__() for d in self.datasets()) if self._idx is None else len(self._idx)
    
    def __iter__(self):
        if self.is_streaming():
            k = -1
            iter = [d.__iter__() for d in self.datasets()]  # round-robin

            for m in range(len(self.datasets())):
                try:
                    while True:
                        k = (k + 1) % len(iter)                                    
                        yield next(iter[k])  # assumes ordered
                except StopIteration:
                    iter.pop(k)
                    k -= 1
            
        else:
            self.index()  # force random access                    
            for (i,j) in self._idx:
                yield self._ds[i][j]  # random access (slower)                

    def __getitem__(self, k):
        self.index()  # force random access        
        if isinstance(k, (int, np.uint64)):
            assert abs(k) &lt; len(self._idx), &#34;invalid index&#34;
            (i,j) = self._idx[int(k)]            
            return self._ds[i][j]
        elif isinstance(k, slice):
            return [self._ds[i][j] for (i,j) in self._idx[k.start:k.stop:k.step]]
        else:
            raise ValueError(&#39;invalid index type &#34;%s&#34;&#39; % type(k))

    def __repr__(self):
        fields = [&#39;id=%s&#39; % truncate_string(self.id(), maxlen=64)] if self.id() else []
        fields += [&#39;len=%d&#39; % len(self)]
        fields += [&#39;union=%s&#39; % str(tuple([truncate_string(d.id(), maxlen=80) for d in self._ds]))]
        return str(&#39;&lt;vipy.dataset.Dataset: %s&gt;&#39; % (&#39;, &#39;.join(fields)))

    def index(self, index=None, strict=False):
        &#34;&#34;&#34;Update the index, useful for filtering of large datasets&#34;&#34;&#34;
        if index is not None:
            self._idx = index
            return self
        if self._idx is None:
            # Index on-demand: zipped (dataset index, element index) tuples, in round-robin dataset order [(0,0),(1,0),...,(0,n),(1,n),...]            
            lengths = [len(d) for d in self.datasets()]            
            self._idx = [c for r in [[(i,j) for i in range(len(self.datasets()))] for j in range(max(lengths))] for c in r if c[1]&lt;lengths[c[0]]]
        return self._idx
    
    def clone(self, deep=False):
        &#34;&#34;&#34;Return a copy of the dataset object&#34;&#34;&#34;
        D = super().clone(deep=deep)
        D._ds =  [d.clone(deep=deep) for d in D._ds]
        return D
    
    def datasets(self):
        &#34;&#34;&#34;Return the dataset union elements, useful for generating unions of unions&#34;&#34;&#34;
        return list(self._ds)

    def shuffle(self, shuffler=None):
        &#34;&#34;&#34;Permute elements in this dataset uniformly at random in place using the best shuffler for the dataset structure&#34;&#34;&#34;
        shuffler = shuffler if shuffler is not None else (Union.streaming_shuffler if self.is_streaming() else Dataset.uniform_shuffler)
        return shuffler(self)
    
    @staticmethod
    def streaming_shuffler(D):
        &#34;&#34;&#34;A uniform shuffle (approximation) on the dataset elements for iterable access only&#34;&#34;&#34;
        assert D._idx is None, &#34;iterable dataset only&#34;
        D._ds = [Dataset.streaming_shuffler(d) for d in D._ds]  # shuffle dataset shards
        random.shuffle(D._ds)  # shuffle union order
        return D</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="vipy.dataset.Dataset" href="#vipy.dataset.Dataset">Dataset</a></li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="vipy.dataset.Union.datasets"><code class="name flex">
<span>def <span class="ident">datasets</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Return the dataset union elements, useful for generating unions of unions</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/visym/vipy/blob/095a3bb922809af0de26f3bbf8f0fb8f9c4157a4/vipy/dataset.py#L712-L714" class="git-link">Browse git</a>
</summary>
<pre><code class="python">def datasets(self):
    &#34;&#34;&#34;Return the dataset union elements, useful for generating unions of unions&#34;&#34;&#34;
    return list(self._ds)</code></pre>
</details>
</dd>
<dt id="vipy.dataset.Union.is_streaming"><code class="name flex">
<span>def <span class="ident">is_streaming</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/visym/vipy/blob/095a3bb922809af0de26f3bbf8f0fb8f9c4157a4/vipy/dataset.py#L653-L654" class="git-link">Browse git</a>
</summary>
<pre><code class="python">def is_streaming(self):
    return self._idx is None and all(d.is_streaming() for d in self.datasets())</code></pre>
</details>
</dd>
<dt id="vipy.dataset.Union.shuffle"><code class="name flex">
<span>def <span class="ident">shuffle</span></span>(<span>self, shuffler=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Permute elements in this dataset uniformly at random in place using the best shuffler for the dataset structure</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/visym/vipy/blob/095a3bb922809af0de26f3bbf8f0fb8f9c4157a4/vipy/dataset.py#L716-L719" class="git-link">Browse git</a>
</summary>
<pre><code class="python">def shuffle(self, shuffler=None):
    &#34;&#34;&#34;Permute elements in this dataset uniformly at random in place using the best shuffler for the dataset structure&#34;&#34;&#34;
    shuffler = shuffler if shuffler is not None else (Union.streaming_shuffler if self.is_streaming() else Dataset.uniform_shuffler)
    return shuffler(self)</code></pre>
</details>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="vipy.dataset.Dataset" href="#vipy.dataset.Dataset">Dataset</a></b></code>:
<ul class="hlist">
<li><code><a title="vipy.dataset.Dataset.balanced" href="#vipy.dataset.Dataset.balanced">balanced</a></code></li>
<li><code><a title="vipy.dataset.Dataset.batch" href="#vipy.dataset.Dataset.batch">batch</a></code></li>
<li><code><a title="vipy.dataset.Dataset.chunk" href="#vipy.dataset.Dataset.chunk">chunk</a></code></li>
<li><code><a title="vipy.dataset.Dataset.chunks" href="#vipy.dataset.Dataset.chunks">chunks</a></code></li>
<li><code><a title="vipy.dataset.Dataset.clone" href="#vipy.dataset.Dataset.clone">clone</a></code></li>
<li><code><a title="vipy.dataset.Dataset.count" href="#vipy.dataset.Dataset.count">count</a></code></li>
<li><code><a title="vipy.dataset.Dataset.even_split" href="#vipy.dataset.Dataset.even_split">even_split</a></code></li>
<li><code><a title="vipy.dataset.Dataset.filter" href="#vipy.dataset.Dataset.filter">filter</a></code></li>
<li><code><a title="vipy.dataset.Dataset.frequency" href="#vipy.dataset.Dataset.frequency">frequency</a></code></li>
<li><code><a title="vipy.dataset.Dataset.from_directory" href="#vipy.dataset.Dataset.from_directory">from_directory</a></code></li>
<li><code><a title="vipy.dataset.Dataset.from_image_urls" href="#vipy.dataset.Dataset.from_image_urls">from_image_urls</a></code></li>
<li><code><a title="vipy.dataset.Dataset.groupby" href="#vipy.dataset.Dataset.groupby">groupby</a></code></li>
<li><code><a title="vipy.dataset.Dataset.id" href="#vipy.dataset.Dataset.id">id</a></code></li>
<li><code><a title="vipy.dataset.Dataset.identity_shuffler" href="#vipy.dataset.Dataset.identity_shuffler">identity_shuffler</a></code></li>
<li><code><a title="vipy.dataset.Dataset.index" href="#vipy.dataset.Dataset.index">index</a></code></li>
<li><code><a title="vipy.dataset.Dataset.inverse_frequency" href="#vipy.dataset.Dataset.inverse_frequency">inverse_frequency</a></code></li>
<li><code><a title="vipy.dataset.Dataset.list" href="#vipy.dataset.Dataset.list">list</a></code></li>
<li><code><a title="vipy.dataset.Dataset.load" href="#vipy.dataset.Dataset.load">load</a></code></li>
<li><code><a title="vipy.dataset.Dataset.localmap" href="#vipy.dataset.Dataset.localmap">localmap</a></code></li>
<li><code><a title="vipy.dataset.Dataset.map" href="#vipy.dataset.Dataset.map">map</a></code></li>
<li><code><a title="vipy.dataset.Dataset.minibatch" href="#vipy.dataset.Dataset.minibatch">minibatch</a></code></li>
<li><code><a title="vipy.dataset.Dataset.partition" href="#vipy.dataset.Dataset.partition">partition</a></code></li>
<li><code><a title="vipy.dataset.Dataset.pipeline" href="#vipy.dataset.Dataset.pipeline">pipeline</a></code></li>
<li><code><a title="vipy.dataset.Dataset.raw" href="#vipy.dataset.Dataset.raw">raw</a></code></li>
<li><code><a title="vipy.dataset.Dataset.repeat" href="#vipy.dataset.Dataset.repeat">repeat</a></code></li>
<li><code><a title="vipy.dataset.Dataset.sample" href="#vipy.dataset.Dataset.sample">sample</a></code></li>
<li><code><a title="vipy.dataset.Dataset.set" href="#vipy.dataset.Dataset.set">set</a></code></li>
<li><code><a title="vipy.dataset.Dataset.shift" href="#vipy.dataset.Dataset.shift">shift</a></code></li>
<li><code><a title="vipy.dataset.Dataset.slice" href="#vipy.dataset.Dataset.slice">slice</a></code></li>
<li><code><a title="vipy.dataset.Dataset.sort" href="#vipy.dataset.Dataset.sort">sort</a></code></li>
<li><code><a title="vipy.dataset.Dataset.split" href="#vipy.dataset.Dataset.split">split</a></code></li>
<li><code><a title="vipy.dataset.Dataset.streaming_map" href="#vipy.dataset.Dataset.streaming_map">streaming_map</a></code></li>
<li><code><a title="vipy.dataset.Dataset.streaming_shuffler" href="#vipy.dataset.Dataset.streaming_shuffler">streaming_shuffler</a></code></li>
<li><code><a title="vipy.dataset.Dataset.take" href="#vipy.dataset.Dataset.take">take</a></code></li>
<li><code><a title="vipy.dataset.Dataset.take_fraction" href="#vipy.dataset.Dataset.take_fraction">take_fraction</a></code></li>
<li><code><a title="vipy.dataset.Dataset.takeby" href="#vipy.dataset.Dataset.takeby">takeby</a></code></li>
<li><code><a title="vipy.dataset.Dataset.takelist" href="#vipy.dataset.Dataset.takelist">takelist</a></code></li>
<li><code><a title="vipy.dataset.Dataset.takeone" href="#vipy.dataset.Dataset.takeone">takeone</a></code></li>
<li><code><a title="vipy.dataset.Dataset.truncate" href="#vipy.dataset.Dataset.truncate">truncate</a></code></li>
<li><code><a title="vipy.dataset.Dataset.tuple" href="#vipy.dataset.Dataset.tuple">tuple</a></code></li>
<li><code><a title="vipy.dataset.Dataset.uniform_shuffler" href="#vipy.dataset.Dataset.uniform_shuffler">uniform_shuffler</a></code></li>
<li><code><a title="vipy.dataset.Dataset.zip" href="#vipy.dataset.Dataset.zip">zip</a></code></li>
</ul>
</li>
</ul>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<header>
<a class="homelink" rel="home" title="VIPY" href="https://github.com/visym/vipy/">
<img src="https://www.visym.com/labs/images/visym_logo_black_notext.png" alt="" width="60">
</a>
<h1 style="font-size:200%;"><b>VIPY:</b> Visual Dataset Transformation</h1>
</header>
<form>
<input id="lunr-search" name="q" placeholder=" Search ..." aria-label="Search"
disabled minlength="2">
</form>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/tingle/0.15.3/tingle.min.css" integrity="sha512-j1u8eUJ4f23xPPxwOrLUPQaCD2dwzNqqmDDcWS4deWsMv2ohLqmXXuP3hU7g8TyzbMSakP/mMqoNBYWj8AEIFg==" crossorigin>
<script src="https://cdnjs.cloudflare.com/ajax/libs/tingle/0.15.3/tingle.min.js" integrity="sha512-plGUER9JkeEWPPqQBE4sdLqBoQug5Ap+BCGMc7bJ8BXkm+VVj6QzkpBz5Yv2yPkkq+cqg9IpkBaGCas6uDbW8g==" crossorigin></script>
<style>
.modal-dialog iframe {
width: 100vw;
height: calc(100vh - 80px);
}
@media screen and (min-width: 700px) {
.modal-dialog iframe {
width: 70vw;
height: 80vh;
}
}
.modal-dialog .tingle-modal-box {width: auto;}
.modal-dialog .tingle-modal-box__content {padding: 0;}
</style>
<script>
const input = document.getElementById('lunr-search');
input.disabled = false;
input.form.addEventListener('submit', (ev) => {
ev.preventDefault();
const url = new URL(window.location);
url.searchParams.set('q', input.value);
history.replaceState({}, null, url.toString());
search(input.value);
});
const query = new URL(window.location).searchParams.get('q');
if (query)
search(query);
function search(query) {
const url = './doc-search.html#' + encodeURIComponent(query);
new tingle.modal({
cssClass: ['modal-dialog'],
onClose: () => {
const url = new URL(window.location);
url.searchParams.delete('q');
history.replaceState({}, null, url.toString());
setTimeout(() => input.focus(), 100);
}
}).setContent('<iframe src="' + url + '"></iframe>').open();
}
</script>
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="vipy" href="index.html">vipy</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="vipy.dataset.registry" href="#vipy.dataset.registry">registry</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="vipy.dataset.Dataset" href="#vipy.dataset.Dataset">Dataset</a></code></h4>
<ul class="two-column">
<li><code><a title="vipy.dataset.Dataset.all" href="#vipy.dataset.Dataset.all">all</a></code></li>
<li><code><a title="vipy.dataset.Dataset.balanced" href="#vipy.dataset.Dataset.balanced">balanced</a></code></li>
<li><code><a title="vipy.dataset.Dataset.batch" href="#vipy.dataset.Dataset.batch">batch</a></code></li>
<li><code><a title="vipy.dataset.Dataset.cast" href="#vipy.dataset.Dataset.cast">cast</a></code></li>
<li><code><a title="vipy.dataset.Dataset.chunk" href="#vipy.dataset.Dataset.chunk">chunk</a></code></li>
<li><code><a title="vipy.dataset.Dataset.chunks" href="#vipy.dataset.Dataset.chunks">chunks</a></code></li>
<li><code><a title="vipy.dataset.Dataset.clone" href="#vipy.dataset.Dataset.clone">clone</a></code></li>
<li><code><a title="vipy.dataset.Dataset.count" href="#vipy.dataset.Dataset.count">count</a></code></li>
<li><code><a title="vipy.dataset.Dataset.countby" href="#vipy.dataset.Dataset.countby">countby</a></code></li>
<li><code><a title="vipy.dataset.Dataset.even_split" href="#vipy.dataset.Dataset.even_split">even_split</a></code></li>
<li><code><a title="vipy.dataset.Dataset.filter" href="#vipy.dataset.Dataset.filter">filter</a></code></li>
<li><code><a title="vipy.dataset.Dataset.frequency" href="#vipy.dataset.Dataset.frequency">frequency</a></code></li>
<li><code><a title="vipy.dataset.Dataset.from_directory" href="#vipy.dataset.Dataset.from_directory">from_directory</a></code></li>
<li><code><a title="vipy.dataset.Dataset.from_image_urls" href="#vipy.dataset.Dataset.from_image_urls">from_image_urls</a></code></li>
<li><code><a title="vipy.dataset.Dataset.from_json" href="#vipy.dataset.Dataset.from_json">from_json</a></code></li>
<li><code><a title="vipy.dataset.Dataset.groupby" href="#vipy.dataset.Dataset.groupby">groupby</a></code></li>
<li><code><a title="vipy.dataset.Dataset.id" href="#vipy.dataset.Dataset.id">id</a></code></li>
<li><code><a title="vipy.dataset.Dataset.identity_shuffler" href="#vipy.dataset.Dataset.identity_shuffler">identity_shuffler</a></code></li>
<li><code><a title="vipy.dataset.Dataset.index" href="#vipy.dataset.Dataset.index">index</a></code></li>
<li><code><a title="vipy.dataset.Dataset.inverse_frequency" href="#vipy.dataset.Dataset.inverse_frequency">inverse_frequency</a></code></li>
<li><code><a title="vipy.dataset.Dataset.is_streaming" href="#vipy.dataset.Dataset.is_streaming">is_streaming</a></code></li>
<li><code><a title="vipy.dataset.Dataset.len" href="#vipy.dataset.Dataset.len">len</a></code></li>
<li><code><a title="vipy.dataset.Dataset.list" href="#vipy.dataset.Dataset.list">list</a></code></li>
<li><code><a title="vipy.dataset.Dataset.load" href="#vipy.dataset.Dataset.load">load</a></code></li>
<li><code><a title="vipy.dataset.Dataset.localmap" href="#vipy.dataset.Dataset.localmap">localmap</a></code></li>
<li><code><a title="vipy.dataset.Dataset.map" href="#vipy.dataset.Dataset.map">map</a></code></li>
<li><code><a title="vipy.dataset.Dataset.minibatch" href="#vipy.dataset.Dataset.minibatch">minibatch</a></code></li>
<li><code><a title="vipy.dataset.Dataset.partition" href="#vipy.dataset.Dataset.partition">partition</a></code></li>
<li><code><a title="vipy.dataset.Dataset.pipeline" href="#vipy.dataset.Dataset.pipeline">pipeline</a></code></li>
<li><code><a title="vipy.dataset.Dataset.raw" href="#vipy.dataset.Dataset.raw">raw</a></code></li>
<li><code><a title="vipy.dataset.Dataset.repeat" href="#vipy.dataset.Dataset.repeat">repeat</a></code></li>
<li><code><a title="vipy.dataset.Dataset.sample" href="#vipy.dataset.Dataset.sample">sample</a></code></li>
<li><code><a title="vipy.dataset.Dataset.set" href="#vipy.dataset.Dataset.set">set</a></code></li>
<li><code><a title="vipy.dataset.Dataset.shift" href="#vipy.dataset.Dataset.shift">shift</a></code></li>
<li><code><a title="vipy.dataset.Dataset.shuffle" href="#vipy.dataset.Dataset.shuffle">shuffle</a></code></li>
<li><code><a title="vipy.dataset.Dataset.slice" href="#vipy.dataset.Dataset.slice">slice</a></code></li>
<li><code><a title="vipy.dataset.Dataset.sort" href="#vipy.dataset.Dataset.sort">sort</a></code></li>
<li><code><a title="vipy.dataset.Dataset.split" href="#vipy.dataset.Dataset.split">split</a></code></li>
<li><code><a title="vipy.dataset.Dataset.streaming_map" href="#vipy.dataset.Dataset.streaming_map">streaming_map</a></code></li>
<li><code><a title="vipy.dataset.Dataset.streaming_shuffler" href="#vipy.dataset.Dataset.streaming_shuffler">streaming_shuffler</a></code></li>
<li><code><a title="vipy.dataset.Dataset.take" href="#vipy.dataset.Dataset.take">take</a></code></li>
<li><code><a title="vipy.dataset.Dataset.take_fraction" href="#vipy.dataset.Dataset.take_fraction">take_fraction</a></code></li>
<li><code><a title="vipy.dataset.Dataset.takeby" href="#vipy.dataset.Dataset.takeby">takeby</a></code></li>
<li><code><a title="vipy.dataset.Dataset.takelist" href="#vipy.dataset.Dataset.takelist">takelist</a></code></li>
<li><code><a title="vipy.dataset.Dataset.takeone" href="#vipy.dataset.Dataset.takeone">takeone</a></code></li>
<li><code><a title="vipy.dataset.Dataset.truncate" href="#vipy.dataset.Dataset.truncate">truncate</a></code></li>
<li><code><a title="vipy.dataset.Dataset.tuple" href="#vipy.dataset.Dataset.tuple">tuple</a></code></li>
<li><code><a title="vipy.dataset.Dataset.uniform_shuffler" href="#vipy.dataset.Dataset.uniform_shuffler">uniform_shuffler</a></code></li>
<li><code><a title="vipy.dataset.Dataset.zip" href="#vipy.dataset.Dataset.zip">zip</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="vipy.dataset.Paged" href="#vipy.dataset.Paged">Paged</a></code></h4>
<ul class="">
<li><code><a title="vipy.dataset.Paged.chunk_shuffler" href="#vipy.dataset.Paged.chunk_shuffler">chunk_shuffler</a></code></li>
<li><code><a title="vipy.dataset.Paged.flush" href="#vipy.dataset.Paged.flush">flush</a></code></li>
<li><code><a title="vipy.dataset.Paged.shuffle" href="#vipy.dataset.Paged.shuffle">shuffle</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="vipy.dataset.Union" href="#vipy.dataset.Union">Union</a></code></h4>
<ul class="">
<li><code><a title="vipy.dataset.Union.datasets" href="#vipy.dataset.Union.datasets">datasets</a></code></li>
<li><code><a title="vipy.dataset.Union.is_streaming" href="#vipy.dataset.Union.is_streaming">is_streaming</a></code></li>
<li><code><a title="vipy.dataset.Union.shuffle" href="#vipy.dataset.Union.shuffle">shuffle</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.11.6</a>.</p>
</footer>
</body>
</html>
