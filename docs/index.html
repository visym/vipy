<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.11.6" />
<meta name="google-site-verification" content="aB8LkQegj94_TJPdrcJm2ldIRWyXY82Jp24Gtkdgyn0" />
<title>vipy documentation</title>
<meta name="description" content="VIPY is a python package for representation, transformation and visualization of annotated videos and images.
Annotations are the ground truth â€¦" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}#lunr-search{width:100%;font-size:1em;padding:6px 9px 5px 9px;border:1px solid silver}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">VIPY</h1>
</header>
<section id="section-intro">
<p>VIPY is a python package for representation, transformation and visualization of annotated videos and images.
Annotations are the ground truth provided by labelers (e.g. object bounding boxes, face identities, temporal activity clips), suitable for training computer vision systems.
VIPY provides tools to easily edit videos and images so that the annotations are transformed along with the pixels.
This enables a clean interface for transforming complex datasets for input to your computer vision training and testing pipeline.</p>
<p>VIPY provides:</p>
<ul>
<li>Representation of videos with labeled activities that can be resized, clipped, rotated, scaled and cropped</li>
<li>Representation of images with object bounding boxes that can be manipulated as easily as editing an image</li>
<li>Clean visualization of annotated images and videos</li>
<li>Lazy loading of images and videos suitable for distributed procesing (e.g. dask, spark)</li>
<li>Straightforward integration into machine learning toolchains (e.g. torch, numpy)</li>
<li>Fluent interface for chaining operations on videos and images</li>
<li>Visual dataset download, unpack and import (e.g. Imagenet21k, Coco 2014, Visual Genome, Open Images V7, Kinetics700, YoutubeBB, ActivityNet, &hellip; )</li>
<li>Video and image web search tools with URL downloading and caching</li>
<li>Minimum dependencies for easy installation (e.g. AWS Lambda)</li>
</ul>
<h1 id="design-goals">Design Goals</h1>
<p>Vipy was created with three design goals.
</p>
<ul>
<li><strong>Simplicity</strong>.
Annotated Videos and images should be as easy to manipulate as the pixels.
We provide a simple fluent API that enables the transformation of media so that pixels are transformed along with the annotations.
We provide a comprehensive unit test suite to validate this pipeline with continuous integration.</li>
<li><strong>Portability</strong>.
Vipy was designed with the goal of allowing it to be easily retargeted to new platforms.
For example, deployment on a serverless architecture such as AWS lambda has restrictions on the allowable code that can be executed in layers.
We designed Vipy with minimal dependencies on standard and mature machine learning tool chains (numpy, matplotlib, ffmpeg, pillow) to ensure that it can be ported to new computational environments. </li>
<li><strong>Efficiency</strong>.
Vipy is written in pure python with the goal of minimizing memory requirementts for large datasets, and performing in place operations to avoid copies.
This enables fast video processing by operating on videos as chains of transformations.
The documentation describes when an object is changed in place vs. copied.
Furthermore, loading is delayed until explicitly requested by the user (or the pixels are needed) to enable lazy loading for distributed processing.
</li>
</ul>
<h1 id="getting-started">Getting started</h1>
<p>The VIPY tools are designed for simple and intuitive interaction with videos and images.
Try to create a <code><a title="vipy.video.Scene" href="video.html#vipy.video.Scene">Scene</a></code> object:</p>
<pre><code class="language-python">v = vipy.video.RandomScene()
</code></pre>
<p>Videos are constructed from URLs (e.g. RTSP/RTMP live camera streams, YouTube videos, public or keyed AWS S3 links), SSH accessible paths, local filenames, <code><a title="vipy.image.Image" href="image.html#vipy.image.Image">Image</a></code> frame lists, numpy arrays or pytorch tensors.
In this example, we create a random video with tracks and activities.
Videos can be natively iterated:</p>
<pre><code class="language-python">for im in v:
    print(im.numpy())
</code></pre>
<p>This will iterate and yield <code><a title="vipy.image.Image" href="image.html#vipy.image.Image">Image</a></code> objects corresponding to each frame of the video.
You can use the <code><a title="vipy.image.Image.numpy" href="image.html#vipy.image.Image.numpy">Image.numpy()</a></code> method to extract the numpy array for this frame.
Long videos are streamed to avoid out of memory errors.
Under the hood, we represent each video as a filter chain to an FFMPEG pipe, which yields frames corresponding to the appropriate filter transform and framerate.
The yielded frames include all of the objects that are present in the video at that frame accessible with the <code><a title="vipy.image.Scene.objects" href="image.html#vipy.image.Scene.objects">Scene.objects()</a></code> method.</p>
<p>VIPY supports more complex iterators.
For example, a common use case for activity detection is iterating over short clips in a video.
You can do this using the stream iterator:</p>
<pre><code class="language-python">for c in v.stream().clip(16):
    print(c.torch())
</code></pre>
<p>This will yield <code><a title="vipy.video.Scene" href="video.html#vipy.video.Scene">Scene</a></code> objects each containing a <code><a title="vipy.video.Stream.clip" href="video.html#vipy.video.Stream.clip">Stream.clip()</a></code> of length 16 frames.
Each clip overlaps by 15 frames with the next clip, and each clip includes a threaded copy of the pixels.
This is useful to provide clips of a fixed length that are output for every frame of the video.
Each clip contais the tracks and activities within this clip time period.
The method <code><a title="vipy.video.Video.torch" href="video.html#vipy.video.Video.torch">Video.torch()</a></code> will output a torch tensor suitable for integration into a pytorch based system.</p>
<p>These python iterators can be combined together in complex ways</p>
<pre><code class="language-python">for (im, c, imdelay) in (v, v.stream().clip(16), v.stream().frame(delay=10), a_gpu_function(v.stream().batch(16)))
    print(im, c.torch(), imdelay)
</code></pre>
<p>This will yield the current frame, a video <code><a title="vipy.video.Stream.clip" href="video.html#vipy.video.Stream.clip">Stream.clip()</a></code> of length 16, a <code><a title="vipy.video.Stream.frame" href="video.html#vipy.video.Stream.frame">Stream.frame()</a></code> 10 frames ago and a <code><a title="vipy.video.Stream.batch" href="video.html#vipy.video.Stream.batch">Stream.batch()</a></code> of 16 frames that is designed for computation and transformation on a GPU.
All of the pixels are copied in threaded processing which is efficiently hidden by GPU I/O bound operations.
For more examples of complex iterators in real world use cases, see the <a href="https://github.com/visym/heyvi">HeyVi package</a> for open source visual analytics.</p>
<p>Videos can be transformed in complex ways, and the pixels will always be transformed along with the annotations.</p>
<pre><code class="language-python">v.fliplr()          # flip horizontally
v.zeropad(10, 20)   # zero pad the video horizontally and vertically
v.mindim(256)       # change the minimum dimension of the video
v.framerate(10)     # change the framerate of the video 
</code></pre>
<p>The transformation is lazy and is incorporated into the FFMPEG complex filter chain so that the transformation is applied when the pixels are needed.
You can always access the current filter chain using <code><a title="vipy.video.Video.commandline" href="video.html#vipy.video.Video.commandline">Video.commandline()</a></code> which will output a commandline string for the ffmpeg executable that you can use to get a deeper underestanding of the transformations that are applied to the video pixels.</p>
<p>Finally, annotated videos can be displayed. </p>
<pre><code class="language-python">v.show()
v.show(notebook=True)
v.frame().show()
v.annotate('/path/to/visualization.mp4')
with vipy.video.Video(url='rtmps://youtu.be/...').mindim(512).framerate(5).stream(write=True) as s:
    for im in v.framerate(5):
        s.write(im.annotate().rgb())
</code></pre>
<p>This will <code><a title="vipy.video.Scene.show" href="video.html#vipy.video.Scene.show">Scene.show()</a></code> the video live on your desktop, in a jupyter notebook, show the first <code><a title="vipy.video.Scene.frame" href="video.html#vipy.video.Scene.frame">Scene.frame()</a></code> as a static image, <code><a title="vipy.video.Scene.annotate" href="video.html#vipy.video.Scene.annotate">Scene.annotate()</a></code> the video so that annotations are in the pixels and save the corresponding video, or live stream a 5Hz video to youtube.
All of the show methods can be configured to customize the colors or captions.</p>
<p>See the <a href="https://github.com/visym/vipy/tree/master/demo">demos</a> for more examples.</p>
<h2 id="import">Import</h2>
<p>Vipy was designed to define annotated videos and imagery as collections of python objects.
The core objects for images are:</p>
<ul>
<li><a href="image.html#vipy.image.Scene">vipy.image.Scene</a></li>
<li><a href="object.html#vipy.object.Detection">vipy.object.Detection</a></li>
<li><a href="object.html#vipy.object.Keypoint2d">vipy.object.Keypoint2d</a></li>
<li><a href="geometry.html#vipy.geometry.BoundingBox">vipy.geometry.BoundingBox</a></li>
<li><a href="geometry.html#vipy.geometry.Point2d">vipy.geometry.Point2d</a></li>
</ul>
<p>The core objects for videos:</p>
<ul>
<li><a href="video.html#vipy.video.Scene">vipy.video.Scene</a></li>
<li><a href="object.html#vipy.object.Track">vipy.object.Track</a></li>
<li><a href="activity.html#vipy.activity.Activity">vipy.activity.Activity</a></li>
</ul>
<p>See the documentation for each object for how to construct them.
</p>
<h2 id="export">Export</h2>
<p>All vipy objects can be imported and exported to JSON for interoperatability with other tool chains.
This allows for introspection of the vipy object state in an open format providing transparency</p>
<pre><code class="language-python">vipy.image.owl().json()
</code></pre>
<h2 id="environment-variables">Environment variables</h2>
<p>You can set the following environment variables to customize the output of vipy</p>
<ul>
<li><strong>VIPY_CACHE</strong>='/path/to/directory'.
This directory will contain all of the cached downloaded filenames when downloading URLs.
For example, the following will download all media to '~/.vipy'.</li>
</ul>
<pre><code class="language-python">os.environ['VIPY_CACHE'] = vipy.util.remkdir('~/.vipy')
vipy.image.Image(url='https://upload.wikimedia.org/wikipedia/commons/thumb/2/23/Bubo_virginianus_06.jpg/1920px-Bubo_virginianus_06.jpg').download()
</code></pre>
<p>This will output an image object:</p>
<pre><code class="language-python">&lt;vipy.image: filename=&quot;~/.vipy/1920px-Bubo_virginianus_06.jpg&quot;, filename=&quot;~/.vipy/1920px-Bubo_virginianus_06.jpg&quot;, url=&quot;https://upload.wikimedia.org/wikipedia/commons/thumb/2/23/Bubo_virginianus_06.jpg/1920px-Bubo_virginianus_06.jpg&quot;&gt;
</code></pre>
<p>This provides control over where large datasets are cached on your local file system.
By default, this will be cached to the system temp directory.</p>
<ul>
<li><strong>VIPY_DATASET_REGISTRY_HOME</strong>='/path/to/dir'.
This is the directory to download datasets in <code><a title="vipy.dataset.registry" href="dataset.html#vipy.dataset.registry">registry()</a></code>.</li>
<li><strong>VIPY_AWS_ACCESS_KEY_ID</strong>='MYKEY'.
This is the <a href="https://docs.aws.amazon.com/general/latest/gr/aws-sec-cred-types.html">AWS key</a> to download urls of the form "s3://".
</li>
<li><strong>VIPY_AWS_SECRET_ACCESS_KEY</strong>='MYKEY'.
This is the <a href="https://docs.aws.amazon.com/general/latest/gr/aws-sec-cred-types.html">AWS secret key</a> to download urls of the form "s3://".</li>
<li><strong>VIPY_BACKEND</strong>.
This is the <a href="https://matplotlib.org/stable/users/explain/backends.html">Matplotlib backend</a> to use when rendering figure windows.
'Agg' is recommended for headless operation, and 'TkAgg' is recommended for Linux based X11 forwarding.
In most cases, matplotlib will choose the best backend available by default, and this environment variable does not need to override this choice.</li>
</ul>
<h2 id="versioning">Versioning</h2>
<p>To determine what vipy version you are running you can use:</p>
<pre><code class="language-python-repl">&gt;&gt;&gt; vipy.__version__
&gt;&gt;&gt; vipy.__version__.is_at_least('1.16.2') 
</code></pre>
<h2 id="parallelization">Parallelization</h2>
<p>Vipy includes integration with <a href="https://docs.python.org/3/library/concurrent.futures.html">concurrent futures</a> and <a href="https://distributed.dask.org">Dask Distributed</a> for parallel processing of video and images.
This is useful for preprocessing of datasets to prepare them for training.
</p>
<p>For example, we can construct a <code><a title="vipy.dataset.Dataset" href="dataset.html#vipy.dataset.Dataset">Dataset</a></code> object from one or more videos.
This dataset can be transformed in parallel using two processes:</p>
<pre><code class="language-python">D = vipy.dataset.Dataset(vipy.video.Scene(filename='/path/to/videofile.mp4'))
with vipy.globals.parallel(2):
    R = D.map(lambda v, outdir='/newpath/to/': v.mindim(128).framerate(5).saveas(so.path.join(outdir, vipy.util.filetail(v.filename()))))
</code></pre>
<p>The result is a transformed dataset which contains transformed videos downsampled to have minimum dimension 128, framerate of 5Hz, with the annotations transformed accordingly.
The <code><a title="vipy.dataset.Dataset.map" href="dataset.html#vipy.dataset.Dataset.map">Dataset.map()</a></code> method allows for a lambda function to be applied in parallel to all elements in a dataset.
The fluent design of the VIPY objects allows for easy chaining of video operations to be expressed as a lambda function.
VIPY objects are designed for integration into parallel processing tool chains and can be easily serialized and deserialized for sending to parallel worker tasks.
</p>
<p>VIPY supports integration with distributed schedulers for massively parallel operation.
</p>
<pre><code class="language-python">D = vipy.dataset.Dataset('/path/to/directory/of/jsonfiles')
with vipy.globals.dask(scheduler='10.0.0.1:8785'):
    R = D.map(lambda v, outdir='/newpath/to': vipy.util.bz2pkl(os.path.join(outdir, '%s.pkl.bz2' % v.videoid()), v.trackcrop().mindim(128).normalize(mean=(128,128,128)).torch()))
</code></pre>
<p>This will lazy load a directory of JSON files, where each JSON file corresponds to the annotations of a single video, such as those collected by <a href="https://visym.github.io/collector">Visym Collector</a>.
The <code><a title="vipy.dataset.Dataset.map" href="dataset.html#vipy.dataset.Dataset.map">Dataset.map()</a></code> method will communicate with a <a href="https://docs.dask.org/en/stable/how-to/deploy-dask/ssh.html">scheduler</a> at a given IP address and port and will process the lambda function in parallel to the workers tasked by the scheduler.
In this example, the video will <code><a title="vipy.video.Scene.trackcrop" href="video.html#vipy.video.Scene.trackcrop">Scene.trackcrop()</a></code> the smallest bounding box containing all tracks in the video, resized so this crop is 128 on the smallest side, loaded and normalized to remove the mean, then saved as a torch tensor in a bzipped python pickle file.
This is useful for preprocesssing videos to torch tensors for fast loading of dataset augmentation during training.</p>
<h1 id="tutorials">Tutorials</h1>
<p>The following tutorials show fluent python chains to achieve transformations of annotated images and videos.</p>
<h2 id="images">Images</h2>
<h3 id="load-an-image">Load an image</h3>
<p>Images can be loaded from URLs, local image files, or numpy arrays.
The images exhibit lazy loading, so that pixels will not be fetched until they are needed.</p>
<pre><code class="language-python">&gt;&gt;&gt; im = vipy.image.Image(filename='/path/to/in.jpg')  
&gt;&gt;&gt; im = vipy.image.Image(url='https://url/to/in.jpg')  
&gt;&gt;&gt; im = vipy.image.Image(array=np.random.rand(224,224,3).astype(np.float32))  
</code></pre>
<h3 id="print-an-image-representation">Print an image representation</h3>
<p>All objects have helpful string representations when printed to stdout.
This is accessible via the <code><a title="vipy.image.Image.print" href="image.html#vipy.image.Image.print">Image.print()</a></code> method or by using builtin print().
In this example, an image is created from a wikipedia URL.
Printing this image object shows the URL, but when it is loaded, the image object shows the size of the image, colorspace and the filename that the URL was downloaded to.
When in doubt, print!</p>
<pre><code class="language-python">&gt;&gt;&gt; print(vipy.image.Scene(url='https://upload.wikimedia.org/wikipedia/commons/thumb/2/23/Bubo_virginianus_06.jpg/1920px-Bubo_virginianus_06.jpg'))
&lt;vipy.image.scene: url=https://upload.wikimedia.org/wikipedia/commons/thumb/2/23/Bubo_virginianus_06.jpg/1920px-Bubo_virginianus_06.jpg&gt;

&gt;&gt;&gt; vipy.image.Scene(url='https://upload.wikimedia.org/wikipedia/commons/thumb/2/23/Bubo_virginianus_06.jpg/1920px-Bubo_virginianus_06.jpg').load().print()
&lt;vipy.image.scene: height=2400, width=1920, color=rgb, filename=&quot;/tmp/1920px-Bubo_virginianus_06.jpg&quot;, url=https://upload.wikimedia.org/wikipedia/commons/thumb/2/23/Bubo_virginianus_06.jpg/1920px-Bubo_virginianus_06.jpg&gt;
</code></pre>
<h3 id="transform-an-image">Transform an image</h3>
<p>Images can be transformed so that the annotations are updated along with the pixels.
In this example, the <code><a title="vipy.image.owl" href="image.html#vipy.image.owl">owl()</a></code> is a demo image to a wikipedia URL with a bounding box.
This can be resized and cropped or anisotropically scaled and the box is updated to match the pixels. </p>
<pre><code class="language-python">&gt;&gt;&gt; im = vipy.image.owl().mindim(512).fliplr().centersquare().show()
&gt;&gt;&gt; im = vipy.image.owl().resize(width=512, height=256).show()
</code></pre>
<p><img src="https://raw.githubusercontent.com/visym/vipy/master/docs/tutorials/transform_an_image_1.jpg" height="250">
<img src="https://raw.githubusercontent.com/visym/vipy/master/docs/tutorials/transform_an_image_2.jpg" height="250"></p>
<h3 id="export-as-numpy-array">Export as numpy array</h3>
<p>All images are represented internally as a private attribute <code>vipy.image.Image._array</code> which is a numpy array representation of the pixels.
Image transformations can be chained to operate sequentially on this pixel buffer.
In this example, the <code><a title="vipy.image.owl" href="image.html#vipy.image.owl">owl()</a></code> test image is cropped to retain the center square, converted from uint8 RGB to float32 greyscale, resized to 224x224 then exported to numpy array.
</p>
<pre><code class="language-python">&gt;&gt;&gt; vipy.image.owl().centersquare().greyscale().mindim(224).numpy()
array([[0.11470564, 0.11794835, 0.13006495, ..., 0.15657625, 0.15867704,
        0.16140679],
       [0.11835834, 0.11993656, 0.12860955, ..., 0.15611856, 0.15460114,
        0.15652661],
       [0.12262769, 0.1245698 , 0.12809968, ..., 0.153694  , 0.15326852,
        0.15336327],
       ...,
       [0.42591274, 0.42745316, 0.4352066 , ..., 0.12994824, 0.13172676,
        0.13424061],
       [0.42972928, 0.43847743, 0.45459685, ..., 0.12558977, 0.12820148,
        0.13141613],
       [0.44050908, 0.45350933, 0.46908155, ..., 0.12246227, 0.1256479 ,
        0.12941177]], dtype=float32)
</code></pre>
<h3 id="display-an-image">Display an image</h3>
<p>All images can be displayed using the matplotlib library.
Matplotlib is the most universally ported GUI library for python, and exhibits minimal dependencies.
We enable the user to show images using figure window or "matlab style" of image display.
This will show pixels with overlayed semi-transparent bounding boxes for objects with captions.</p>
<pre><code class="language-python">&gt;&gt;&gt; im = vipy.image.owl().mindim(512).show()
</code></pre>
<p><img src="https://raw.githubusercontent.com/visym/vipy/master/docs/tutorials/display_an_image.jpg" height="500"></p>
<p>All images can be displayed in a dark theme or a light theme.
Light themes show captions on light backgrounds, dark theme shows captions on a dark background.</p>
<pre><code class="language-python">&gt;&gt;&gt; im = vipy.image.owl().mindim(512).show(theme='dark')
</code></pre>
<h3 id="annotate-an-image">Annotate an image</h3>
<p>By default, images and annotations are represented independently.
However, it is sometimes useful to export the annotations into the pixels.
The <code><a title="vipy.image.Scene.annotate" href="image.html#vipy.image.Scene.annotate">Scene.annotate()</a></code> method will export the same visualization as when the image is displayed, but the pixel buffer will be overwritten with the shown image.
This means that calling <code><a title="vipy.image.Image.numpy" href="image.html#vipy.image.Image.numpy">Image.numpy()</a></code> will return the pixel buffer with boxes and captions in the pixels.</p>
<pre><code class="language-python">&gt;&gt;&gt; vipy.image.owl().mindim(512).maxmatte().annotate().rgb().saveas('out.jpg')
</code></pre>
<p><img src="https://raw.githubusercontent.com/visym/vipy/master/docs/tutorials/annotate_an_image.jpg" height="500"></p>
<h3 id="save-an-image">Save an image</h3>
<p>Images can be saved (without annotations) using the <code><a title="vipy.image.Image.saveas" href="image.html#vipy.image.Image.saveas">Image.saveas()</a></code> method.
Calling this method with no arguments will save to a random temporary image.
In this example, we crop the image, convert from RGB colorspace to BGR colorspace, flip up/down and resize.</p>
<pre><code class="language-python">&gt;&gt;&gt; vipy.image.owl().centersquare().bgr().flipud().mindim(224).saveas('save_an_image.jpg')
</code></pre>
<p><img src="https://raw.githubusercontent.com/visym/vipy/master/docs/tutorials/save_an_image.jpg" height="300"></p>
<h3 id="convert-image-colorspace">Convert image colorspace</h3>
<p>All images can be converted between different colorspaces (e.g. RGB, BGR, RGBA, BGRA, HSV, GREY, LUM, float).
This will convert the underlying pixel buffer to support the corresponding colorspace.
</p>
<pre><code>&gt;&gt;&gt; vipy.image.owl().hsv().saveas('hsv.jpg')
</code></pre>
<h3 id="rescale-image">Rescale image</h3>
<p>All images can be rescaled to a standard range, including the Matlab inspired <code><a title="vipy.image.Image.mat2gray" href="image.html#vipy.image.Image.mat2gray">Image.mat2gray()</a></code>, which will rescale the pixel buffer between [min, max] -&gt; [0, 1]
This rescaling will take advantage of numba optimization if the optimal numba package is installed</p>
<h3 id="visualize-scenes">Visualize scenes</h3>
<p>Scenes containing objects can be visualized to display only a subset of objects.
In this example, we show the demo image <code><a title="vipy.image.vehicles" href="image.html#vipy.image.vehicles">vehicles()</a></code> which contains four annotated vehicles.
There are many more vehicles in this image, but the end user may be interested in these four in particular.
Each object is represented internally as a list of <code><a title="vipy.object.Detection" href="object.html#vipy.object.Detection">Detection</a></code> objects which encodes a bounding box and category.
This can be visualized just as with images with single objects.</p>
<pre><code class="language-python">&gt;&gt;&gt; vipy.image.vehicles().show().objects()
[&lt;vipy.object.detection: category=&quot;car&quot;, bbox=(xmin=210.2, ymin=263.2, width=41.1, height=32.6)&gt;,
 &lt;vipy.object.detection: category=&quot;car&quot;, bbox=(xmin=626.7, ymin=336.0, width=77.9, height=65.5)&gt;,
 &lt;vipy.object.detection: category=&quot;car&quot;, bbox=(xmin=140.8, ymin=284.5, width=53.1, height=53.1)&gt;,
 &lt;vipy.object.detection: category=&quot;car&quot;, bbox=(xmin=394.2, ymin=396.8, width=99.5, height=87.4)&gt;]
</code></pre>
<p><img src="https://raw.githubusercontent.com/visym/vipy/master/docs/tutorials/visualize_complex_scenes.jpg" height="500"></p>
<h3 id="get-all-of-the-object-categories">Get all of the object categories</h3>
<pre><code class="language-python">im = vipy.image.people()
categories = set(o.category() for o in im.objects())
</code></pre>
<h3 id="get-all-of-the-object-boxes">Get all of the object boxes</h3>
<pre><code class="language-python">im = vipy.image.people()
ulbr_boxes = [o.ulbr() for o in im.objects()]  # [(xmin,ymin,xmax,ymax),...] in upper-left-bottom-roght boxformat
xywh_boxes = [o.xywh() for o in im.objects()]  # [(xmin,ymin,width,height),...] in upper-left-width-height box format
</code></pre>
<h3 id="crop-and-resize-annotated-objects-in-a-scene">Crop and resize annotated objects in a scene</h3>
<pre><code class="language-python">&gt;&gt;&gt; im = vipy.image.vehicles().show()
&gt;&gt;&gt; vipy.visualize.montage([o.objectsquare(dilate=1.2) for o in im]).show()
</code></pre>
<p><img src="https://raw.githubusercontent.com/visym/vipy/master/docs/tutorials/vipy_image_vehicles.png" height="300">
<img src="https://raw.githubusercontent.com/visym/vipy/master/docs/tutorials/vipy_image_vehicles_objectcrop.png" height="300"></p>
<p>When iterating over a scene, each object yielded is a <code><a title="vipy.image.Scene" href="image.html#vipy.image.Scene">Scene</a></code> with a single object.
Objectsquare will crop the image using the bounding box equal to the union of all boxes in the current scene.
dilate will expand the size of the object bounding boxes by a scale factor.
The result is a cropped square image for each object that is centered on the object.
You can access the pixels for each cropped object, with ot without forcing the cropped region to be square:</p>
<pre><code class="language-python">&gt;&gt;&gt; pixels = [o.objectsquare(dilate=1.2).array() for o in im]
&gt;&gt;&gt; pixels = [o.objectcrop(dilate=1.2).array() for o in im]  # don't force cropped region to be square
</code></pre>
<p>As with all other <code><a title="vipy.image.Scene" href="image.html#vipy.image.Scene">Scene</a></code> objects, the original image can be arbitrarily transformed such as resizing or padded prior to exporting the object pixels.</p>
<h3 id="find-all-images-in-directory">Find all images in directory</h3>
<p>Searching for all images recursively from a root directory and lazy load them as <code><a title="vipy.image.Image" href="image.html#vipy.image.Image">Image</a></code> objects.
This will not trigger loading pixels until the pixel buffers are needed.
This is helpful for importing large number of images.</p>
<pre><code class="language-python">&gt;&gt;&gt; [vipy.image.Image(filename=f) for f in vipy.util.findimages('./docs/tutorials')]
[&lt;vipy.image: filename=&quot;/Users/myaccount/dev/vipy/docs/tutorials/transform_an_image_1.jpg&quot;&gt;, &lt;vipy.image: filename=&quot;/Users/myaccount/dev/vipy/docs/tutorials/transform_an_image_2.jpg&quot;&gt;, ... 
</code></pre>
<h3 id="export-scene-to-json">Export scene to JSON</h3>
<p>All annotated images can be imported and exported to an open JSON format. If images are loaded, then the pixels will be serialized in the JSON output.
If this is not desired, then use the `vipy.image.Image.flush`` method to clear the cached pixel buffer prior to serialization.
This can always be reloaded after deserialization as long as the source image or URL is acessible.</p>
<pre><code class="language-python">&gt;&gt;&gt; json = vipy.image.owl().flush().json()
&gt;&gt;&gt; im = vipy.image.Scene.from_json(json)
&gt;&gt;&gt; print(json)
'{&quot;_filename&quot;:&quot;\/Users\/myaccount\/.vipy\/1920px-Bubo_virginianus_06.jpg&quot;,&quot;_url&quot;:&quot;https:\/\/upload.wikimedia.org\/wikipedia\/commons\/thumb\/2\/23\/Bubo_virginianus_06.jpg\/1920px-Bubo_virginianus_06.jpg&quot;,&quot;_loader&quot;:null,&quot;_array&quot;:null,&quot;_colorspace&quot;:&quot;rgb&quot;,&quot;attributes&quot;:{},&quot;_category&quot;:&quot;Nature&quot;,&quot;_objectlist&quot;:[{&quot;_xmin&quot;:93.33333333333333,&quot;_ymin&quot;:85.33333333333333,&quot;_xmax&quot;:466.6666666666667,&quot;_ymax&quot;:645.3333333333334,&quot;_id&quot;:&quot;a047e21d&quot;,&quot;_label&quot;:&quot;Great Horned Owl&quot;,&quot;_shortlabel&quot;:&quot;Great Horned Owl&quot;}]}'
</code></pre>
<h3 id="export-scene-to-csv">Export scene to CSV</h3>
<p>All annotated images can be exported to a CSV format using object iterators.
Object precision can be changed using <code><a title="vipy.object.Detection.int" href="geometry.html#vipy.geometry.BoundingBox.int">BoundingBox.int()</a></code>.
CSV headers can be added with <code><a title="vipy.util.writecsv" href="util.html#vipy.util.writecsv">writecsv()</a></code>.</p>
<pre><code class="language-python">&gt;&gt;&gt; im = vipy.image.vehicles()
&gt;&gt;&gt; vipy.util.writecsv([(im.filename(), o.category(), o.xmin(), o.ymin(), o.width(), o.height()) for o in im.objects()], 'out.csv')
&gt;&gt;&gt; cat out.csv
/Users/myaccount/.vipy/I-80_Eastshore_Fwy.jpg,car,210.2222222222222,263.2,41.06666666666666,32.622222222222206
/Users/myaccount/.vipy/I-80_Eastshore_Fwy.jpg,car,626.6666666666666,336.0444444444444,77.86666666666667,65.4666666666667
/Users/myaccount/.vipy/I-80_Eastshore_Fwy.jpg,car,140.84444444444443,284.4888888888889,53.066666666666634,53.111111111111086
/Users/myaccount/.vipy/I-80_Eastshore_Fwy.jpg,car,394.17777777777775,396.84444444444443,99.4666666666667,87.37777777777774
</code></pre>
<h3 id="image-deduplication">Image deduplication</h3>
<p>Vipy provides a 128 bit differential perceptual hashing function which is used for near-duplicate detection.
This is useful for identifying pairs of images that differ slightly due to cropping, resizing, watermarkings.
The binary Hamming distance between two perceptual hashes is a similarity metric that can be used to identify duplicates, such that smaller is more likely to be a duplicate.</p>
<pre><code class="language-python">&gt;&gt;&gt; p = vipy.image.vehicles().perceptualhash()  # hex string
&gt;&gt;&gt; print(p)
'50515541d545f04101a005e801c25945'
&gt;&gt;&gt; q = vipy.image.vehicles().greyscale().perceptualhash()
&gt;&gt;&gt; print(q)
'50515541d545f04101a905e801c27945'
&gt;&gt;&gt; vipy.image.Image.perceptualhash_distance(p, q)  # Hamming distance
3
</code></pre>
<p>The perceptual hash function also allows for ignoring detected objects in the foreground.
A background hash <code><a title="vipy.image.Scene.bghash" href="image.html#vipy.image.Scene.bghash">Scene.bghash()</a></code> computes the perceptual hash function using only the regions not contained within the foreground bounding boxes.
This is useful for identifying near duplicate background locations where there may be different foreground objects in the scene between images.
If the <code><a title="vipy.image.Scene" href="image.html#vipy.image.Scene">Scene</a></code> has no associated foreground objects, then the background hash is equivalent to the perceptual hash above.</p>
<h3 id="blur-faces">Blur Faces</h3>
<pre><code class="language-python">&gt;&gt;&gt; im = vipy.image.Image(url='https://upload.wikimedia.org/wikipedia/en/d/d6/Friends_season_one_cast.jpg')
&gt;&gt;&gt; im.facepixelize().show()
&gt;&gt;&gt; im.faceblur().show()
</code></pre>
<p><img src="https://raw.githubusercontent.com/visym/vipy/master/docs/tutorials/blur_faces_1.jpg" height="250">
<img src="https://raw.githubusercontent.com/visym/vipy/master/docs/tutorials/blur_faces_2.jpg" height="250"></p>
<p>This is an experimental feature and may be removed in future releases.</p>
<h3 id="data-augmentation-for-training">Data augmentation for training</h3>
<p>Data augmentation is the process of introducing synthetic transformations of a given image to introduce additional variation during training.
Data augmentation considers scales, crops, translations, mirrors, rotations or chromatic noise which are applied to a source image to generate one or more augmentations.
All pixel buffers are shared by default for speed, so the clone() method will enforce that pixel buffers are copied when needed.</p>
<pre><code class="language-python">im = vipy.image.vehicles()
vipy.visualize.montage([[o.crop().fliplr(),                # spatial mirror
                         o.clone().dilate(1.2).crop(),     # zoom out
                         o.clone().translate(4,5).crop(),  # translation 
                         o.clone().translate(-2,9).crop(), # translation 
                         o.clone().dilate(0.8).crop(),     # zoom in 
                         o.crop().blur(sigma=1),           # spatial blur
                         o.crop().additive_noise()]        # chromatic noise 
                         for o in im])                     # for all objects in the scene
</code></pre>
<p><img src="https://raw.githubusercontent.com/visym/vipy/master/docs/tutorials/data_augmentation_for_training.jpg" height="350"></p>
<p>These functions have been integrated into a single package <code><a title="vipy.noise" href="noise.html">vipy.noise</a></code> which implements photometric and geometric perturbations.</p>
<pre><code class="language-python">im = vipy.image.vehicles()
new_im = vipy.noise.randomcrop(im)
</code></pre>
<p>The image returned is cloned, and includes the provenance of the noise source in new_im.attributes</p>
<h3 id="vipy-and-torchvision">Vipy and Torchvision</h3>
<p>All vipy objects can be exported to torch tensors:</p>
<pre><code class="language-python">im = vipy.image.vehicles().load().tensor(order='CHW')
</code></pre>
<p>with axis permutations to export in channel first order used by pytorch tensors.</p>
<h3 id="visualization-behind-ssh">Visualization behind SSH</h3>
<p>Data repositories are often accessed via data storage behind SSH.
You can set up port forwarding to visualize this data, but this may require root access to configure firewall rules.
If you have SSH public key access to your cluster machine, you can do the following:</p>
<p>On a remote machine (e.g. the cluster machine you have accessed via ssh), run:</p>
<pre><code class="language-python">remote&gt;&gt;&gt; vipy.util.scpsave(vipy.image.owl())
[vipy.util.scpsave]: On a local machine where you have public key ssh access to this remote machine run:
&gt;&gt;&gt; V = vipy.util.scpload('scp://hostname:/var/folders/sn/6n34qjp513742_5y3lvmhnlw0000gn/T/c4237a25a99b776f.json')
</code></pre>
<p>Then, on your local machine (e.g. your laptop), run the command output above:</p>
<pre><code class="language-python">local&gt;&gt;&gt; print(vipy.util.scpload('scp://hostname:/var/folders/sn/6n34qjp513742_5y3lvmhnlw0000gn/T/c4237a25a99b776f.json'))
&lt;vipy.image.scene: height=640, width=512, color=rgb, filename=&quot;/tmp/.vipy/1920px-Bubo_virginianus_06.jpg&quot;, url=https://upload.wikimedia.org/wikipedia/commons/thumb/2/23/Bubo_virginianus_06.jpg/1920px-Bubo_virginianus_06.jpg, category=&quot;Nature&quot;, objects=1&gt;
</code></pre>
<p>The method <code><a title="vipy.util.scpsave" href="util.html#vipy.util.scpsave">scpsave()</a></code> will save a list of vipy objects to a temporary archive file, such that the URL of each object is prepended with "scp://".
When calling <code><a title="vipy.util.scpload" href="util.html#vipy.util.scpload">scpload()</a></code> on the local machine, this will fetch the pickle file from the remote machine via scp using the default public key.
Then, when each vipy object is accessed, it will fetch the URL of the media object via scp from the remote machine.
This provides an on-demand fetching of each image from a data storage behind a SSH server without any port forwarding, and uses public key scp.
This allows for visualization of datasets that cannot be copied locally, but can be reduced on the local machine which are then fetched for visualization.</p>
<h3 id="visualization-behind-aws-s3">Visualization behind AWS S3</h3>
<p>Data repositories are often stored with cloud service providers, such as Amazon AWS.
These providers require credentials to access URLs in Simple Storage Service (S3).
Vipy supports accessing AWS S3 URLs with credential restricted access.
Set the following environment variables for the access key and secret access key provided by Amazon AWS.
Follow the links below to get a key:</p>
<ul>
<li><strong>VIPY_AWS_ACCESS_KEY_ID</strong>='MYKEY'.
This is the <a href="https://docs.aws.amazon.com/general/latest/gr/aws-sec-cred-types.html">AWS key</a> to download urls of the form "s3://".
</li>
<li><strong>VIPY_AWS_SECRET_ACCESS_KEY</strong>='MYKEY'.
This is the <a href="https://docs.aws.amazon.com/general/latest/gr/aws-sec-cred-types.html">AWS secret key</a> to download urls of the form "s3://".</li>
</ul>
<p>Then prepend the URL scheme "s3://BUCKET_NAME.s3.amazonaws.com/OBJECT_PATH" when constructing a URL.
Here is an example that we use for <a href="https://visym.com/collector">Visym Collector</a> to store videos uploaded from around the world:</p>
<pre><code class="language-python">&gt;&gt;&gt; vipy.image.Image(url=&quot;s3://bucket/path/to/file.mp4&quot;)
</code></pre>
<p>Finally, if the credentials you provide are authorized to access this bucket and object, then this object will be downloaded on-demand when the pixels are needed.
This provides a convenient method of on-demand downloading and caching of large datasets.</p>
<h2 id="videos">Videos</h2>
<h3 id="load-from-youtube">Load from YouTube</h3>
<pre><code class="language-python">v = vipy.video.Video(url='https://youtu.be/kpBCzzzX6zA').download()
</code></pre>
<h3 id="inspect-the-ffmpeg-command-line">Inspect the FFMPEG command line</h3>
<pre><code class="language-python">print(vipy.video.Video(filename='/path/to/in.mp4').mindim(512).framerate(2).commandline())
</code></pre>
<pre><code>'ffmpeg -i /path/to/in.mp4 -filter_complex &quot;[0]fps=fps=2.0:round=up[s0];[s0]scale=-1:512[s1]&quot; -map &quot;[s1]&quot; dummyfile'
</code></pre>
<h3 id="export-frames-as-vipy-images">Export frames as vipy images</h3>
<pre><code class="language-python">frames = [im for im in v.framerate(1)]   # 1 Hz export
</code></pre>
<h3 id="export-frames-as-numpy-array">Export frames as numpy array</h3>
<pre><code class="language-python">frames = v.framerate(0.1).numpy()   # 0.1 Hz export
</code></pre>
<h3 id="generate-webp-animations">Generate WEBP animations</h3>
<pre><code class="language-python">v = vipy.video.RandomScene().clip(0,30).webp()
</code></pre>
<h3 id="find-all-videos-in-directory">Find all videos in directory</h3>
<pre><code class="language-python">dataset = vipy.dataset.Dataset.from_directory('/path/to/dir', filetype='mp4')
</code></pre>
<h3 id="import-rtsp-camera-streams">Import RTSP camera streams</h3>
<pre><code class="language-python">v = vipy.video.Video(url='rtsp://user:password@10.0.1.19/live0')
</code></pre>
<p>An RTSP camera can be modified to change the framerate or the resolution then the video can be streamed live or iterated as with other videos'</p>
<pre><code class="language-python">vipy.video.Video(url='rtsp://user:password@10.0.1.19/live0', framerate=5).mindim(256).show()
</code></pre>
<p>To grab a single frame:</p>
<pre><code class="language-python">im = vipy.video.Video(url='rtsp://user:password@10.0.1.19/live0').frame()
</code></pre>
<p>To iterate:</p>
<pre><code class="language-python">for im in vipy.video.Video(url='rtsp://user:password@10.0.1.19/live0'):
    print(im)
</code></pre>
<p>To save to a video file:</p>
<pre><code class="language-python">vipy.video.Video(url='rtsp://user:password@10.0.1.19/live0').save('/path/to/video.mp4')
</code></pre>
<p>You will need to interrupt the saving when you're done and the video up to that point will be available in the video file</p>
<h3 id="split-a-video-into-activity-clips">Split a video into activity clips</h3>
<pre><code class="language-python">clips = vipy.video.RandomScene().activityclip()
</code></pre>
<p>Each clip is a separate video with a single activity, such that the video is clipped at the temporal extent of this activity</p>
<h3 id="split-a-video-into-track-clips">Split a video into track clips</h3>
<pre><code class="language-python">clips = vipy.video.RandomScene().trackclip()
</code></pre>
<p>Each clip is a separate video with a single track, such that the video is clipped at the temporal extent of this track</p>
<h3 id="create-quicklooks-for-fast-video-watching">Create quicklooks for fast video watching</h3>
<pre><code class="language-python">vipy.video.RandomScene().quicklook(),show()
</code></pre>
<p>A quicklook is a montage constructed by nine frames sampled from the video.
This is a convenient way to visualize a video</p>
<h3 id="export-to-json">Export to JSON</h3>
<pre><code class="language-python">vipy.video.RandomScene().flush().json()
</code></pre>
<h2 id="datasets">Datasets</h2>
<h3 id="load-a-dataset-from-the-registry">Load a dataset from the registry</h3>
<pre><code class="language-python">vipy.dataset.registry('mnist')
</code></pre>
<p>The registry is the common entry point for loading collections of annotated visual data.
The datasets are downloaded and imported when requested.
Datasets are constructed as a large number of python objects, which can lead to slow garbage collection.
During registry import, the garbage collector is frozen by default, so that the loaded dataset is disabled for reference cycle counting.</p>
<h3 id="shuffling">Shuffling</h3>
<p>Shuffling a dataset uniformly at random can lead to inefficiencies due to random data access.
The <code>vipy.dataset.Datase.shuffle</code> method support uniform random shuffling, but it will attempt to use a more efficient streaming shuffler to allow for iterative access rather than random access.
This streaming shuffler will shuffle the underlying dataset (or dataset shards) rather than shuffling the dataset index.
This leads to better data locality in data access and more efficient iterative access for large datasets.</p>
<h3 id="download-a-dataset-with-urls">Download a dataset with URLs</h3>
<pre><code class="language-python">with vipy.parallel.multiprocessing(4):
    vipy.dataset.registry('kinetics').map(vipy.video.Transform.downloader(outdir='/tmp')) 
</code></pre>
<p>This will download the youtube videos from the kinetics dataset with four parallel processes. Videos will be stored in the requested outdir.</p>
<h3 id="create-a-dataset-from-images">Create a dataset from images</h3>
<pre><code class="language-python">D = vipy.dataset.Dataset.from_directory('/path/to/dir', filetype='images')
</code></pre>
<h3 id="determine-the-set-of-classes">Determine the set of classes</h3>
<pre><code class="language-python">trainset = vipy.dataset.registry('cifar10')
print(trainset.set(lambda im: im.category()))
</code></pre>
<h3 id="count-objects-in-a-scene">Count objects in a scene</h3>
<pre><code class="language-python">im = vipy.dataset.registry('coco_2014').takeone()
vipy.dataset.Dataset(im.objects()).frequency(lambda o: o.category())
</code></pre>
<p>Return a dictionary of the counts of object categories from a random coco-2014 scene.</p>
<h3 id="take-a-subset">Take a subset</h3>
<pre><code class="language-python">trainset = vipy.dataset.registry('cifar10')
print(trainset.take(1024))
</code></pre>
<h3 id="compute-inverse-frequency-weights">Compute inverse frequency weights</h3>
<pre><code class="language-python">vipy.dataset.registry('cifar10').inverse_frequency(lambda im: im.category())
</code></pre>
<p>These weights can be used for weighting a loss function for imbalanced classes.</p>
<h3 id="iterate-over-minibatches">Iterate over minibatches</h3>
<pre><code class="language-python">for b in vipy.dataset.registry('cifar10').minibatch(128):
     print(b)
</code></pre>
<h3 id="iterate-over-minibatches-with-parallel-preprocessing">Iterate over minibatches with parallel preprocessing</h3>
<pre><code class="language-python">with vipy.globals.multiprocessing(4):
    for b in vipy.dataset.registry('cifar10').minibatch(128, loader=vipy.image.Transform.to_tensor(shape=(16,16), gain=1/255)):
        print(b)  # these batches have been preprocessed to shape (16,16) and in the range [0,1]
</code></pre>
<h3 id="combine-datasets-as-a-union">Combine datasets as a union</h3>
<pre><code class="language-python">vipy.dataset.registry(('mnist', 'cifar10', 'caltech101'))
</code></pre>
<p>A dataset untion will iterate over the component datasets</p>
<h1 id="contact">Contact</h1>
<p>Visym Labs &lt;<a href="&#109;&#97;&#105;&#108;&#116;&#111;&#58;&#105;&#110;&#102;&#111;&#64;&#118;&#105;&#115;&#121;&#109;&#46;&#99;&#111;&#109;">&#105;&#110;&#102;&#111;&#64;&#118;&#105;&#115;&#121;&#109;&#46;&#99;&#111;&#109;</a>&gt;</p>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/visym/vipy/blob/a3e4013930bb458420e1fcc86216fa191c42b880/vipy/__init__.py#L1-L688" class="git-link">Browse git</a>
</summary>
<pre><code class="python">&#34;&#34;&#34;
VIPY is a python package for representation, transformation and visualization of annotated videos and images.  Annotations are the ground truth provided by labelers (e.g. object bounding boxes, face identities, temporal activity clips), suitable for training computer vision systems.  VIPY provides tools to easily edit videos and images so that the annotations are transformed along with the pixels.  This enables a clean interface for transforming complex datasets for input to your computer vision training and testing pipeline.

VIPY provides:

* Representation of videos with labeled activities that can be resized, clipped, rotated, scaled and cropped
* Representation of images with object bounding boxes that can be manipulated as easily as editing an image
* Clean visualization of annotated images and videos
* Lazy loading of images and videos suitable for distributed procesing (e.g. dask, spark)
* Straightforward integration into machine learning toolchains (e.g. torch, numpy)
* Fluent interface for chaining operations on videos and images
* Visual dataset download, unpack and import (e.g. Imagenet21k, Coco 2014, Visual Genome, Open Images V7, Kinetics700, YoutubeBB, ActivityNet, ... )
* Video and image web search tools with URL downloading and caching
* Minimum dependencies for easy installation (e.g. AWS Lambda)

# Design Goals

Vipy was created with three design goals.  

* **Simplicity**.  Annotated Videos and images should be as easy to manipulate as the pixels.  We provide a simple fluent API that enables the transformation of media so that pixels are transformed along with the annotations.  We provide a comprehensive unit test suite to validate this pipeline with continuous integration.
* **Portability**.  Vipy was designed with the goal of allowing it to be easily retargeted to new platforms.  For example, deployment on a serverless architecture such as AWS lambda has restrictions on the allowable code that can be executed in layers.  We designed Vipy with minimal dependencies on standard and mature machine learning tool chains (numpy, matplotlib, ffmpeg, pillow) to ensure that it can be ported to new computational environments. 
* **Efficiency**.  Vipy is written in pure python with the goal of minimizing memory requirementts for large datasets, and performing in place operations to avoid copies.  This enables fast video processing by operating on videos as chains of transformations.  The documentation describes when an object is changed in place vs. copied.  Furthermore, loading is delayed until explicitly requested by the user (or the pixels are needed) to enable lazy loading for distributed processing.  


# Getting started

The VIPY tools are designed for simple and intuitive interaction with videos and images.  Try to create a `vipy.video.Scene` object:

```python
v = vipy.video.RandomScene()
```

Videos are constructed from URLs (e.g. RTSP/RTMP live camera streams, YouTube videos, public or keyed AWS S3 links), SSH accessible paths, local filenames, `vipy.image.Image` frame lists, numpy arrays or pytorch tensors.  In this example, we create a random video with tracks and activities.  Videos can be natively iterated:


```python
for im in v:
    print(im.numpy())
```

This will iterate and yield `vipy.image.Image` objects corresponding to each frame of the video.  You can use the `vipy.image.Image.numpy` method to extract the numpy array for this frame.  Long videos are streamed to avoid out of memory errors.  Under the hood, we represent each video as a filter chain to an FFMPEG pipe, which yields frames corresponding to the appropriate filter transform and framerate.  The yielded frames include all of the objects that are present in the video at that frame accessible with the `vipy.image.Scene.objects` method.

VIPY supports more complex iterators.  For example, a common use case for activity detection is iterating over short clips in a video.  You can do this using the stream iterator:


```python
for c in v.stream().clip(16):
    print(c.torch())
```
       
This will yield `vipy.video.Scene` objects each containing a `vipy.video.Stream.clip` of length 16 frames.  Each clip overlaps by 15 frames with the next clip, and each clip includes a threaded copy of the pixels.  This is useful to provide clips of a fixed length that are output for every frame of the video.  Each clip contais the tracks and activities within this clip time period.  The method `vipy.video.Video.torch` will output a torch tensor suitable for integration into a pytorch based system.

These python iterators can be combined together in complex ways

```python
for (im, c, imdelay) in (v, v.stream().clip(16), v.stream().frame(delay=10), a_gpu_function(v.stream().batch(16)))
    print(im, c.torch(), imdelay)
```

This will yield the current frame, a video `vipy.video.Stream.clip` of length 16, a `vipy.video.Stream.frame` 10 frames ago and a `vipy.video.Stream.batch` of 16 frames that is designed for computation and transformation on a GPU.  All of the pixels are copied in threaded processing which is efficiently hidden by GPU I/O bound operations.  For more examples of complex iterators in real world use cases, see the [HeyVi package](https://github.com/visym/heyvi) for open source visual analytics.

Videos can be transformed in complex ways, and the pixels will always be transformed along with the annotations.

```python
v.fliplr()          # flip horizontally
v.zeropad(10, 20)   # zero pad the video horizontally and vertically
v.mindim(256)       # change the minimum dimension of the video
v.framerate(10)     # change the framerate of the video 
```

The transformation is lazy and is incorporated into the FFMPEG complex filter chain so that the transformation is applied when the pixels are needed.  You can always access the current filter chain using `vipy.video.Video.commandline` which will output a commandline string for the ffmpeg executable that you can use to get a deeper underestanding of the transformations that are applied to the video pixels.

Finally, annotated videos can be displayed. 

```python
v.show()
v.show(notebook=True)
v.frame().show()
v.annotate(&#39;/path/to/visualization.mp4&#39;)
with vipy.video.Video(url=&#39;rtmps://youtu.be/...&#39;).mindim(512).framerate(5).stream(write=True) as s:
    for im in v.framerate(5):
        s.write(im.annotate().rgb())
```

This will `vipy.video.Scene.show` the video live on your desktop, in a jupyter notebook, show the first `vipy.video.Scene.frame` as a static image, `vipy.video.Scene.annotate` the video so that annotations are in the pixels and save the corresponding video, or live stream a 5Hz video to youtube.  All of the show methods can be configured to customize the colors or captions.

See the [demos](https://github.com/visym/vipy/tree/master/demo) for more examples.



## Import

Vipy was designed to define annotated videos and imagery as collections of python objects.  The core objects for images are:

* [vipy.image.Scene](image.html#vipy.image.Scene)
* [vipy.object.Detection](object.html#vipy.object.Detection)
* [vipy.object.Keypoint2d](object.html#vipy.object.Keypoint2d)
* [vipy.geometry.BoundingBox](geometry.html#vipy.geometry.BoundingBox)
* [vipy.geometry.Point2d](geometry.html#vipy.geometry.Point2d)

The core objects for videos:

* [vipy.video.Scene](video.html#vipy.video.Scene)
* [vipy.object.Track](object.html#vipy.object.Track)
* [vipy.activity.Activity](activity.html#vipy.activity.Activity)

See the documentation for each object for how to construct them.  


## Export

All vipy objects can be imported and exported to JSON for interoperatability with other tool chains.  This allows for introspection of the vipy object state in an open format providing transparency

```python
vipy.image.owl().json()
```

## Environment variables

You can set the following environment variables to customize the output of vipy

* **VIPY_CACHE**=&#39;/path/to/directory&#39;.  This directory will contain all of the cached downloaded filenames when downloading URLs.  For example, the following will download all media to &#39;~/.vipy&#39;.

```python
os.environ[&#39;VIPY_CACHE&#39;] = vipy.util.remkdir(&#39;~/.vipy&#39;)
vipy.image.Image(url=&#39;https://upload.wikimedia.org/wikipedia/commons/thumb/2/23/Bubo_virginianus_06.jpg/1920px-Bubo_virginianus_06.jpg&#39;).download()
```

This will output an image object:
```python
&lt;vipy.image: filename=&#34;~/.vipy/1920px-Bubo_virginianus_06.jpg&#34;, filename=&#34;~/.vipy/1920px-Bubo_virginianus_06.jpg&#34;, url=&#34;https://upload.wikimedia.org/wikipedia/commons/thumb/2/23/Bubo_virginianus_06.jpg/1920px-Bubo_virginianus_06.jpg&#34;&gt;
```

This provides control over where large datasets are cached on your local file system.  By default, this will be cached to the system temp directory.

* **VIPY_DATASET_REGISTRY_HOME**=&#39;/path/to/dir&#39;.  This is the directory to download datasets in `vipy.dataset.registry`.
* **VIPY_AWS_ACCESS_KEY_ID**=&#39;MYKEY&#39;.  This is the [AWS key](https://docs.aws.amazon.com/general/latest/gr/aws-sec-cred-types.html) to download urls of the form &#34;s3://&#34;.  
* **VIPY_AWS_SECRET_ACCESS_KEY**=&#39;MYKEY&#39;.   This is the [AWS secret key](https://docs.aws.amazon.com/general/latest/gr/aws-sec-cred-types.html) to download urls of the form &#34;s3://&#34;.
* **VIPY_BACKEND**.   This is the [Matplotlib backend](https://matplotlib.org/stable/users/explain/backends.html) to use when rendering figure windows.  &#39;Agg&#39; is recommended for headless operation, and &#39;TkAgg&#39; is recommended for Linux based X11 forwarding.  In most cases, matplotlib will choose the best backend available by default, and this environment variable does not need to override this choice.


## Versioning

To determine what vipy version you are running you can use:

&gt;&gt;&gt; vipy.__version__
&gt;&gt;&gt; vipy.__version__.is_at_least(&#39;1.16.2&#39;) 

## Parallelization

Vipy includes integration with [concurrent futures](https://docs.python.org/3/library/concurrent.futures.html) and [Dask Distributed](https://distributed.dask.org) for parallel processing of video and images.   This is useful for preprocessing of datasets to prepare them for training.  

For example, we can construct a `vipy.dataset.Dataset` object from one or more videos.  This dataset can be transformed in parallel using two processes:

```python
D = vipy.dataset.Dataset(vipy.video.Scene(filename=&#39;/path/to/videofile.mp4&#39;))
with vipy.globals.parallel(2):
    R = D.map(lambda v, outdir=&#39;/newpath/to/&#39;: v.mindim(128).framerate(5).saveas(so.path.join(outdir, vipy.util.filetail(v.filename()))))
```

The result is a transformed dataset which contains transformed videos downsampled to have minimum dimension 128, framerate of 5Hz, with the annotations transformed accordingly.  The `vipy.dataset.Dataset.map` method allows for a lambda function to be applied in parallel to all elements in a dataset.  The fluent design of the VIPY objects allows for easy chaining of video operations to be expressed as a lambda function.  VIPY objects are designed for integration into parallel processing tool chains and can be easily serialized and deserialized for sending to parallel worker tasks.  

VIPY supports integration with distributed schedulers for massively parallel operation.  

```python
D = vipy.dataset.Dataset(&#39;/path/to/directory/of/jsonfiles&#39;)
with vipy.globals.dask(scheduler=&#39;10.0.0.1:8785&#39;):
    R = D.map(lambda v, outdir=&#39;/newpath/to&#39;: vipy.util.bz2pkl(os.path.join(outdir, &#39;%s.pkl.bz2&#39; % v.videoid()), v.trackcrop().mindim(128).normalize(mean=(128,128,128)).torch()))
```

This will lazy load a directory of JSON files, where each JSON file corresponds to the annotations of a single video, such as those collected by [Visym Collector](https://visym.github.io/collector).   The `vipy.dataset.Dataset.map` method will communicate with a [scheduler](https://docs.dask.org/en/stable/how-to/deploy-dask/ssh.html) at a given IP address and port and will process the lambda function in parallel to the workers tasked by the scheduler.  In this example, the video will `vipy.video.Scene.trackcrop` the smallest bounding box containing all tracks in the video, resized so this crop is 128 on the smallest side, loaded and normalized to remove the mean, then saved as a torch tensor in a bzipped python pickle file.  This is useful for preprocesssing videos to torch tensors for fast loading of dataset augmentation during training.

# Tutorials

The following tutorials show fluent python chains to achieve transformations of annotated images and videos.

## Images

### Load an image

Images can be loaded from URLs, local image files, or numpy arrays.  The images exhibit lazy loading, so that pixels will not be fetched until they are needed.

```python
&gt;&gt;&gt; im = vipy.image.Image(filename=&#39;/path/to/in.jpg&#39;)  
&gt;&gt;&gt; im = vipy.image.Image(url=&#39;https://url/to/in.jpg&#39;)  
&gt;&gt;&gt; im = vipy.image.Image(array=np.random.rand(224,224,3).astype(np.float32))  
```

### Print an image representation 

All objects have helpful string representations when printed to stdout.  This is accessible via the `vipy.image.Image.print` method or by using builtin print().  In this example, an image is created from a wikipedia URL.  Printing this image object shows the URL, but when it is loaded, the image object shows the size of the image, colorspace and the filename that the URL was downloaded to.  When in doubt, print!

```python
&gt;&gt;&gt; print(vipy.image.Scene(url=&#39;https://upload.wikimedia.org/wikipedia/commons/thumb/2/23/Bubo_virginianus_06.jpg/1920px-Bubo_virginianus_06.jpg&#39;))
&lt;vipy.image.scene: url=https://upload.wikimedia.org/wikipedia/commons/thumb/2/23/Bubo_virginianus_06.jpg/1920px-Bubo_virginianus_06.jpg&gt;

&gt;&gt;&gt; vipy.image.Scene(url=&#39;https://upload.wikimedia.org/wikipedia/commons/thumb/2/23/Bubo_virginianus_06.jpg/1920px-Bubo_virginianus_06.jpg&#39;).load().print()
&lt;vipy.image.scene: height=2400, width=1920, color=rgb, filename=&#34;/tmp/1920px-Bubo_virginianus_06.jpg&#34;, url=https://upload.wikimedia.org/wikipedia/commons/thumb/2/23/Bubo_virginianus_06.jpg/1920px-Bubo_virginianus_06.jpg&gt;
```

### Transform an image

Images can be transformed so that the annotations are updated along with the pixels.  In this example, the `vipy.image.owl` is a demo image to a wikipedia URL with a bounding box.  This can be resized and cropped or anisotropically scaled and the box is updated to match the pixels. 

```python
&gt;&gt;&gt; im = vipy.image.owl().mindim(512).fliplr().centersquare().show()
&gt;&gt;&gt; im = vipy.image.owl().resize(width=512, height=256).show()
```
&lt;img src=&#34;https://raw.githubusercontent.com/visym/vipy/master/docs/tutorials/transform_an_image_1.jpg&#34; height=&#34;250&#34;&gt;
&lt;img src=&#34;https://raw.githubusercontent.com/visym/vipy/master/docs/tutorials/transform_an_image_2.jpg&#34; height=&#34;250&#34;&gt;


### Export as numpy array

All images are represented internally as a private attribute `vipy.image.Image._array` which is a numpy array representation of the pixels.  Image transformations can be chained to operate sequentially on this pixel buffer.  In this example, the `vipy.image.owl` test image is cropped to retain the center square, converted from uint8 RGB to float32 greyscale, resized to 224x224 then exported to numpy array.  

```python
&gt;&gt;&gt; vipy.image.owl().centersquare().greyscale().mindim(224).numpy()
array([[0.11470564, 0.11794835, 0.13006495, ..., 0.15657625, 0.15867704,
        0.16140679],
       [0.11835834, 0.11993656, 0.12860955, ..., 0.15611856, 0.15460114,
        0.15652661],
       [0.12262769, 0.1245698 , 0.12809968, ..., 0.153694  , 0.15326852,
        0.15336327],
       ...,
       [0.42591274, 0.42745316, 0.4352066 , ..., 0.12994824, 0.13172676,
        0.13424061],
       [0.42972928, 0.43847743, 0.45459685, ..., 0.12558977, 0.12820148,
        0.13141613],
       [0.44050908, 0.45350933, 0.46908155, ..., 0.12246227, 0.1256479 ,
        0.12941177]], dtype=float32)
```

### Display an image

All images can be displayed using the matplotlib library.  Matplotlib is the most universally ported GUI library for python, and exhibits minimal dependencies.  We enable the user to show images using figure window or &#34;matlab style&#34; of image display.  This will show pixels with overlayed semi-transparent bounding boxes for objects with captions.

```python
&gt;&gt;&gt; im = vipy.image.owl().mindim(512).show()
```
&lt;img src=&#34;https://raw.githubusercontent.com/visym/vipy/master/docs/tutorials/display_an_image.jpg&#34; height=&#34;500&#34;&gt;

All images can be displayed in a dark theme or a light theme.  Light themes show captions on light backgrounds, dark theme shows captions on a dark background.

```python
&gt;&gt;&gt; im = vipy.image.owl().mindim(512).show(theme=&#39;dark&#39;)
```

### Annotate an image

By default, images and annotations are represented independently.  However, it is sometimes useful to export the annotations into the pixels.  The `vipy.image.Scene.annotate` method will export the same visualization as when the image is displayed, but the pixel buffer will be overwritten with the shown image.  This means that calling `vipy.image.Image.numpy` will return the pixel buffer with boxes and captions in the pixels.

```python
&gt;&gt;&gt; vipy.image.owl().mindim(512).maxmatte().annotate().rgb().saveas(&#39;out.jpg&#39;)
```
&lt;img src=&#34;https://raw.githubusercontent.com/visym/vipy/master/docs/tutorials/annotate_an_image.jpg&#34; height=&#34;500&#34;&gt;


### Save an image

Images can be saved (without annotations) using the `vipy.image.Image.saveas` method.  Calling this method with no arguments will save to a random temporary image.  In this example, we crop the image, convert from RGB colorspace to BGR colorspace, flip up/down and resize.

```python
&gt;&gt;&gt; vipy.image.owl().centersquare().bgr().flipud().mindim(224).saveas(&#39;save_an_image.jpg&#39;)
```
&lt;img src=&#34;https://raw.githubusercontent.com/visym/vipy/master/docs/tutorials/save_an_image.jpg&#34; height=&#34;300&#34;&gt;

### Convert image colorspace

All images can be converted between different colorspaces (e.g. RGB, BGR, RGBA, BGRA, HSV, GREY, LUM, float).  This will convert the underlying pixel buffer to support the corresponding colorspace.  

``` 
&gt;&gt;&gt; vipy.image.owl().hsv().saveas(&#39;hsv.jpg&#39;)
```

### Rescale image

All images can be rescaled to a standard range, including the Matlab inspired `vipy.image.Image.mat2gray`, which will rescale the pixel buffer between [min, max] -&gt; [0, 1]
This rescaling will take advantage of numba optimization if the optimal numba package is installed

### Visualize scenes

Scenes containing objects can be visualized to display only a subset of objects.  In this example, we show the demo image `vipy.image.vehicles` which contains four annotated vehicles.  There are many more vehicles in this image, but the end user may be interested in these four in particular.  Each object is represented internally as a list of `vipy.object.Detection` objects which encodes a bounding box and category.  This can be visualized just as with images with single objects.

```python
&gt;&gt;&gt; vipy.image.vehicles().show().objects()
[&lt;vipy.object.detection: category=&#34;car&#34;, bbox=(xmin=210.2, ymin=263.2, width=41.1, height=32.6)&gt;,
 &lt;vipy.object.detection: category=&#34;car&#34;, bbox=(xmin=626.7, ymin=336.0, width=77.9, height=65.5)&gt;,
 &lt;vipy.object.detection: category=&#34;car&#34;, bbox=(xmin=140.8, ymin=284.5, width=53.1, height=53.1)&gt;,
 &lt;vipy.object.detection: category=&#34;car&#34;, bbox=(xmin=394.2, ymin=396.8, width=99.5, height=87.4)&gt;]
```
&lt;img src=&#34;https://raw.githubusercontent.com/visym/vipy/master/docs/tutorials/visualize_complex_scenes.jpg&#34; height=&#34;500&#34;&gt;

### Get all of the object categories

```python
im = vipy.image.people()
categories = set(o.category() for o in im.objects())
```

### Get all of the object boxes

```python
im = vipy.image.people()
ulbr_boxes = [o.ulbr() for o in im.objects()]  # [(xmin,ymin,xmax,ymax),...] in upper-left-bottom-roght boxformat
xywh_boxes = [o.xywh() for o in im.objects()]  # [(xmin,ymin,width,height),...] in upper-left-width-height box format
```

### Crop and resize annotated objects in a scene

```python
&gt;&gt;&gt; im = vipy.image.vehicles().show()
&gt;&gt;&gt; vipy.visualize.montage([o.objectsquare(dilate=1.2) for o in im]).show()
```
&lt;img src=&#34;https://raw.githubusercontent.com/visym/vipy/master/docs/tutorials/vipy_image_vehicles.png&#34; height=&#34;300&#34;&gt;
&lt;img src=&#34;https://raw.githubusercontent.com/visym/vipy/master/docs/tutorials/vipy_image_vehicles_objectcrop.png&#34; height=&#34;300&#34;&gt;

When iterating over a scene, each object yielded is a `vipy.image.Scene` with a single object.  Objectsquare will crop the image using the bounding box equal to the union of all boxes in the current scene.  dilate will expand the size of the object bounding boxes by a scale factor.  The result is a cropped square image for each object that is centered on the object.  You can access the pixels for each cropped object, with ot without forcing the cropped region to be square:

```python
&gt;&gt;&gt; pixels = [o.objectsquare(dilate=1.2).array() for o in im]
&gt;&gt;&gt; pixels = [o.objectcrop(dilate=1.2).array() for o in im]  # don&#39;t force cropped region to be square
```

As with all other `vipy.image.Scene` objects, the original image can be arbitrarily transformed such as resizing or padded prior to exporting the object pixels.

### Find all images in directory

Searching for all images recursively from a root directory and lazy load them as `vipy.image.Image` objects.  This will not trigger loading pixels until the pixel buffers are needed.  This is helpful for importing large number of images.

```python
&gt;&gt;&gt; [vipy.image.Image(filename=f) for f in vipy.util.findimages(&#39;./docs/tutorials&#39;)]
[&lt;vipy.image: filename=&#34;/Users/myaccount/dev/vipy/docs/tutorials/transform_an_image_1.jpg&#34;&gt;, &lt;vipy.image: filename=&#34;/Users/myaccount/dev/vipy/docs/tutorials/transform_an_image_2.jpg&#34;&gt;, ... 
```

### Export scene to JSON

All annotated images can be imported and exported to an open JSON format. If images are loaded, then the pixels will be serialized in the JSON output.  If this is not desired, then use the `vipy.image.Image.flush`` method to clear the cached pixel buffer prior to serialization.  This can always be reloaded after deserialization as long as the source image or URL is acessible.

```python
&gt;&gt;&gt; json = vipy.image.owl().flush().json()
&gt;&gt;&gt; im = vipy.image.Scene.from_json(json)
&gt;&gt;&gt; print(json)
&#39;{&#34;_filename&#34;:&#34;\\/Users\\/myaccount\\/.vipy\\/1920px-Bubo_virginianus_06.jpg&#34;,&#34;_url&#34;:&#34;https:\\/\\/upload.wikimedia.org\\/wikipedia\\/commons\\/thumb\\/2\\/23\\/Bubo_virginianus_06.jpg\\/1920px-Bubo_virginianus_06.jpg&#34;,&#34;_loader&#34;:null,&#34;_array&#34;:null,&#34;_colorspace&#34;:&#34;rgb&#34;,&#34;attributes&#34;:{},&#34;_category&#34;:&#34;Nature&#34;,&#34;_objectlist&#34;:[{&#34;_xmin&#34;:93.33333333333333,&#34;_ymin&#34;:85.33333333333333,&#34;_xmax&#34;:466.6666666666667,&#34;_ymax&#34;:645.3333333333334,&#34;_id&#34;:&#34;a047e21d&#34;,&#34;_label&#34;:&#34;Great Horned Owl&#34;,&#34;_shortlabel&#34;:&#34;Great Horned Owl&#34;}]}&#39;
```

### Export scene to CSV

All annotated images can be exported to a CSV format using object iterators.  Object precision can be changed using `vipy.object.Detection.int`.  CSV headers can be added with `vipy.util.writecsv`.

```python
&gt;&gt;&gt; im = vipy.image.vehicles()
&gt;&gt;&gt; vipy.util.writecsv([(im.filename(), o.category(), o.xmin(), o.ymin(), o.width(), o.height()) for o in im.objects()], &#39;out.csv&#39;)
&gt;&gt;&gt; cat out.csv
/Users/myaccount/.vipy/I-80_Eastshore_Fwy.jpg,car,210.2222222222222,263.2,41.06666666666666,32.622222222222206
/Users/myaccount/.vipy/I-80_Eastshore_Fwy.jpg,car,626.6666666666666,336.0444444444444,77.86666666666667,65.4666666666667
/Users/myaccount/.vipy/I-80_Eastshore_Fwy.jpg,car,140.84444444444443,284.4888888888889,53.066666666666634,53.111111111111086
/Users/myaccount/.vipy/I-80_Eastshore_Fwy.jpg,car,394.17777777777775,396.84444444444443,99.4666666666667,87.37777777777774
```

### Image deduplication

Vipy provides a 128 bit differential perceptual hashing function which is used for near-duplicate detection.  This is useful for identifying pairs of images that differ slightly due to cropping, resizing, watermarkings.  The binary Hamming distance between two perceptual hashes is a similarity metric that can be used to identify duplicates, such that smaller is more likely to be a duplicate.

```python
&gt;&gt;&gt; p = vipy.image.vehicles().perceptualhash()  # hex string
&gt;&gt;&gt; print(p)
&#39;50515541d545f04101a005e801c25945&#39;
&gt;&gt;&gt; q = vipy.image.vehicles().greyscale().perceptualhash()
&gt;&gt;&gt; print(q)
&#39;50515541d545f04101a905e801c27945&#39;
&gt;&gt;&gt; vipy.image.Image.perceptualhash_distance(p, q)  # Hamming distance
3
```

The perceptual hash function also allows for ignoring detected objects in the foreground.  A background hash `vipy.image.Scene.bghash` computes the perceptual hash function using only the regions not contained within the foreground bounding boxes.  This is useful for identifying near duplicate background locations where there may be different foreground objects in the scene between images.  If the `vipy.image.Scene` has no associated foreground objects, then the background hash is equivalent to the perceptual hash above.


### Blur Faces

```python
&gt;&gt;&gt; im = vipy.image.Image(url=&#39;https://upload.wikimedia.org/wikipedia/en/d/d6/Friends_season_one_cast.jpg&#39;)
&gt;&gt;&gt; im.facepixelize().show()
&gt;&gt;&gt; im.faceblur().show()
```
&lt;img src=&#34;https://raw.githubusercontent.com/visym/vipy/master/docs/tutorials/blur_faces_1.jpg&#34; height=&#34;250&#34;&gt;
&lt;img src=&#34;https://raw.githubusercontent.com/visym/vipy/master/docs/tutorials/blur_faces_2.jpg&#34; height=&#34;250&#34;&gt;

This is an experimental feature and may be removed in future releases.



### Data augmentation for training

Data augmentation is the process of introducing synthetic transformations of a given image to introduce additional variation during training.  Data augmentation considers scales, crops, translations, mirrors, rotations or chromatic noise which are applied to a source image to generate one or more augmentations.  All pixel buffers are shared by default for speed, so the clone() method will enforce that pixel buffers are copied when needed.

```python
im = vipy.image.vehicles()
vipy.visualize.montage([[o.crop().fliplr(),                # spatial mirror
                         o.clone().dilate(1.2).crop(),     # zoom out
                         o.clone().translate(4,5).crop(),  # translation 
                         o.clone().translate(-2,9).crop(), # translation 
                         o.clone().dilate(0.8).crop(),     # zoom in 
                         o.crop().blur(sigma=1),           # spatial blur
                         o.crop().additive_noise()]        # chromatic noise 
                         for o in im])                     # for all objects in the scene
```
&lt;img src=&#34;https://raw.githubusercontent.com/visym/vipy/master/docs/tutorials/data_augmentation_for_training.jpg&#34; height=&#34;350&#34;&gt;

These functions have been integrated into a single package `vipy.noise` which implements photometric and geometric perturbations.

```python
im = vipy.image.vehicles()
new_im = vipy.noise.randomcrop(im)
```

The image returned is cloned, and includes the provenance of the noise source in new_im.attributes

### Vipy and Torchvision

All vipy objects can be exported to torch tensors:

```python
im = vipy.image.vehicles().load().tensor(order=&#39;CHW&#39;)
```

with axis permutations to export in channel first order used by pytorch tensors.

### Visualization behind SSH 

Data repositories are often accessed via data storage behind SSH.  You can set up port forwarding to visualize this data, but this may require root access to configure firewall rules.  If you have SSH public key access to your cluster machine, you can do the following:

On a remote machine (e.g. the cluster machine you have accessed via ssh), run:

```python
remote&gt;&gt;&gt; vipy.util.scpsave(vipy.image.owl())
[vipy.util.scpsave]: On a local machine where you have public key ssh access to this remote machine run:
&gt;&gt;&gt; V = vipy.util.scpload(&#39;scp://hostname:/var/folders/sn/6n34qjp513742_5y3lvmhnlw0000gn/T/c4237a25a99b776f.json&#39;)
```

Then, on your local machine (e.g. your laptop), run the command output above:

```python
local&gt;&gt;&gt; print(vipy.util.scpload(&#39;scp://hostname:/var/folders/sn/6n34qjp513742_5y3lvmhnlw0000gn/T/c4237a25a99b776f.json&#39;))
&lt;vipy.image.scene: height=640, width=512, color=rgb, filename=&#34;/tmp/.vipy/1920px-Bubo_virginianus_06.jpg&#34;, url=https://upload.wikimedia.org/wikipedia/commons/thumb/2/23/Bubo_virginianus_06.jpg/1920px-Bubo_virginianus_06.jpg, category=&#34;Nature&#34;, objects=1&gt;
```

The method `vipy.util.scpsave` will save a list of vipy objects to a temporary archive file, such that the URL of each object is prepended with &#34;scp://&#34;.  When calling `vipy.util.scpload` on the local machine, this will fetch the pickle file from the remote machine via scp using the default public key.  Then, when each vipy object is accessed, it will fetch the URL of the media object via scp from the remote machine.  This provides an on-demand fetching of each image from a data storage behind a SSH server without any port forwarding, and uses public key scp.  This allows for visualization of datasets that cannot be copied locally, but can be reduced on the local machine which are then fetched for visualization.


### Visualization behind AWS S3 

Data repositories are often stored with cloud service providers, such as Amazon AWS.  These providers require credentials to access URLs in Simple Storage Service (S3).  Vipy supports accessing AWS S3 URLs with credential restricted access.  Set the following environment variables for the access key and secret access key provided by Amazon AWS.  Follow the links below to get a key:

* **VIPY_AWS_ACCESS_KEY_ID**=&#39;MYKEY&#39;.  This is the [AWS key](https://docs.aws.amazon.com/general/latest/gr/aws-sec-cred-types.html) to download urls of the form &#34;s3://&#34;.  
* **VIPY_AWS_SECRET_ACCESS_KEY**=&#39;MYKEY&#39;.   This is the [AWS secret key](https://docs.aws.amazon.com/general/latest/gr/aws-sec-cred-types.html) to download urls of the form &#34;s3://&#34;.

Then prepend the URL scheme &#34;s3://BUCKET_NAME.s3.amazonaws.com/OBJECT_PATH&#34; when constructing a URL.  Here is an example that we use for [Visym Collector](https://visym.com/collector) to store videos uploaded from around the world:

```python
&gt;&gt;&gt; vipy.image.Image(url=&#34;s3://bucket/path/to/file.mp4&#34;)
```

Finally, if the credentials you provide are authorized to access this bucket and object, then this object will be downloaded on-demand when the pixels are needed.  This provides a convenient method of on-demand downloading and caching of large datasets.


## Videos

### Load from YouTube

```python
v = vipy.video.Video(url=&#39;https://youtu.be/kpBCzzzX6zA&#39;).download()
```

### Inspect the FFMPEG command line

```python
print(vipy.video.Video(filename=&#39;/path/to/in.mp4&#39;).mindim(512).framerate(2).commandline())
```
```
&#39;ffmpeg -i /path/to/in.mp4 -filter_complex &#34;[0]fps=fps=2.0:round=up[s0];[s0]scale=-1:512[s1]&#34; -map &#34;[s1]&#34; dummyfile&#39;
```

### Export frames as vipy images

```python
frames = [im for im in v.framerate(1)]   # 1 Hz export
```

### Export frames as numpy array

```python
frames = v.framerate(0.1).numpy()   # 0.1 Hz export
```

### Generate WEBP animations

```python
v = vipy.video.RandomScene().clip(0,30).webp()
```

### Find all videos in directory

```python
dataset = vipy.dataset.Dataset.from_directory(&#39;/path/to/dir&#39;, filetype=&#39;mp4&#39;)
```

### Import RTSP camera streams

```python
v = vipy.video.Video(url=&#39;rtsp://user:password@10.0.1.19/live0&#39;)
```

An RTSP camera can be modified to change the framerate or the resolution then the video can be streamed live or iterated as with other videos&#39;

```python
vipy.video.Video(url=&#39;rtsp://user:password@10.0.1.19/live0&#39;, framerate=5).mindim(256).show()
```

To grab a single frame:

```python
im = vipy.video.Video(url=&#39;rtsp://user:password@10.0.1.19/live0&#39;).frame()
```

To iterate:

```python
for im in vipy.video.Video(url=&#39;rtsp://user:password@10.0.1.19/live0&#39;):
    print(im)
```

To save to a video file:

```python
vipy.video.Video(url=&#39;rtsp://user:password@10.0.1.19/live0&#39;).save(&#39;/path/to/video.mp4&#39;)
```

You will need to interrupt the saving when you&#39;re done and the video up to that point will be available in the video file

### Split a video into activity clips

```python
clips = vipy.video.RandomScene().activityclip()
```

Each clip is a separate video with a single activity, such that the video is clipped at the temporal extent of this activity

### Split a video into track clips

```python
clips = vipy.video.RandomScene().trackclip()
```

Each clip is a separate video with a single track, such that the video is clipped at the temporal extent of this track

### Create quicklooks for fast video watching

```python
vipy.video.RandomScene().quicklook(),show()
```

A quicklook is a montage constructed by nine frames sampled from the video.  This is a convenient way to visualize a video



### Export to JSON

```python
vipy.video.RandomScene().flush().json()
```

## Datasets

### Load a dataset from the registry

```python
vipy.dataset.registry(&#39;mnist&#39;)
```

The registry is the common entry point for loading collections of annotated visual data.  The datasets are downloaded and imported when requested.  Datasets are constructed as a large number of python objects, which can lead to slow garbage collection.  During registry import, the garbage collector is frozen by default, so that the loaded dataset is disabled for reference cycle counting.


### Shuffling

Shuffling a dataset uniformly at random can lead to inefficiencies due to random data access.  The `vipy.dataset.Datase.shuffle` method support uniform random shuffling, but it will attempt to use a more efficient streaming shuffler to allow for iterative access rather than random access.  This streaming shuffler will shuffle the underlying dataset (or dataset shards) rather than shuffling the dataset index.  This leads to better data locality in data access and more efficient iterative access for large datasets.


### Download a dataset with URLs 

```python
with vipy.parallel.multiprocessing(4):
    vipy.dataset.registry(&#39;kinetics&#39;).map(vipy.video.Transform.downloader(outdir=&#39;/tmp&#39;)) 
```

This will download the youtube videos from the kinetics dataset with four parallel processes. Videos will be stored in the requested outdir.


### Create a dataset from images

```python
D = vipy.dataset.Dataset.from_directory(&#39;/path/to/dir&#39;, filetype=&#39;images&#39;)
```

### Determine the set of classes

```python
trainset = vipy.dataset.registry(&#39;cifar10&#39;)
print(trainset.set(lambda im: im.category()))
```

### Count objects in a scene 

```python
im = vipy.dataset.registry(&#39;coco_2014&#39;).takeone()
vipy.dataset.Dataset(im.objects()).frequency(lambda o: o.category())
```
Return a dictionary of the counts of object categories from a random coco-2014 scene.


### Take a subset

```python
trainset = vipy.dataset.registry(&#39;cifar10&#39;)
print(trainset.take(1024))
```

### Compute inverse frequency weights

```python
vipy.dataset.registry(&#39;cifar10&#39;).inverse_frequency(lambda im: im.category())
```

These weights can be used for weighting a loss function for imbalanced classes.

### Iterate over minibatches

```python
for b in vipy.dataset.registry(&#39;cifar10&#39;).minibatch(128):
     print(b)
```

### Iterate over minibatches with parallel preprocessing

```python
with vipy.globals.multiprocessing(4):
    for b in vipy.dataset.registry(&#39;cifar10&#39;).minibatch(128, loader=vipy.image.Transform.to_tensor(shape=(16,16), gain=1/255)):
        print(b)  # these batches have been preprocessed to shape (16,16) and in the range [0,1]
```

### Combine datasets as a union

```python
vipy.dataset.registry((&#39;mnist&#39;, &#39;cifar10&#39;, &#39;caltech101&#39;))
```

A dataset untion will iterate over the component datasets

# Contact

Visym Labs &lt;&lt;info@visym.com&gt;&gt;

&#34;&#34;&#34;

# Import key subpackages
import vipy.show  # matplotlib first
import vipy.activity
import vipy.annotation
#import vipy.calibration
import vipy.dataset
import vipy.downloader
import vipy.geometry
import vipy.image
#import vipy.linalg
import vipy.math
import vipy.object
import vipy.util
import vipy.version
import vipy.video
import vipy.visualize


# Top level functions
# &gt;&gt;&gt; vipy.save(...)
from vipy.util import save, load, env

__version__ = vipy.version.VERSION</code></pre>
</details>
</section>
<section>
<h2 class="section-title" id="header-submodules">Sub-modules</h2>
<dl>
<dt><code class="name"><a title="vipy.activity" href="activity.html">vipy.activity</a></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt><code class="name"><a title="vipy.annotation" href="annotation.html">vipy.annotation</a></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt><code class="name"><a title="vipy.batch" href="batch.html">vipy.batch</a></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt><code class="name"><a title="vipy.calibration" href="calibration.html">vipy.calibration</a></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt><code class="name"><a title="vipy.data" href="data/index.html">vipy.data</a></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt><code class="name"><a title="vipy.dataset" href="dataset.html">vipy.dataset</a></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt><code class="name"><a title="vipy.downloader" href="downloader.html">vipy.downloader</a></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt><code class="name"><a title="vipy.flow" href="flow.html">vipy.flow</a></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt><code class="name"><a title="vipy.geometry" href="geometry.html">vipy.geometry</a></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt><code class="name"><a title="vipy.globals" href="globals.html">vipy.globals</a></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt><code class="name"><a title="vipy.gui" href="gui/index.html">vipy.gui</a></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt><code class="name"><a title="vipy.image" href="image.html">vipy.image</a></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt><code class="name"><a title="vipy.linalg" href="linalg.html">vipy.linalg</a></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt><code class="name"><a title="vipy.math" href="math.html">vipy.math</a></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt><code class="name"><a title="vipy.metrics" href="metrics.html">vipy.metrics</a></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt><code class="name"><a title="vipy.noise" href="noise.html">vipy.noise</a></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt><code class="name"><a title="vipy.object" href="object.html">vipy.object</a></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt><code class="name"><a title="vipy.parallel" href="parallel.html">vipy.parallel</a></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt><code class="name"><a title="vipy.pyramid" href="pyramid.html">vipy.pyramid</a></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt><code class="name"><a title="vipy.show" href="show.html">vipy.show</a></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt><code class="name"><a title="vipy.torch" href="torch.html">vipy.torch</a></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt><code class="name"><a title="vipy.util" href="util.html">vipy.util</a></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt><code class="name"><a title="vipy.version" href="version.html">vipy.version</a></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt><code class="name"><a title="vipy.video" href="video.html">vipy.video</a></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt><code class="name"><a title="vipy.visualize" href="visualize.html">vipy.visualize</a></code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
</article>
<nav id="sidebar">
<header>
<a class="homelink" rel="home" title="VIPY" href="https://github.com/visym/vipy/">
<img src="https://www.visym.com/labs/images/visym_logo_black_notext.png" alt="" width="60">
</a>
<h1 style="font-size:200%;"><b>VIPY:</b> Visual Dataset Transformation</h1>
</header>
<form>
<input id="lunr-search" name="q" placeholder="ðŸ”Ž Search ..." aria-label="Search"
disabled minlength="2">
</form>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/tingle/0.15.3/tingle.min.css" integrity="sha512-j1u8eUJ4f23xPPxwOrLUPQaCD2dwzNqqmDDcWS4deWsMv2ohLqmXXuP3hU7g8TyzbMSakP/mMqoNBYWj8AEIFg==" crossorigin>
<script src="https://cdnjs.cloudflare.com/ajax/libs/tingle/0.15.3/tingle.min.js" integrity="sha512-plGUER9JkeEWPPqQBE4sdLqBoQug5Ap+BCGMc7bJ8BXkm+VVj6QzkpBz5Yv2yPkkq+cqg9IpkBaGCas6uDbW8g==" crossorigin></script>
<style>
.modal-dialog iframe {
width: 100vw;
height: calc(100vh - 80px);
}
@media screen and (min-width: 700px) {
.modal-dialog iframe {
width: 70vw;
height: 80vh;
}
}
.modal-dialog .tingle-modal-box {width: auto;}
.modal-dialog .tingle-modal-box__content {padding: 0;}
</style>
<script>
const input = document.getElementById('lunr-search');
input.disabled = false;
input.form.addEventListener('submit', (ev) => {
ev.preventDefault();
const url = new URL(window.location);
url.searchParams.set('q', input.value);
history.replaceState({}, null, url.toString());
search(input.value);
});
const query = new URL(window.location).searchParams.get('q');
if (query)
search(query);
function search(query) {
const url = './doc-search.html#' + encodeURIComponent(query);
new tingle.modal({
cssClass: ['modal-dialog'],
onClose: () => {
const url = new URL(window.location);
url.searchParams.delete('q');
history.replaceState({}, null, url.toString());
setTimeout(() => input.focus(), 100);
}
}).setContent('<iframe src="' + url + '"></iframe>').open();
}
</script>
<h1>Index</h1>
<div class="toc">
<ul>
<li><a href="#design-goals">Design Goals</a></li>
<li><a href="#getting-started">Getting started</a><ul>
<li><a href="#import">Import</a></li>
<li><a href="#export">Export</a></li>
<li><a href="#environment-variables">Environment variables</a></li>
<li><a href="#versioning">Versioning</a></li>
<li><a href="#parallelization">Parallelization</a></li>
</ul>
</li>
<li><a href="#tutorials">Tutorials</a><ul>
<li><a href="#images">Images</a><ul>
<li><a href="#load-an-image">Load an image</a></li>
<li><a href="#print-an-image-representation">Print an image representation</a></li>
<li><a href="#transform-an-image">Transform an image</a></li>
<li><a href="#export-as-numpy-array">Export as numpy array</a></li>
<li><a href="#display-an-image">Display an image</a></li>
<li><a href="#annotate-an-image">Annotate an image</a></li>
<li><a href="#save-an-image">Save an image</a></li>
<li><a href="#convert-image-colorspace">Convert image colorspace</a></li>
<li><a href="#rescale-image">Rescale image</a></li>
<li><a href="#visualize-scenes">Visualize scenes</a></li>
<li><a href="#get-all-of-the-object-categories">Get all of the object categories</a></li>
<li><a href="#get-all-of-the-object-boxes">Get all of the object boxes</a></li>
<li><a href="#crop-and-resize-annotated-objects-in-a-scene">Crop and resize annotated objects in a scene</a></li>
<li><a href="#find-all-images-in-directory">Find all images in directory</a></li>
<li><a href="#export-scene-to-json">Export scene to JSON</a></li>
<li><a href="#export-scene-to-csv">Export scene to CSV</a></li>
<li><a href="#image-deduplication">Image deduplication</a></li>
<li><a href="#blur-faces">Blur Faces</a></li>
<li><a href="#data-augmentation-for-training">Data augmentation for training</a></li>
<li><a href="#vipy-and-torchvision">Vipy and Torchvision</a></li>
<li><a href="#visualization-behind-ssh">Visualization behind SSH</a></li>
<li><a href="#visualization-behind-aws-s3">Visualization behind AWS S3</a></li>
</ul>
</li>
<li><a href="#videos">Videos</a><ul>
<li><a href="#load-from-youtube">Load from YouTube</a></li>
<li><a href="#inspect-the-ffmpeg-command-line">Inspect the FFMPEG command line</a></li>
<li><a href="#export-frames-as-vipy-images">Export frames as vipy images</a></li>
<li><a href="#export-frames-as-numpy-array">Export frames as numpy array</a></li>
<li><a href="#generate-webp-animations">Generate WEBP animations</a></li>
<li><a href="#find-all-videos-in-directory">Find all videos in directory</a></li>
<li><a href="#import-rtsp-camera-streams">Import RTSP camera streams</a></li>
<li><a href="#split-a-video-into-activity-clips">Split a video into activity clips</a></li>
<li><a href="#split-a-video-into-track-clips">Split a video into track clips</a></li>
<li><a href="#create-quicklooks-for-fast-video-watching">Create quicklooks for fast video watching</a></li>
<li><a href="#export-to-json">Export to JSON</a></li>
</ul>
</li>
<li><a href="#datasets">Datasets</a><ul>
<li><a href="#load-a-dataset-from-the-registry">Load a dataset from the registry</a></li>
<li><a href="#shuffling">Shuffling</a></li>
<li><a href="#download-a-dataset-with-urls">Download a dataset with URLs</a></li>
<li><a href="#create-a-dataset-from-images">Create a dataset from images</a></li>
<li><a href="#determine-the-set-of-classes">Determine the set of classes</a></li>
<li><a href="#count-objects-in-a-scene">Count objects in a scene</a></li>
<li><a href="#take-a-subset">Take a subset</a></li>
<li><a href="#compute-inverse-frequency-weights">Compute inverse frequency weights</a></li>
<li><a href="#iterate-over-minibatches">Iterate over minibatches</a></li>
<li><a href="#iterate-over-minibatches-with-parallel-preprocessing">Iterate over minibatches with parallel preprocessing</a></li>
<li><a href="#combine-datasets-as-a-union">Combine datasets as a union</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#contact">Contact</a></li>
</ul>
</div>
<ul id="index">
<li><h3><a href="#header-submodules">Sub-modules</a></h3>
<ul>
<li><code><a title="vipy.activity" href="activity.html">vipy.activity</a></code></li>
<li><code><a title="vipy.annotation" href="annotation.html">vipy.annotation</a></code></li>
<li><code><a title="vipy.batch" href="batch.html">vipy.batch</a></code></li>
<li><code><a title="vipy.calibration" href="calibration.html">vipy.calibration</a></code></li>
<li><code><a title="vipy.data" href="data/index.html">vipy.data</a></code></li>
<li><code><a title="vipy.dataset" href="dataset.html">vipy.dataset</a></code></li>
<li><code><a title="vipy.downloader" href="downloader.html">vipy.downloader</a></code></li>
<li><code><a title="vipy.flow" href="flow.html">vipy.flow</a></code></li>
<li><code><a title="vipy.geometry" href="geometry.html">vipy.geometry</a></code></li>
<li><code><a title="vipy.globals" href="globals.html">vipy.globals</a></code></li>
<li><code><a title="vipy.gui" href="gui/index.html">vipy.gui</a></code></li>
<li><code><a title="vipy.image" href="image.html">vipy.image</a></code></li>
<li><code><a title="vipy.linalg" href="linalg.html">vipy.linalg</a></code></li>
<li><code><a title="vipy.math" href="math.html">vipy.math</a></code></li>
<li><code><a title="vipy.metrics" href="metrics.html">vipy.metrics</a></code></li>
<li><code><a title="vipy.noise" href="noise.html">vipy.noise</a></code></li>
<li><code><a title="vipy.object" href="object.html">vipy.object</a></code></li>
<li><code><a title="vipy.parallel" href="parallel.html">vipy.parallel</a></code></li>
<li><code><a title="vipy.pyramid" href="pyramid.html">vipy.pyramid</a></code></li>
<li><code><a title="vipy.show" href="show.html">vipy.show</a></code></li>
<li><code><a title="vipy.torch" href="torch.html">vipy.torch</a></code></li>
<li><code><a title="vipy.util" href="util.html">vipy.util</a></code></li>
<li><code><a title="vipy.version" href="version.html">vipy.version</a></code></li>
<li><code><a title="vipy.video" href="video.html">vipy.video</a></code></li>
<li><code><a title="vipy.visualize" href="visualize.html">vipy.visualize</a></code></li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.11.6</a>.</p>
</footer>
</body>
</html>
