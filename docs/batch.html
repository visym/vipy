<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.11.6" />
<meta name="google-site-verification" content="aB8LkQegj94_TJPdrcJm2ldIRWyXY82Jp24Gtkdgyn0" />
<title>vipy.batch API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}#lunr-search{width:100%;font-size:1em;padding:6px 9px 5px 9px;border:1px solid silver}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>vipy.batch</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/visym/vipy/blob/f10b3f6aec0ef011e86ee11667579388379a8ac3/vipy/batch.py#L1-L225" class="git-link">Browse git</a>
</summary>
<pre><code class="python">import os
import sys
from vipy.util import try_import, islist, tolist, tempdir, remkdir, chunklistbysize, listpkl, filetail, filebase, tempdir
from itertools import repeat
import numpy as np
import vipy.globals
from vipy.globals import log


try_import(&#39;dill&#39;,&#39;dill&#39;); import dill
dill.extend(False)  # https://github.com/uqfoundation/dill/issues/383
try_import(&#39;dask&#39;, &#39;dask distributed&#39;)
from dask.distributed import as_completed, wait
import dask
import dask.config
from dask.distributed import Client
from dask.distributed import as_completed, wait
from dask.distributed import get_worker         
dill.extend(True)  # https://github.com/uqfoundation/dill/issues/383


class Batch():
    &#34;&#34;&#34;vipy.batch.Batch class

    This class provides a representation of a set of vipy objects.  All of the object types must be the same.  If so, then an operation on the batch is performed on each of the elements in the batch in parallel.

    Examples:

    &gt;&gt;&gt; b = vipy.batch.Batch([Image(filename=&#39;img_%06d.png&#39; % k) for k in range(0,100)])
    &gt;&gt;&gt; b.map(lambda im: im.bgr())  
    &gt;&gt;&gt; b.map(lambda im: np.sum(im.array())) 
    &gt;&gt;&gt; b.map(lambda im, f: im.saveas(f), args=[&#39;out%d.jpg&#39; % k for k in range(0,100)])
    
    &gt;&gt;&gt; v = vipy.video.RandomSceneActivity()
    &gt;&gt;&gt; b = vipy.batch.Batch(v, n_processes=16)
    &gt;&gt;&gt; b.map(lambda v,k: v[k], args=[(k,) for k in range(0, len(v))])  # paralle interpolation

    &gt;&gt;&gt; d = vipy.data.kinetics.Kinetics700(&#39;/path/to/kinetics&#39;).download().trainset()
    &gt;&gt;&gt; b = vipy.batch.Batch(d, n_processes=32)
    &gt;&gt;&gt; b.map(lambda v: v.download().save())  # will download and clip dataset in parallel

    &gt;&gt;&gt; b.result()  # retrieve results after a sequence of map or filter chains
    &gt;&gt;&gt; list(b)     # equivalent to b.result()

    Args:
        strict: [bool] if distributed processing fails, return None for that element and print the exception rather than raise
        as_completed: [bool] Return the objects to the scheduler as they complete, this can introduce instabilities for large complex objects, use with caution
        ordered: [bool]: If True, then preserve the order of objects in objlist in distributed processing

    .. notes:: `vipy.dataset.Dataset.map` supports batch processing and is the preferred method for paralle processing of a dataset

    &#34;&#34;&#34;    
             
    def __init__(self, objlist, strict=False, as_completed=False, warnme=False, minscatter=None, ordered=False):
        &#34;&#34;&#34;Create a batch of homogeneous vipy.image objects from an iterable that can be operated on with a single parallel function call
        &#34;&#34;&#34;
        assert isinstance(objlist, list), &#34;Input must be a list&#34;
        self._batchtype = type(objlist[0]) if len(objlist)&gt;0 else type(None)
        assert all([isinstance(im, self._batchtype) for im in objlist]), &#34;Invalid input - Must be homogeneous list of the same type&#34;                

        # Move this into map and disable using localmap
        if vipy.globals.dask() is None and warnme:
            log.info(&#39;[vipy.batch.Batch]: vipy.batch.Batch() is not set to use parallelism.  This is set using:\n    &gt;&gt;&gt; with vipy.globals.parallel(n) for multi-processing with n processes\n    &gt;&gt;&gt; vipy.globals.parallel(pct=0.8) for multiprocessing that uses a percentage of the current system resources\n    &gt;&gt;&gt; vipy.globals.dask(address=&#34;SCHEDULER:PORT&#34;) which connects to a Dask distributed scheduler.\n    &gt;&gt;&gt; vipy.globals.noparallel() to completely disable all parallelism.&#39;)

        self._strict = strict
        self._as_completed = as_completed  # this may introduce instabilities for large complex objects, use with caution
        self._minscatter = minscatter
        self._ordered = ordered
        self._objlist = [(k,o) for (k,o) in enumerate(objlist)] if ordered else objlist

    def __enter__(self):
        return self

    def __exit__(self, type, value, traceback):
        return self
    
    def __len__(self):
        return len(self._objlist)

    def __repr__(self):        
        return str(&#39;&lt;vipy.batch: type=%s, len=%d%s&gt;&#39; % (str(self._batchtype), len(self), (&#39;, client=%s&#39; % str(vipy.globals.dask())) if vipy.globals.dask() is not None else &#39;&#39;))

    def _client(self):
        return vipy.globals.dask()._client if vipy.globals.dask() is not None else None
    
    def _batch_wait(self, futures):
        try:
            results = []            
            for (k, batch) in enumerate(as_completed(futures, with_results=True, raise_errors=False).batches()):
                for (future, result) in batch:
                    if future.status != &#39;error&#39;:
                        results.append(result)  # not order preserving, will restore order in result()
                    else:
                        if self._strict:
                            typ, exc, tb = result
                            raise exc.with_traceback(tb)
                        else:
                            log.warning(&#39;[vipy.batch]: future %s failed with error &#34;%s&#34; - SKIPPING&#39; % (str(future), str(result)))
                        results.append(None)
                    del future, result  # distributed memory cleanup

                # Distributed memory cleanup
                del batch
  
            return results

        except KeyboardInterrupt:
            # warnings.warn(&#39;[vipy.batch]: batch cannot be restarted after killing with ctrl-c - You must create a new Batch()&#39;)
            #vipy.globals.dask().shutdown()
            #self._client = None
            return results
        except:
            # warnings.warn(&#39;[vipy.batch]: batch cannot be restarted after exception - Recreate Batch()&#39;)                
            raise
    
    def _wait(self, futures):
        assert islist(futures) and all([hasattr(f, &#39;result&#39;) for f in futures])
        if self._as_completed:
            return self._batch_wait(futures)        
        try:
            results = []            
            wait(futures)
            for f in futures:  
                try:
                    results.append(f.result()) 
                except:
                    if self._strict:
                        raise
                    try:
                        log.error(&#39;[vipy.batch]: future %s failed with error &#34;%s&#34; for batch &#34;%s&#34;&#39; % (str(f), str(f.exception()), str(self)))
                    except:
                        log.error(&#39;[vipy.batch]: future failed&#39;)
                    results.append(None)
            return results

        except KeyboardInterrupt:
            # warnings.warn(&#39;[vipy.batch]: batch cannot be restarted after killing with ctrl-c - You must create a new Batch()&#39;)
            #vipy.globals.dask().shutdown()
            #self._client = None
            return None  
        except:
            # warnings.warn(&#39;[vipy.batch]: batch cannot be restarted after exception - Recreate Batch()&#39;)                
            raise

    def result(self):
        &#34;&#34;&#34;Return the result of the batch processing, ordered&#34;&#34;&#34;
        if self._ordered:
            objlist = {int(v[0]):v[1] for v in self._objlist if v is not None}
            return [objlist[k] if k in objlist else None for k in range(len(self._objlist))]  # restore order
        else:
            return self._objlist

    def __iter__(self):
        for x in self.result():
            yield x
            
    def map(self, f_lambda, args=None):
        &#34;&#34;&#34;Run the lambda function on each of the elements of the batch and return the batch object.
        
        &gt;&gt;&gt; iml = [vipy.image.RandomScene(512,512) for k in range(0,1000)]   
        &gt;&gt;&gt; imb = vipy.image.Batch(iml) 
        &gt;&gt;&gt; imb.map(lambda im: im.rgb())  

        The lambda function f_lambda should not include closures.  If it does, construct the lambda with default parameter capture:

        &gt;&gt;&gt; f = lambda x, prm1=42: x+prm1

        instead of:

        &gt;&gt;&gt; prm1 = 42
        &gt;&gt;&gt; f = lambda x: x+prm1

        &#34;&#34;&#34;
        c = self._client()

        if c is None:
            f_lambda_ordered = (lambda x,f=f_lambda: (x[0], f(x[1]))) if self._ordered else f_lambda
            self._objlist = [f_lambda_ordered(o) for o in self._objlist]  # no parallelism
        else:
            f_lambda_ordered = (lambda x,f=f_lambda: (x[0], f(x[1]))) if self._ordered else f_lambda
            objlist = c.scatter(self._objlist) if (self._minscatter is not None and len(self._objlist) &gt;= self._minscatter) else self._objlist
            self._objlist = self._wait(c.map(f_lambda_ordered, objlist))
        return self

    def filter(self, f_lambda):
        &#34;&#34;&#34;Run the lambda function on each of the elements of the batch and filter based on the provided lambda keeping those elements that return true 
        &#34;&#34;&#34;
        c = self._client()
        f_lambda_ordered = (lambda x,f=f_lambda: (x[0], f(x[1]))) if self._ordered else f_lambda

        if c is None:
            self._objlist = [o for o in self._objlist if f_lambda_ordered(o)[1]]  # no parallelism
        else:
            objlist = self._objlist  # original list
            is_filtered = self._wait(c.map(f_lambda_ordered, c.scatter(self._objlist)))  # distributed filter (replaces self._objlist)
            self._objlist = [obj for (f, obj) in zip(is_filtered, objlist) if f[1] is True]  # keep only elements that filter true
        return self
        
    def scattermap(self, f_lambda, obj):
        &#34;&#34;&#34;Scatter obj to all workers, and apply lambda function f(obj, im) to each element in batch
        
           Usage: 
         
           &gt;&gt;&gt; Batch(mylist, ngpu=8).scattermap(lambda net, im: net(im), net).result()
        
           This will scatter the large object net to all workers, and pin it to a specific GPU.  Within the net object, you can call 
           vipy.global.gpuindex() to retrieve your assigned GPU index, which can be used by torch.cuda.device().  Then, the net
           object processes each element in the batch using net according to the lambda, and returns the results.  This function 
           includes ngpu processes, and assumes there are ngpu available on the target machine.  Each net is replicated in a different
           process, so it is the callers responsibility for getting vipy.global.gpuindex() from within the process and setting 
           net to take advantage of this GPU rather than using the default cuda:0.  

        &#34;&#34;&#34;
        c = self._client()
        f_lambda_ordered = (lambda net,x,f=f_lambda: (x[0], f(net,x[1]))) if self._ordered else (lambda net,x,f=f_lambda: f(net, x))

        if c is None:
            self._objlist = [f_lambda_ordered(obj, o) for o in self._objlist]  # no parallelism
        else:
            objdist = c.scatter(obj, broadcast=True)        
            objlist = c.scatter(self._objlist) if (self._minscatter is not None and len(self._objlist) &gt;= self._minscatter) else self._objlist
            self._objlist = self._wait([c.submit(f_lambda_ordered, objdist, im) for im in objlist])
        return self</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="vipy.batch.Batch"><code class="flex name class">
<span>class <span class="ident">Batch</span></span>
<span>(</span><span>objlist, strict=False, as_completed=False, warnme=False, minscatter=None, ordered=False)</span>
</code></dt>
<dd>
<div class="desc"><p>vipy.batch.Batch class</p>
<p>This class provides a representation of a set of vipy objects.
All of the object types must be the same.
If so, then an operation on the batch is performed on each of the elements in the batch in parallel.</p>
<p>Examples:</p>
<pre><code class="language-python-repl">&gt;&gt;&gt; b = vipy.batch.Batch([Image(filename='img_%06d.png' % k) for k in range(0,100)])
&gt;&gt;&gt; b.map(lambda im: im.bgr())  
&gt;&gt;&gt; b.map(lambda im: np.sum(im.array())) 
&gt;&gt;&gt; b.map(lambda im, f: im.saveas(f), args=['out%d.jpg' % k for k in range(0,100)])
</code></pre>
<pre><code class="language-python-repl">&gt;&gt;&gt; v = vipy.video.RandomSceneActivity()
&gt;&gt;&gt; b = vipy.batch.Batch(v, n_processes=16)
&gt;&gt;&gt; b.map(lambda v,k: v[k], args=[(k,) for k in range(0, len(v))])  # paralle interpolation
</code></pre>
<pre><code class="language-python-repl">&gt;&gt;&gt; d = vipy.data.kinetics.Kinetics700('/path/to/kinetics').download().trainset()
&gt;&gt;&gt; b = vipy.batch.Batch(d, n_processes=32)
&gt;&gt;&gt; b.map(lambda v: v.download().save())  # will download and clip dataset in parallel
</code></pre>
<pre><code class="language-python-repl">&gt;&gt;&gt; b.result()  # retrieve results after a sequence of map or filter chains
&gt;&gt;&gt; list(b)     # equivalent to b.result()
</code></pre>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>strict</code></strong></dt>
<dd>[bool] if distributed processing fails, return None for that element and print the exception rather than raise</dd>
<dt><strong><code>as_completed</code></strong></dt>
<dd>[bool] Return the objects to the scheduler as they complete, this can introduce instabilities for large complex objects, use with caution</dd>
<dt><strong><code>ordered</code></strong></dt>
<dd>[bool]: If True, then preserve the order of objects in objlist in distributed processing</dd>
</dl>
<div class="admonition notes">
<p class="admonition-title">Notes:&ensp;<code><a title="vipy.dataset.Dataset.map" href="dataset.html#vipy.dataset.Dataset.map">Dataset.map()</a></code> supports batch processing and is the preferred method for paralle processing of a dataset</p>
</div>
<p>Create a batch of homogeneous vipy.image objects from an iterable that can be operated on with a single parallel function call</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/visym/vipy/blob/f10b3f6aec0ef011e86ee11667579388379a8ac3/vipy/batch.py#L22-L223" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class Batch():
    &#34;&#34;&#34;vipy.batch.Batch class

    This class provides a representation of a set of vipy objects.  All of the object types must be the same.  If so, then an operation on the batch is performed on each of the elements in the batch in parallel.

    Examples:

    &gt;&gt;&gt; b = vipy.batch.Batch([Image(filename=&#39;img_%06d.png&#39; % k) for k in range(0,100)])
    &gt;&gt;&gt; b.map(lambda im: im.bgr())  
    &gt;&gt;&gt; b.map(lambda im: np.sum(im.array())) 
    &gt;&gt;&gt; b.map(lambda im, f: im.saveas(f), args=[&#39;out%d.jpg&#39; % k for k in range(0,100)])
    
    &gt;&gt;&gt; v = vipy.video.RandomSceneActivity()
    &gt;&gt;&gt; b = vipy.batch.Batch(v, n_processes=16)
    &gt;&gt;&gt; b.map(lambda v,k: v[k], args=[(k,) for k in range(0, len(v))])  # paralle interpolation

    &gt;&gt;&gt; d = vipy.data.kinetics.Kinetics700(&#39;/path/to/kinetics&#39;).download().trainset()
    &gt;&gt;&gt; b = vipy.batch.Batch(d, n_processes=32)
    &gt;&gt;&gt; b.map(lambda v: v.download().save())  # will download and clip dataset in parallel

    &gt;&gt;&gt; b.result()  # retrieve results after a sequence of map or filter chains
    &gt;&gt;&gt; list(b)     # equivalent to b.result()

    Args:
        strict: [bool] if distributed processing fails, return None for that element and print the exception rather than raise
        as_completed: [bool] Return the objects to the scheduler as they complete, this can introduce instabilities for large complex objects, use with caution
        ordered: [bool]: If True, then preserve the order of objects in objlist in distributed processing

    .. notes:: `vipy.dataset.Dataset.map` supports batch processing and is the preferred method for paralle processing of a dataset

    &#34;&#34;&#34;    
             
    def __init__(self, objlist, strict=False, as_completed=False, warnme=False, minscatter=None, ordered=False):
        &#34;&#34;&#34;Create a batch of homogeneous vipy.image objects from an iterable that can be operated on with a single parallel function call
        &#34;&#34;&#34;
        assert isinstance(objlist, list), &#34;Input must be a list&#34;
        self._batchtype = type(objlist[0]) if len(objlist)&gt;0 else type(None)
        assert all([isinstance(im, self._batchtype) for im in objlist]), &#34;Invalid input - Must be homogeneous list of the same type&#34;                

        # Move this into map and disable using localmap
        if vipy.globals.dask() is None and warnme:
            log.info(&#39;[vipy.batch.Batch]: vipy.batch.Batch() is not set to use parallelism.  This is set using:\n    &gt;&gt;&gt; with vipy.globals.parallel(n) for multi-processing with n processes\n    &gt;&gt;&gt; vipy.globals.parallel(pct=0.8) for multiprocessing that uses a percentage of the current system resources\n    &gt;&gt;&gt; vipy.globals.dask(address=&#34;SCHEDULER:PORT&#34;) which connects to a Dask distributed scheduler.\n    &gt;&gt;&gt; vipy.globals.noparallel() to completely disable all parallelism.&#39;)

        self._strict = strict
        self._as_completed = as_completed  # this may introduce instabilities for large complex objects, use with caution
        self._minscatter = minscatter
        self._ordered = ordered
        self._objlist = [(k,o) for (k,o) in enumerate(objlist)] if ordered else objlist

    def __enter__(self):
        return self

    def __exit__(self, type, value, traceback):
        return self
    
    def __len__(self):
        return len(self._objlist)

    def __repr__(self):        
        return str(&#39;&lt;vipy.batch: type=%s, len=%d%s&gt;&#39; % (str(self._batchtype), len(self), (&#39;, client=%s&#39; % str(vipy.globals.dask())) if vipy.globals.dask() is not None else &#39;&#39;))

    def _client(self):
        return vipy.globals.dask()._client if vipy.globals.dask() is not None else None
    
    def _batch_wait(self, futures):
        try:
            results = []            
            for (k, batch) in enumerate(as_completed(futures, with_results=True, raise_errors=False).batches()):
                for (future, result) in batch:
                    if future.status != &#39;error&#39;:
                        results.append(result)  # not order preserving, will restore order in result()
                    else:
                        if self._strict:
                            typ, exc, tb = result
                            raise exc.with_traceback(tb)
                        else:
                            log.warning(&#39;[vipy.batch]: future %s failed with error &#34;%s&#34; - SKIPPING&#39; % (str(future), str(result)))
                        results.append(None)
                    del future, result  # distributed memory cleanup

                # Distributed memory cleanup
                del batch
  
            return results

        except KeyboardInterrupt:
            # warnings.warn(&#39;[vipy.batch]: batch cannot be restarted after killing with ctrl-c - You must create a new Batch()&#39;)
            #vipy.globals.dask().shutdown()
            #self._client = None
            return results
        except:
            # warnings.warn(&#39;[vipy.batch]: batch cannot be restarted after exception - Recreate Batch()&#39;)                
            raise
    
    def _wait(self, futures):
        assert islist(futures) and all([hasattr(f, &#39;result&#39;) for f in futures])
        if self._as_completed:
            return self._batch_wait(futures)        
        try:
            results = []            
            wait(futures)
            for f in futures:  
                try:
                    results.append(f.result()) 
                except:
                    if self._strict:
                        raise
                    try:
                        log.error(&#39;[vipy.batch]: future %s failed with error &#34;%s&#34; for batch &#34;%s&#34;&#39; % (str(f), str(f.exception()), str(self)))
                    except:
                        log.error(&#39;[vipy.batch]: future failed&#39;)
                    results.append(None)
            return results

        except KeyboardInterrupt:
            # warnings.warn(&#39;[vipy.batch]: batch cannot be restarted after killing with ctrl-c - You must create a new Batch()&#39;)
            #vipy.globals.dask().shutdown()
            #self._client = None
            return None  
        except:
            # warnings.warn(&#39;[vipy.batch]: batch cannot be restarted after exception - Recreate Batch()&#39;)                
            raise

    def result(self):
        &#34;&#34;&#34;Return the result of the batch processing, ordered&#34;&#34;&#34;
        if self._ordered:
            objlist = {int(v[0]):v[1] for v in self._objlist if v is not None}
            return [objlist[k] if k in objlist else None for k in range(len(self._objlist))]  # restore order
        else:
            return self._objlist

    def __iter__(self):
        for x in self.result():
            yield x
            
    def map(self, f_lambda, args=None):
        &#34;&#34;&#34;Run the lambda function on each of the elements of the batch and return the batch object.
        
        &gt;&gt;&gt; iml = [vipy.image.RandomScene(512,512) for k in range(0,1000)]   
        &gt;&gt;&gt; imb = vipy.image.Batch(iml) 
        &gt;&gt;&gt; imb.map(lambda im: im.rgb())  

        The lambda function f_lambda should not include closures.  If it does, construct the lambda with default parameter capture:

        &gt;&gt;&gt; f = lambda x, prm1=42: x+prm1

        instead of:

        &gt;&gt;&gt; prm1 = 42
        &gt;&gt;&gt; f = lambda x: x+prm1

        &#34;&#34;&#34;
        c = self._client()

        if c is None:
            f_lambda_ordered = (lambda x,f=f_lambda: (x[0], f(x[1]))) if self._ordered else f_lambda
            self._objlist = [f_lambda_ordered(o) for o in self._objlist]  # no parallelism
        else:
            f_lambda_ordered = (lambda x,f=f_lambda: (x[0], f(x[1]))) if self._ordered else f_lambda
            objlist = c.scatter(self._objlist) if (self._minscatter is not None and len(self._objlist) &gt;= self._minscatter) else self._objlist
            self._objlist = self._wait(c.map(f_lambda_ordered, objlist))
        return self

    def filter(self, f_lambda):
        &#34;&#34;&#34;Run the lambda function on each of the elements of the batch and filter based on the provided lambda keeping those elements that return true 
        &#34;&#34;&#34;
        c = self._client()
        f_lambda_ordered = (lambda x,f=f_lambda: (x[0], f(x[1]))) if self._ordered else f_lambda

        if c is None:
            self._objlist = [o for o in self._objlist if f_lambda_ordered(o)[1]]  # no parallelism
        else:
            objlist = self._objlist  # original list
            is_filtered = self._wait(c.map(f_lambda_ordered, c.scatter(self._objlist)))  # distributed filter (replaces self._objlist)
            self._objlist = [obj for (f, obj) in zip(is_filtered, objlist) if f[1] is True]  # keep only elements that filter true
        return self
        
    def scattermap(self, f_lambda, obj):
        &#34;&#34;&#34;Scatter obj to all workers, and apply lambda function f(obj, im) to each element in batch
        
           Usage: 
         
           &gt;&gt;&gt; Batch(mylist, ngpu=8).scattermap(lambda net, im: net(im), net).result()
        
           This will scatter the large object net to all workers, and pin it to a specific GPU.  Within the net object, you can call 
           vipy.global.gpuindex() to retrieve your assigned GPU index, which can be used by torch.cuda.device().  Then, the net
           object processes each element in the batch using net according to the lambda, and returns the results.  This function 
           includes ngpu processes, and assumes there are ngpu available on the target machine.  Each net is replicated in a different
           process, so it is the callers responsibility for getting vipy.global.gpuindex() from within the process and setting 
           net to take advantage of this GPU rather than using the default cuda:0.  

        &#34;&#34;&#34;
        c = self._client()
        f_lambda_ordered = (lambda net,x,f=f_lambda: (x[0], f(net,x[1]))) if self._ordered else (lambda net,x,f=f_lambda: f(net, x))

        if c is None:
            self._objlist = [f_lambda_ordered(obj, o) for o in self._objlist]  # no parallelism
        else:
            objdist = c.scatter(obj, broadcast=True)        
            objlist = c.scatter(self._objlist) if (self._minscatter is not None and len(self._objlist) &gt;= self._minscatter) else self._objlist
            self._objlist = self._wait([c.submit(f_lambda_ordered, objdist, im) for im in objlist])
        return self</code></pre>
</details>
<h3>Methods</h3>
<dl>
<dt id="vipy.batch.Batch.filter"><code class="name flex">
<span>def <span class="ident">filter</span></span>(<span>self, f_lambda)</span>
</code></dt>
<dd>
<div class="desc"><p>Run the lambda function on each of the elements of the batch and filter based on the provided lambda keeping those elements that return true</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/visym/vipy/blob/f10b3f6aec0ef011e86ee11667579388379a8ac3/vipy/batch.py#L185-L197" class="git-link">Browse git</a>
</summary>
<pre><code class="python">def filter(self, f_lambda):
    &#34;&#34;&#34;Run the lambda function on each of the elements of the batch and filter based on the provided lambda keeping those elements that return true 
    &#34;&#34;&#34;
    c = self._client()
    f_lambda_ordered = (lambda x,f=f_lambda: (x[0], f(x[1]))) if self._ordered else f_lambda

    if c is None:
        self._objlist = [o for o in self._objlist if f_lambda_ordered(o)[1]]  # no parallelism
    else:
        objlist = self._objlist  # original list
        is_filtered = self._wait(c.map(f_lambda_ordered, c.scatter(self._objlist)))  # distributed filter (replaces self._objlist)
        self._objlist = [obj for (f, obj) in zip(is_filtered, objlist) if f[1] is True]  # keep only elements that filter true
    return self</code></pre>
</details>
</dd>
<dt id="vipy.batch.Batch.map"><code class="name flex">
<span>def <span class="ident">map</span></span>(<span>self, f_lambda, args=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Run the lambda function on each of the elements of the batch and return the batch object.</p>
<pre><code class="language-python-repl">&gt;&gt;&gt; iml = [vipy.image.RandomScene(512,512) for k in range(0,1000)]   
&gt;&gt;&gt; imb = vipy.image.Batch(iml) 
&gt;&gt;&gt; imb.map(lambda im: im.rgb())  
</code></pre>
<p>The lambda function f_lambda should not include closures.
If it does, construct the lambda with default parameter capture:</p>
<pre><code class="language-python-repl">&gt;&gt;&gt; f = lambda x, prm1=42: x+prm1
</code></pre>
<p>instead of:</p>
<pre><code class="language-python-repl">&gt;&gt;&gt; prm1 = 42
&gt;&gt;&gt; f = lambda x: x+prm1
</code></pre></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/visym/vipy/blob/f10b3f6aec0ef011e86ee11667579388379a8ac3/vipy/batch.py#L157-L183" class="git-link">Browse git</a>
</summary>
<pre><code class="python">def map(self, f_lambda, args=None):
    &#34;&#34;&#34;Run the lambda function on each of the elements of the batch and return the batch object.
    
    &gt;&gt;&gt; iml = [vipy.image.RandomScene(512,512) for k in range(0,1000)]   
    &gt;&gt;&gt; imb = vipy.image.Batch(iml) 
    &gt;&gt;&gt; imb.map(lambda im: im.rgb())  

    The lambda function f_lambda should not include closures.  If it does, construct the lambda with default parameter capture:

    &gt;&gt;&gt; f = lambda x, prm1=42: x+prm1

    instead of:

    &gt;&gt;&gt; prm1 = 42
    &gt;&gt;&gt; f = lambda x: x+prm1

    &#34;&#34;&#34;
    c = self._client()

    if c is None:
        f_lambda_ordered = (lambda x,f=f_lambda: (x[0], f(x[1]))) if self._ordered else f_lambda
        self._objlist = [f_lambda_ordered(o) for o in self._objlist]  # no parallelism
    else:
        f_lambda_ordered = (lambda x,f=f_lambda: (x[0], f(x[1]))) if self._ordered else f_lambda
        objlist = c.scatter(self._objlist) if (self._minscatter is not None and len(self._objlist) &gt;= self._minscatter) else self._objlist
        self._objlist = self._wait(c.map(f_lambda_ordered, objlist))
    return self</code></pre>
</details>
</dd>
<dt id="vipy.batch.Batch.result"><code class="name flex">
<span>def <span class="ident">result</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Return the result of the batch processing, ordered</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/visym/vipy/blob/f10b3f6aec0ef011e86ee11667579388379a8ac3/vipy/batch.py#L145-L151" class="git-link">Browse git</a>
</summary>
<pre><code class="python">def result(self):
    &#34;&#34;&#34;Return the result of the batch processing, ordered&#34;&#34;&#34;
    if self._ordered:
        objlist = {int(v[0]):v[1] for v in self._objlist if v is not None}
        return [objlist[k] if k in objlist else None for k in range(len(self._objlist))]  # restore order
    else:
        return self._objlist</code></pre>
</details>
</dd>
<dt id="vipy.batch.Batch.scattermap"><code class="name flex">
<span>def <span class="ident">scattermap</span></span>(<span>self, f_lambda, obj)</span>
</code></dt>
<dd>
<div class="desc"><p>Scatter obj to all workers, and apply lambda function f(obj, im) to each element in batch</p>
<p>Usage: </p>
<pre><code class="language-python-repl">&gt;&gt;&gt; Batch(mylist, ngpu=8).scattermap(lambda net, im: net(im), net).result()
</code></pre>
<p>This will scatter the large object net to all workers, and pin it to a specific GPU.
Within the net object, you can call
vipy.global.gpuindex() to retrieve your assigned GPU index, which can be used by torch.cuda.device().
Then, the net
object processes each element in the batch using net according to the lambda, and returns the results.
This function
includes ngpu processes, and assumes there are ngpu available on the target machine.
Each net is replicated in a different
process, so it is the callers responsibility for getting vipy.global.gpuindex() from within the process and setting
net to take advantage of this GPU rather than using the default cuda:0.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/visym/vipy/blob/f10b3f6aec0ef011e86ee11667579388379a8ac3/vipy/batch.py#L199-L223" class="git-link">Browse git</a>
</summary>
<pre><code class="python">def scattermap(self, f_lambda, obj):
    &#34;&#34;&#34;Scatter obj to all workers, and apply lambda function f(obj, im) to each element in batch
    
       Usage: 
     
       &gt;&gt;&gt; Batch(mylist, ngpu=8).scattermap(lambda net, im: net(im), net).result()
    
       This will scatter the large object net to all workers, and pin it to a specific GPU.  Within the net object, you can call 
       vipy.global.gpuindex() to retrieve your assigned GPU index, which can be used by torch.cuda.device().  Then, the net
       object processes each element in the batch using net according to the lambda, and returns the results.  This function 
       includes ngpu processes, and assumes there are ngpu available on the target machine.  Each net is replicated in a different
       process, so it is the callers responsibility for getting vipy.global.gpuindex() from within the process and setting 
       net to take advantage of this GPU rather than using the default cuda:0.  

    &#34;&#34;&#34;
    c = self._client()
    f_lambda_ordered = (lambda net,x,f=f_lambda: (x[0], f(net,x[1]))) if self._ordered else (lambda net,x,f=f_lambda: f(net, x))

    if c is None:
        self._objlist = [f_lambda_ordered(obj, o) for o in self._objlist]  # no parallelism
    else:
        objdist = c.scatter(obj, broadcast=True)        
        objlist = c.scatter(self._objlist) if (self._minscatter is not None and len(self._objlist) &gt;= self._minscatter) else self._objlist
        self._objlist = self._wait([c.submit(f_lambda_ordered, objdist, im) for im in objlist])
    return self</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<header>
<a class="homelink" rel="home" title="VIPY" href="https://github.com/visym/vipy/">
<img src="https://www.visym.com/labs/images/visym_logo_black_notext.png" alt="" width="60">
</a>
<h1 style="font-size:200%;"><b>VIPY:</b> Visual Dataset Transformation</h1>
</header>
<form>
<input id="lunr-search" name="q" placeholder="ðŸ”Ž Search ..." aria-label="Search"
disabled minlength="2">
</form>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/tingle/0.15.3/tingle.min.css" integrity="sha512-j1u8eUJ4f23xPPxwOrLUPQaCD2dwzNqqmDDcWS4deWsMv2ohLqmXXuP3hU7g8TyzbMSakP/mMqoNBYWj8AEIFg==" crossorigin>
<script src="https://cdnjs.cloudflare.com/ajax/libs/tingle/0.15.3/tingle.min.js" integrity="sha512-plGUER9JkeEWPPqQBE4sdLqBoQug5Ap+BCGMc7bJ8BXkm+VVj6QzkpBz5Yv2yPkkq+cqg9IpkBaGCas6uDbW8g==" crossorigin></script>
<style>
.modal-dialog iframe {
width: 100vw;
height: calc(100vh - 80px);
}
@media screen and (min-width: 700px) {
.modal-dialog iframe {
width: 70vw;
height: 80vh;
}
}
.modal-dialog .tingle-modal-box {width: auto;}
.modal-dialog .tingle-modal-box__content {padding: 0;}
</style>
<script>
const input = document.getElementById('lunr-search');
input.disabled = false;
input.form.addEventListener('submit', (ev) => {
ev.preventDefault();
const url = new URL(window.location);
url.searchParams.set('q', input.value);
history.replaceState({}, null, url.toString());
search(input.value);
});
const query = new URL(window.location).searchParams.get('q');
if (query)
search(query);
function search(query) {
const url = './doc-search.html#' + encodeURIComponent(query);
new tingle.modal({
cssClass: ['modal-dialog'],
onClose: () => {
const url = new URL(window.location);
url.searchParams.delete('q');
history.replaceState({}, null, url.toString());
setTimeout(() => input.focus(), 100);
}
}).setContent('<iframe src="' + url + '"></iframe>').open();
}
</script>
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="vipy" href="index.html">vipy</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="vipy.batch.Batch" href="#vipy.batch.Batch">Batch</a></code></h4>
<ul class="">
<li><code><a title="vipy.batch.Batch.filter" href="#vipy.batch.Batch.filter">filter</a></code></li>
<li><code><a title="vipy.batch.Batch.map" href="#vipy.batch.Batch.map">map</a></code></li>
<li><code><a title="vipy.batch.Batch.result" href="#vipy.batch.Batch.result">result</a></code></li>
<li><code><a title="vipy.batch.Batch.scattermap" href="#vipy.batch.Batch.scattermap">scattermap</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.11.6</a>.</p>
</footer>
</body>
</html>
