<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<meta name="google-site-verification" content="aB8LkQegj94_TJPdrcJm2ldIRWyXY82Jp24Gtkdgyn0" />
<title>vipy documentation</title>
<meta name="description" content="VIPY is a python package for representation, transformation and visualization of annotated videos and images.
Annotations are the ground truth â€¦" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}#lunr-search{width:100%;font-size:1em;padding:6px 9px 5px 9px;border:1px solid silver}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">VIPY</h1>
</header>
<section id="section-intro">
<p>VIPY is a python package for representation, transformation and visualization of annotated videos and images.
Annotations are the ground truth provided by labelers (e.g. object bounding boxes, face identities, temporal activity clips), suitable for training computer vision systems.
VIPY provides tools to easily edit videos and images so that the annotations are transformed along with the pixels.
This enables a clean interface for transforming complex datasets for input to your computer vision training and testing pipeline.</p>
<p>VIPY provides:</p>
<ul>
<li>Representation of videos with labeled activities that can be resized, clipped, rotated, scaled and cropped</li>
<li>Representation of images with object bounding boxes that can be manipulated as easily as editing an image</li>
<li>Clean visualization of annotated images and videos</li>
<li>Lazy loading of images and videos suitable for distributed procesing (e.g. dask, spark)</li>
<li>Straightforward integration into machine learning toolchains (e.g. torch, numpy)</li>
<li>Fluent interface for chaining operations on videos and images</li>
<li>Dataset download, unpack and import (e.g. Charades, AVA, ActivityNet, Kinetics, Moments in Time)</li>
<li>Video and image web search tools with URL downloading and caching</li>
<li>Minimum dependencies for easy installation (e.g. AWS Lambda)</li>
</ul>
<h1 id="design-goals">Design Goals</h1>
<p>Vipy was created with three design goals.
</p>
<ul>
<li><strong>Simplicity</strong>.
Annotated Videos and images should be as easy to manipulate as the pixels.
We provide a simple fluent API that enables the transformation of media so that pixels are transformed along with the annotations.
We provide a comprehensive unit test suite to validate this pipeline with continuous integration.</li>
<li><strong>Portability</strong>.
Vipy was designed with the goal of allowing it to be easily retargeted to new platforms.
For example, deployment on a serverless architecture such as AWS lambda has restrictions on the allowable code that can be executed in layers.
We designed Vipy with minimal dependencies on standard and mature machine learning tool chains (numpy, matplotlib, ffmpeg, pillow) to ensure that it can be ported to new computational environments. </li>
<li><strong>Efficiency</strong>.
Vipy is written in pure python with the goal of performing in place operations and avoiding copies of media whenever possible.
This enables fast video processing by operating on videos as chains of transformations.
The documentation describes when an object is changed in place vs. copied.
Furthermore, loading of media is delayed until explicitly requested by the user (or the pixels are needed) to enable lazy loading for distributed processing.
</li>
</ul>
<h1 id="getting-started">Getting started</h1>
<p>The VIPY tools are designed for simple and intuitive interaction with videos and images.
Try to create a <code><a title="vipy.video.Scene" href="video.html#vipy.video.Scene">Scene</a></code> object:</p>
<pre><code class="language-python">v = vipy.video.RandomScene()
</code></pre>
<p>Videos are constructed from URLs (e.g. RTSP/RTMP live camera streams, YouTube videos, public or keyed AWS S3 links), SSH accessible paths, local filenames, <code><a title="vipy.image.Image" href="image.html#vipy.image.Image">Image</a></code> frame lists, numpy arrays or pytorch tensors.
In this example, we create a random video with tracks and activities.
Videos can be natively iterated:</p>
<pre><code class="language-python">for im in v:
    print(im.numpy())
</code></pre>
<p>This will iterate and yield <code><a title="vipy.image.Image" href="image.html#vipy.image.Image">Image</a></code> objects corresponding to each frame of the video.
You can use the <code><a title="vipy.image.Image.numpy" href="image.html#vipy.image.Image.numpy">Image.numpy()</a></code> method to extract the numpy array for this frame.
Long videos are streamed to avoid out of memory errors.
Under the hood, we represent each video as a filter chain to an FFMPEG pipe, which yields frames corresponding to the appropriate filter transform and framerate.
The yielded frames include all of the objects that are present in the video at that frame accessible with the <code><a title="vipy.image.Scene.objects" href="image.html#vipy.image.Scene.objects">Scene.objects()</a></code> method.</p>
<p>VIPY supports more complex iterators.
For example, a common use case for activity detection is iterating over short clips in a video.
You can do this using the stream iterator:</p>
<pre><code class="language-python">for c in v.stream().clip(16):
    print(c.torch())
</code></pre>
<p>This will yield <code><a title="vipy.video.Scene" href="video.html#vipy.video.Scene">Scene</a></code> objects each containing a <code><a title="vipy.video.Stream.clip" href="video.html#vipy.video.Stream.clip">Stream.clip()</a></code> of length 16 frames.
Each clip overlaps by 15 frames with the next clip, and each clip includes a threaded copy of the pixels.
This is useful to provide clips of a fixed length that are output for every frame of the video.
Each clip contais the tracks and activities within this clip time period.
The method <code><a title="vipy.video.Video.torch" href="video.html#vipy.video.Video.torch">Video.torch()</a></code> will output a torch tensor suitable for integration into a pytorch based system.</p>
<p>These python iterators can be combined together in complex ways</p>
<pre><code class="language-python">for (im, c, imdelay) in (v, v.stream().clip(16), v.stream().frame(delay=10), a_gpu_function(v.stream().batch(16)))
    print(im, c.torch(), imdelay)
</code></pre>
<p>This will yield the current frame, a video <code><a title="vipy.video.Stream.clip" href="video.html#vipy.video.Stream.clip">Stream.clip()</a></code> of length 16, a <code><a title="vipy.video.Stream.frame" href="video.html#vipy.video.Stream.frame">Stream.frame()</a></code> 10 frames ago and a <code><a title="vipy.video.Stream.batch" href="video.html#vipy.video.Stream.batch">Stream.batch()</a></code> of 16 frames that is designed for computation and transformation on a GPU.
All of the pixels are copied in threaded processing which is efficiently hidden by GPU I/O bound operations.
For more examples of complex iterators in real world use cases, see the <a href="https://github.com/visym/heyvi">HeyVi package</a> for open source visual analytics.</p>
<p>Videos can be transformed in complex ways, and the pixels will always be transformed along with the annotations.</p>
<pre><code class="language-python">v.fliplr()          # flip horizontally
v.zeropad(10, 20)   # zero pad the video horizontally and vertically
v.mindim(256)       # change the minimum dimension of the video
v.framerate(10)     # change the framerate of the video 
</code></pre>
<p>The transformation is lazy and is incorporated into the FFMPEG complex filter chain so that the transformation is applied when the pixels are needed.
You can always access the current filter chain using <code><a title="vipy.video.Video.commandline" href="video.html#vipy.video.Video.commandline">Video.commandline()</a></code> which will output a commandline string for the ffmpeg executable that you can use to get a deeper underestanding of the transformations that are applied to the video pixels.</p>
<p>Finally, annotated videos can be displayed. </p>
<pre><code class="language-python">v.show()
v.show(notebook=True)
v.frame().show()
v.annotate('/path/to/visualization.mp4')
with vipy.video.Video(url='rtmps://youtu.be/...').mindim(512).framerate(5).stream(write=True) as s:
    for im in v.framerate(5):
        s.write(im.annotate().rgb())
</code></pre>
<p>This will <code><a title="vipy.video.Scene.show" href="video.html#vipy.video.Scene.show">Scene.show()</a></code> the video live on your desktop, in a jupyter notebook, show the first <code><a title="vipy.video.Scene.frame" href="video.html#vipy.video.Scene.frame">Scene.frame()</a></code> as a static image, <code><a title="vipy.video.Scene.annotate" href="video.html#vipy.video.Scene.annotate">Scene.annotate()</a></code> the video so that annotations are in the pixels and save the corresponding video, or live stream a 5Hz video to youtube.
All of the show methods can be configured to customize the colors or captions.</p>
<p>See the <a href="https://github.com/visym/vipy/tree/master/demo">demos</a> for more examples.</p>
<h2 id="parallelization">Parallelization</h2>
<p>Vipy includes integration with <a href="https://distributed.dask.org">Dask Distributed</a> for parallel processing of video and images.
This is useful for video preprocessing of datasets to prepare them for training.
</p>
<p>For example, we can construct a <code><a title="vipy.dataset.Dataset" href="dataset.html#vipy.dataset.Dataset">Dataset</a></code> object from one or more videos.
This dataset can be transformed in parallel using two processes:</p>
<pre><code class="language-python">D = vipy.dataset.Dataset(vipy.video.Scene(filename='/path/to/videofile.mp4'))
with vipy.globals.parallel(2):
    R = D.map(lambda v, outdir='/newpath/to/': v.mindim(128).framerate(5).saveas(so.path.join(outdir, vipy.util.filetail(v.filename()))))
</code></pre>
<p>The result is a transformed dataset which contains transformed videos downsampled to have minimum dimension 128, framerate of 5Hz, with the annotations transformed accordingly.
The <code><a title="vipy.dataset.Dataset.map" href="dataset.html#vipy.dataset.Dataset.map">Dataset.map()</a></code> method allows for a lambda function to be applied in parallel to all elements in a dataset.
The fluent design of the VIPY objects allows for easy chaining of video operations to be expressed as a lambda function.
VIPY objects are designed for integration into parallel processing tool chains and can be easily serialized and deserialized for sending to parallel worker tasks.
</p>
<p>VIPY supports integration with distributed schedulers for massively parallel operation.
</p>
<pre><code class="language-python">D = vipy.dataset.Dataset('/path/to/directory/of/jsonfiles')
with vipy.globals.parallel(scheduler='10.0.0.1:8785'):
    R = D.map(lambda v, outdir='/newpath/to': vipy.util.bz2pkl(os.path.join(outdir, '%s.pkl.bz2' % v.videoid()), v.trackcrop().mindim(128).normalize(mean=(128,128,128)).torch()))
</code></pre>
<p>This will lazy load a directory of JSON files, where each JSON file corresponds to the annotations of a single video, such as those collected by <a href="https://visym.github.io/collector">Visym Collector</a>.
The <code><a title="vipy.dataset.Dataset.map" href="dataset.html#vipy.dataset.Dataset.map">Dataset.map()</a></code> method will communicate with a <a href="https://docs.dask.org/en/stable/how-to/deploy-dask/ssh.html">scheduler</a> at a given IP address and port and will process the lambda function in parallel to the workers tasked by the scheduler.
In this example, the video will <code><a title="vipy.video.Scene.trackcrop" href="video.html#vipy.video.Scene.trackcrop">Scene.trackcrop()</a></code> the smallest bounding box containing all tracks in the video, resized so this crop is 128 on the smallest side, loaded and normalized to remove the mean, then saved as a torch tensor in a bzipped python pickle file.
This is useful for preprocesssing videos to torch tensors for fast loading of dataset augmentation during training.</p>
<h2 id="import">Import</h2>
<p>Vipy was designed to define annotated videos and imagery as collections of python objects.
The core objects for images are:</p>
<ul>
<li><a href="image.html#vipy.image.Scene">vipy.image.Scene</a></li>
<li><a href="object.html#vipy.object.Detection">vipy.object.Detection</a></li>
<li><a href="geometry.html#vipy.geometry.BoundingBox">vipy.geometry.BoundingBox</a></li>
</ul>
<p>The core objects for videos:</p>
<ul>
<li><a href="video.html#vipy.video.Scene">vipy.video.Scene</a></li>
<li><a href="object.html#vipy.object.Track">vipy.object.Track</a></li>
<li><a href="activity.html#vipy.activity.Activity">vipy.activity.Activity</a></li>
</ul>
<p>See the documentation for each object for how to construct them.
Alternatively, see our <a href="https://github.com/visym/heyvi">open source visual analytics</a> for construction of vipy objects from activity and object detectors.</p>
<h2 id="export">Export</h2>
<p>All vipy objects can be imported and exported to JSON for interoperatability with other tool chains.
This allows for introspection of the vipy object state in an open format providing transparency</p>
<pre><code class="language-python">vipy.image.owl().json()
</code></pre>
<h2 id="environment-variables">Environment variables</h2>
<p>You can set the following environment variables to customize the output of vipy</p>
<ul>
<li><strong>VIPY_CACHE</strong>='/path/to/directory'.
This directory will contain all of the cached downloaded filenames when downloading URLs.
For example, the following will download all media to '~/.vipy'.</li>
</ul>
<pre><code class="language-python">os.environ['VIPY_CACHE'] = vipy.util.remkdir('~/.vipy')
vipy.image.Image(url='https://upload.wikimedia.org/wikipedia/commons/thumb/2/23/Bubo_virginianus_06.jpg/1920px-Bubo_virginianus_06.jpg').download()
</code></pre>
<p>This will output an image object:</p>
<pre><code class="language-python">&lt;vipy.image: filename=&quot;~/.vipy/1920px-Bubo_virginianus_06.jpg&quot;, filename=&quot;~/.vipy/1920px-Bubo_virginianus_06.jpg&quot;, url=&quot;https://upload.wikimedia.org/wikipedia/commons/thumb/2/23/Bubo_virginianus_06.jpg/1920px-Bubo_virginianus_06.jpg&quot;&gt;
</code></pre>
<p>This provides control over where large datasets are cached on your local file system.
By default, this will be cached to the system temp directory.</p>
<ul>
<li><strong>VIPY_AWS_ACCESS_KEY_ID</strong>='MYKEY'.
This is the <a href="https://docs.aws.amazon.com/general/latest/gr/aws-sec-cred-types.html">AWS key</a> to download urls of the form "s3://".
</li>
<li><strong>VIPY_AWS_SECRET_ACCESS_KEY</strong>='MYKEY'.
This is the <a href="https://docs.aws.amazon.com/general/latest/gr/aws-sec-cred-types.html">AWS secret key</a> to download urls of the form "s3://".</li>
</ul>
<h2 id="versioning">Versioning</h2>
<p>To determine what vipy version you are running you can use:</p>
<pre><code class="language-python-repl">&gt;&gt;&gt; vipy.__version__
&gt;&gt;&gt; vipy.version.is_at_least('1.11.1') 
</code></pre>
<h1 id="tutorials">Tutorials</h1>
<p>The following tutorials show fluent python chains to achieve transformations of annotated images and videos.</p>
<h2 id="images">Images</h2>
<h3 id="load-an-image">Load an image</h3>
<p>Images can be loaded from URLs, local image files, or numpy arrays.
The images exhibit lazy loading, so that pixels will not be fetched until they are needed.</p>
<pre><code class="language-python">&gt;&gt;&gt; im = vipy.image.Image(filename='/path/to/in.jpg')  
&gt;&gt;&gt; im = vipy.image.Image(url='https://url/to/in.jpg')  
&gt;&gt;&gt; im = vipy.image.Image(array=np.random.rand(224,224,3).astype(np.float32))  
</code></pre>
<h3 id="display-an-image-to-stdout">Display an image to stdout</h3>
<p>All objects have helpful string representations when printed to stdout.
This is accessible via the <code><a title="vipy.image.Image.print" href="image.html#vipy.image.Image.print">Image.print()</a></code> method or by using builtin print().
In this example, an image is created from a wikipedia URL.
Printing this image object shows the URL, but when it is loaded, the image object shows the size of the image, colorspace and the filename that the URL was downloaded to.
When in doubt, print!</p>
<pre><code class="language-python">&gt;&gt;&gt; print(vipy.image.Scene(url='https://upload.wikimedia.org/wikipedia/commons/thumb/2/23/Bubo_virginianus_06.jpg/1920px-Bubo_virginianus_06.jpg'))
&lt;vipy.image.scene: url=https://upload.wikimedia.org/wikipedia/commons/thumb/2/23/Bubo_virginianus_06.jpg/1920px-Bubo_virginianus_06.jpg&gt;

&gt;&gt;&gt; vipy.image.Scene(url='https://upload.wikimedia.org/wikipedia/commons/thumb/2/23/Bubo_virginianus_06.jpg/1920px-Bubo_virginianus_06.jpg').load().print()
&lt;vipy.image.scene: height=2400, width=1920, color=rgb, filename=&quot;/tmp/1920px-Bubo_virginianus_06.jpg&quot;, url=https://upload.wikimedia.org/wikipedia/commons/thumb/2/23/Bubo_virginianus_06.jpg/1920px-Bubo_virginianus_06.jpg&gt;
</code></pre>
<h3 id="transform-an-image">Transform an image</h3>
<p>Images can be transformed so that the annotations are updated along with the pixels.
In this example, the <code><a title="vipy.image.owl" href="image.html#vipy.image.owl">owl()</a></code> is a demo image to a wikipedia URL with a bounding box.
This can be resized and cropped or anisotropically scaled and the box is updated to match the pixels. </p>
<pre><code class="language-python">&gt;&gt;&gt; im = vipy.image.owl().mindim(512).fliplr().centersquare().show()
&gt;&gt;&gt; im = vipy.image.owl().resize(width=512, height=256).show()
</code></pre>
<p><img src="https://raw.githubusercontent.com/visym/vipy/master/docs/tutorials/transform_an_image_1.jpg" height="250">
<img src="https://raw.githubusercontent.com/visym/vipy/master/docs/tutorials/transform_an_image_2.jpg" height="250"></p>
<h3 id="export-as-numpy-array">Export as numpy array</h3>
<p>All images are represented internally as a private attribute <code>vipy.image.Image._array</code> which is a numpy array representation of the pixels.
Image transformations can be chained to operate sequentially on this pixel buffer.
In this example, the <code><a title="vipy.image.owl" href="image.html#vipy.image.owl">owl()</a></code> test image is cropped to retain the center square, converted from uint8 RGB to float32 greyscale, resized to 224x224 then exported to numpy array.
</p>
<pre><code class="language-python">&gt;&gt;&gt; vipy.image.owl().centersquare().greyscale().mindim(224).numpy()
array([[0.11470564, 0.11794835, 0.13006495, ..., 0.15657625, 0.15867704,
        0.16140679],
       [0.11835834, 0.11993656, 0.12860955, ..., 0.15611856, 0.15460114,
        0.15652661],
       [0.12262769, 0.1245698 , 0.12809968, ..., 0.153694  , 0.15326852,
        0.15336327],
       ...,
       [0.42591274, 0.42745316, 0.4352066 , ..., 0.12994824, 0.13172676,
        0.13424061],
       [0.42972928, 0.43847743, 0.45459685, ..., 0.12558977, 0.12820148,
        0.13141613],
       [0.44050908, 0.45350933, 0.46908155, ..., 0.12246227, 0.1256479 ,
        0.12941177]], dtype=float32)
</code></pre>
<h3 id="display-an-image">Display an image</h3>
<p>All images can be displayed using the matplotlib library.
Matplotlib is the most universally ported GUI library for python, and exhibits minimal dependencies.
We enable the user to show images using figure window or "matlab style" of image display.
This will show pixels with overlayed semi-transparent bounding boxes for objects with captions.</p>
<pre><code class="language-python">&gt;&gt;&gt; im = vipy.image.owl().mindim(512).show()
</code></pre>
<p><img src="https://raw.githubusercontent.com/visym/vipy/master/docs/tutorials/display_an_image.jpg" height="500"></p>
<h3 id="annotate-an-image">Annotate an image</h3>
<p>By default, images and annotations are represented independently.
However, it is sometimes useful to export the annotations into the pixels.
The <code><a title="vipy.image.Scene.annotate" href="image.html#vipy.image.Scene.annotate">Scene.annotate()</a></code> method will export the same visualization as when the image is displayed, but the pixel buffer will be overwritten with the shown image.
This means that calling <code><a title="vipy.image.Image.numpy" href="image.html#vipy.image.Image.numpy">Image.numpy()</a></code> will return the pixel buffer with boxes and captions in the pixels.</p>
<pre><code class="language-python">&gt;&gt;&gt; vipy.image.owl().mindim(512).maxmatte().annotate().rgb().saveas('out.jpg')
</code></pre>
<p><img src="https://raw.githubusercontent.com/visym/vipy/master/docs/tutorials/annotate_an_image.jpg" height="500"></p>
<h3 id="save-an-image">Save an image</h3>
<p>Images can be saved (without annotations) using the <code><a title="vipy.image.Image.saveas" href="image.html#vipy.image.Image.saveas">Image.saveas()</a></code> method.
Calling this method with no arguments will save to a random temporary image.
In this example, we crop the image, convert from RGB colorspace to BGR colorspace, flip up/down and resize.</p>
<pre><code class="language-python">&gt;&gt;&gt; vipy.image.owl().centersquare().bgr().flipud().mindim(224).saveas('save_an_image.jpg')
</code></pre>
<p><img src="https://raw.githubusercontent.com/visym/vipy/master/docs/tutorials/save_an_image.jpg" height="300"></p>
<h3 id="convert-image-colorspace">Convert image colorspace</h3>
<p>All images can be converted between different colorspaces (e.g. RGB, BGR, RGBA, BGRA, HSV, GREY, LUM, float).
This will convert the underlying pixel buffer to support the corresponding colorspace.
</p>
<pre><code>&gt;&gt;&gt; vipy.image.owl().hsv().saveas('hsv.jpg')
</code></pre>
<h3 id="rescale-image">Rescale image</h3>
<p>All images can be rescaled to a standard range, including the Matlab inspired <code><a title="vipy.image.Image.mat2gray" href="image.html#vipy.image.Image.mat2gray">Image.mat2gray()</a></code>, which will rescale the pixel buffer between [min, max] -&gt; [0, 1]</p>
<h3 id="visualize-scenes">Visualize scenes</h3>
<p>Scenes containing objects can be visualized to display only a subset of objects.
In this example, we show the demo image <code><a title="vipy.image.vehicles" href="image.html#vipy.image.vehicles">vehicles()</a></code> which contains four annotated vehicles.
There are many more vehicles in this image, but the end user may be interested in these four in particular.
Each object is represented internally as a list of <code><a title="vipy.object.Detection" href="object.html#vipy.object.Detection">Detection</a></code> objects which encodes a bounding box and category.
This can be visualized just as with images with single objects.</p>
<pre><code class="language-python">&gt;&gt;&gt; vipy.image.vehicles().show().objects()
[&lt;vipy.object.detection: category=&quot;car&quot;, bbox=(xmin=210.2, ymin=263.2, width=41.1, height=32.6)&gt;,
 &lt;vipy.object.detection: category=&quot;car&quot;, bbox=(xmin=626.7, ymin=336.0, width=77.9, height=65.5)&gt;,
 &lt;vipy.object.detection: category=&quot;car&quot;, bbox=(xmin=140.8, ymin=284.5, width=53.1, height=53.1)&gt;,
 &lt;vipy.object.detection: category=&quot;car&quot;, bbox=(xmin=394.2, ymin=396.8, width=99.5, height=87.4)&gt;]
</code></pre>
<p><img src="https://raw.githubusercontent.com/visym/vipy/master/docs/tutorials/visualize_complex_scenes.jpg" height="500"></p>
<h3 id="crop-and-resize-annotated-objects-in-a-scene">Crop and resize annotated objects in a scene</h3>
<pre><code class="language-python">&gt;&gt;&gt; im = vipy.image.vehicles().show()
&gt;&gt;&gt; vipy.visualize.montage([o.dilate(1.2).maxsquare().crop() for o in im]).show()
</code></pre>
<p><img src="https://raw.githubusercontent.com/visym/vipy/master/docs/tutorials/vipy_image_vehicles.png" height="300">
<img src="https://raw.githubusercontent.com/visym/vipy/master/docs/tutorials/vipy_image_vehicles_objectcrop.png" height="300"></p>
<h3 id="find-all-images-in-directory">Find all images in directory</h3>
<p>Searching for all images recursively from a root directory and lazy load them as <code><a title="vipy.image.Image" href="image.html#vipy.image.Image">Image</a></code> objects.
This will not trigger loading pixels until the pixel buffers are needed.
This is helpful for importing large number of images.</p>
<pre><code class="language-python">&gt;&gt;&gt; [vipy.image.Image(filename=f) for f in vipy.util.findimages('./docs/tutorials')]
[&lt;vipy.image: filename=&quot;/Users/jebyrne/dev/vipy/docs/tutorials/transform_an_image_1.jpg&quot;&gt;, &lt;vipy.image: filename=&quot;/Users/jebyrne/dev/vipy/docs/tutorials/transform_an_image_2.jpg&quot;&gt;, ... 
</code></pre>
<h3 id="export-scene-to-json">Export scene to JSON</h3>
<p>All annotated images can be imported and exported to an open JSON format. If images are loaded, then the pixels will be serialized in the JSON output.
If this is not desired, then use the `vipy.image.Image.flush`` method to clear the cached pixel buffer prior to serialization.
This can always be reloaded after deserialization as long as the source image or URL is acessible.</p>
<pre><code class="language-python">&gt;&gt;&gt; json = vipy.image.owl().flush().json()
&gt;&gt;&gt; im = vipy.image.Scene.from_json(json)
&gt;&gt;&gt; print(json)
'{&quot;_filename&quot;:&quot;\/Users\/jebyrne\/.vipy\/1920px-Bubo_virginianus_06.jpg&quot;,&quot;_url&quot;:&quot;https:\/\/upload.wikimedia.org\/wikipedia\/commons\/thumb\/2\/23\/Bubo_virginianus_06.jpg\/1920px-Bubo_virginianus_06.jpg&quot;,&quot;_loader&quot;:null,&quot;_array&quot;:null,&quot;_colorspace&quot;:&quot;rgb&quot;,&quot;attributes&quot;:{},&quot;_category&quot;:&quot;Nature&quot;,&quot;_objectlist&quot;:[{&quot;_xmin&quot;:93.33333333333333,&quot;_ymin&quot;:85.33333333333333,&quot;_xmax&quot;:466.6666666666667,&quot;_ymax&quot;:645.3333333333334,&quot;_id&quot;:&quot;a047e21d&quot;,&quot;_label&quot;:&quot;Great Horned Owl&quot;,&quot;_shortlabel&quot;:&quot;Great Horned Owl&quot;}]}'
</code></pre>
<h3 id="export-scene-to-csv">Export scene to CSV</h3>
<p>All annotated images can be exported to a CSV format using object iterators.
Object precision can be changed using <code><a title="vipy.object.Detection.int" href="geometry.html#vipy.geometry.BoundingBox.int">BoundingBox.int()</a></code>.
CSV headers can be added with <code><a title="vipy.util.writecsv" href="util.html#vipy.util.writecsv">writecsv()</a></code>.</p>
<pre><code class="language-python">&gt;&gt;&gt; im = vipy.image.vehicles()
&gt;&gt;&gt; vipy.util.writecsv([(im.filename(), o.category(), o.xmin(), o.ymin(), o.width(), o.height()) for o in im.objects()], 'out.csv')
&gt;&gt;&gt; cat out.csv
/Users/jebyrne/.vipy/I-80_Eastshore_Fwy.jpg,car,210.2222222222222,263.2,41.06666666666666,32.622222222222206
/Users/jebyrne/.vipy/I-80_Eastshore_Fwy.jpg,car,626.6666666666666,336.0444444444444,77.86666666666667,65.4666666666667
/Users/jebyrne/.vipy/I-80_Eastshore_Fwy.jpg,car,140.84444444444443,284.4888888888889,53.066666666666634,53.111111111111086
/Users/jebyrne/.vipy/I-80_Eastshore_Fwy.jpg,car,394.17777777777775,396.84444444444443,99.4666666666667,87.37777777777774
</code></pre>
<h3 id="image-deduplication">Image deduplication</h3>
<p>Vipy provides a 128 bit differential perceptual hashing function which is used for near-duplicate detection.
This is useful for identifying pairs of images that differ slightly due to cropping, resizing, watermarkings.
The binary Hamming distance between two perceptual hashes is a similarity metric that can be used to identify duplicates, such that smaller is more likely to be a duplicate.</p>
<pre><code class="language-python">&gt;&gt;&gt; p = vipy.image.vehicles().perceptualhash()  # hex string
&gt;&gt;&gt; print(p)
'50515541d545f04101a005e801c25945'
&gt;&gt;&gt; q = vipy.image.vehicles().greyscale().perceptualhash()
&gt;&gt;&gt; print(q)
'50515541d545f04101a905e801c27945'
&gt;&gt;&gt; vipy.image.Image.perceptualhash_distance(p, q)  # Hamming distance
3
</code></pre>
<p>The perceptual hash function also allows for ignoring detected objects in the foreground.
A background hash <code><a title="vipy.image.Scene.bghash" href="image.html#vipy.image.Scene.bghash">Scene.bghash()</a></code> computes the perceptual hash function using only the regions not contained within the foreground bounding boxes.
This is useful for identifying near duplicate background locations where there may be different foreground objects in the scene between images.
If the <code><a title="vipy.image.Scene" href="image.html#vipy.image.Scene">Scene</a></code> has no associated foreground objects, then the background hash is equivalent to the perceptual hash above.</p>
<h3 id="blur-faces">Blur Faces</h3>
<pre><code class="language-python">&gt;&gt;&gt; im = vipy.image.Image(url='https://upload.wikimedia.org/wikipedia/en/d/d6/Friends_season_one_cast.jpg')
&gt;&gt;&gt; im.facepixelize().show()
&gt;&gt;&gt; im.faceblur().show()
</code></pre>
<p><img src="https://raw.githubusercontent.com/visym/vipy/master/docs/tutorials/blur_faces_1.jpg" height="250">
<img src="https://raw.githubusercontent.com/visym/vipy/master/docs/tutorials/blur_faces_2.jpg" height="250"></p>
<h3 id="data-augmentation-for-training">Data augmentation for training</h3>
<p>Data augmentation is the process of introducing synthetic transformations of a given image to introduce additional variation during training.
Data augmentation considers scales, crops, translations, mirrors, rotations or chromatic noise which are applied to a source image to generate one or more augmentations.
</p>
<pre><code class="language-python">im = vipy.image.vehicles()
vipy.visualize.montage([[o.crop().fliplr(),                # spatial mirror
                         o.clone().dilate(1.2).crop(),     # zoom out
                         o.clone().translate(4,5).crop(),  # translation 
                         o.clone().translate(-2,9).crop(), # translation 
                         o.clone().dilate(0.8).crop(),     # zoom in 
                         o.crop().blur(sigma=1),           # spatial blur
                         o.crop().additive_noise()]        # chromatic noise 
                         for o in im])                     # for all objects in the scene
</code></pre>
<p><img src="https://raw.githubusercontent.com/visym/vipy/master/docs/tutorials/data_augmentation_for_training.jpg" height="250"></p>
<h3 id="vipy-vs-torchvision">Vipy vs. Torchvision</h3>
<h3 id="visualization-behind-ssh">Visualization behind SSH</h3>
<p>Data repositories are often accessed via data storage behind SSH.
You can set up port forwarding to visualize this data, but this may require root access to configure firewall rules.
If you have SSH public key access to your cluster machine, you can do the following:</p>
<p>On a remote machine (e.g. the cluster machine you have accessed via ssh), run:</p>
<pre><code class="language-python">remote&gt;&gt;&gt; vipy.util.scpsave(vipy.image.owl())
[vipy.util.scpsave]: On a local machine where you have public key ssh access to this remote machine run:
&gt;&gt;&gt; V = vipy.util.scpload('scp://hostname:/var/folders/sn/6n34qjp513742_5y3lvmhnlw0000gn/T/c4237a25a99b776f.pkl')
</code></pre>
<p>Then, on your local machine (e.g. your laptop), run the command output above:</p>
<pre><code class="language-python">local&gt;&gt;&gt; print(vipy.util.scpload('scp://hostname:/var/folders/sn/6n34qjp513742_5y3lvmhnlw0000gn/T/c4237a25a99b776f.pkl'))
&lt;vipy.image.scene: height=640, width=512, color=rgb, filename=&quot;/Users/jebyrne/.vipy/1920px-Bubo_virginianus_06.jpg&quot;, url=https://upload.wikimedia.org/wikipedia/commons/thumb/2/23/Bubo_virginianus_06.jpg/1920px-Bubo_virginianus_06.jpg, category=&quot;Nature&quot;, objects=1&gt;
</code></pre>
<p>The method <code><a title="vipy.util.scpsave" href="util.html#vipy.util.scpsave">scpsave()</a></code> will save a list of vipy objects to a temporary pickle file, such that the URL of each object is prepended with "scp://".
When calling <code><a title="vipy.util.scpload" href="util.html#vipy.util.scpload">scpload()</a></code> on the local machine, this will fetch the pickle file from the remote machine via scp using the default public key.
Then, when each vipy object is accessed, it will fetch the URL of the media object via scp from the remote machine.
This provides an on-demand fetching of each image from a data storage behind a SSH server without any port forwarding, and uses public key scp.
This allows for visualization of datasets that cannot be copied locally, but can be reduced on the local machine which are then fetched for visualization.</p>
<h3 id="visualization-behind-aws-s3">Visualization behind AWS S3</h3>
<h2 id="videos">Videos</h2>
<h3 id="load-from-youtube">Load from YouTube</h3>
<pre><code class="language-python">v = vipy.video.Video(url='https://youtu.be/kpBCzzzX6zA')
</code></pre>
<h3 id="inspect-the-ffmpeg-command-line">Inspect the FFMPEG command line</h3>
<pre><code class="language-python">print(vipy.video.Video(filename='/path/to/in.mp4').mindim(512).framerate(2).commandline())
</code></pre>
<pre><code>'ffmpeg -i /path/to/in.mp4 -filter_complex &quot;[0]fps=fps=2.0:round=up[s0];[s0]scale=-1:512[s1]&quot; -map &quot;[s1]&quot; dummyfile'
</code></pre>
<h3 id="export-frames-as-vipy-images">Export frames as vipy images</h3>
<pre><code class="language-python">frames = [im for im in v.framerate(1)]   # 1 Hz export
</code></pre>
<h3 id="export-frames-as-numpy-array">Export frames as numpy array</h3>
<pre><code class="language-python">frames = v.framerate(0.1).numpy()   # 0.1 Hz export
</code></pre>
<h3 id="generate-webp-animations">Generate WEBP animations</h3>
<pre><code class="language-python">v = vipy.video.RandomScene().clip(0,30).webp()
</code></pre>
<h3 id="find-all-videos-in-directory">Find all videos in directory</h3>
<h3 id="track-objects-in-video">Track objects in video</h3>
<h3 id="import-rtsp-camera-streams">Import RTSP camera streams</h3>
<h3 id="detect-activities-in-video">Detect activities in video</h3>
<h3 id="blur-people-and-cars">Blur People and Cars</h3>
<h3 id="make-a-video-mosaic-from-many-streams">Make a video mosaic from many streams</h3>
<h3 id="split-a-video-into-activity-clips">Split a video into activity clips</h3>
<h3 id="create-quicklooks-for-fast-video-watching">Create quicklooks for fast video watching</h3>
<h3 id="stabilize-background">Stabilize background</h3>
<h3 id="create-video-thumbnails">Create video thumbnails</h3>
<h3 id="create-compressed-and-cached-tensors-for-large-scale-training">Create compressed and cached tensors for large-scale training</h3>
<h3 id="export-to-json">Export to JSON</h3>
<h2 id="datasets">Datasets</h2>
<h3 id="create-a-dataset">Create a dataset</h3>
<h3 id="resize-and-crop">Resize and crop</h3>
<h3 id="import-to-torch">Import to torch</h3>
<h3 id="archive-to-targz">Archive to .tar.gz</h3>
<h3 id="export-to-json_1">Export to JSON</h3>
<h3 id="video-data-augmentation-for-training">Video data augmentation for training</h3>
<h3 id="create-standalone-html-visualizations-of-images">Create standalone HTML visualizations of images</h3>
<h1 id="contact">Contact</h1>
<p>Visym Labs &lt;<a href="&#109;&#97;&#105;&#108;&#116;&#111;&#58;&#105;&#110;&#102;&#111;&#64;&#118;&#105;&#115;&#121;&#109;&#46;&#99;&#111;&#109;">&#105;&#110;&#102;&#111;&#64;&#118;&#105;&#115;&#121;&#109;&#46;&#99;&#111;&#109;</a>&gt;</p>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/visym/vipy/blob/a3fbf42115c8bbd3bf35bbcd8e76edb6c67bed36/vipy/__init__.py#L1-L504" class="git-link">Browse git</a>
</summary>
<pre><code class="python">&#34;&#34;&#34;
VIPY is a python package for representation, transformation and visualization of annotated videos and images.  Annotations are the ground truth provided by labelers (e.g. object bounding boxes, face identities, temporal activity clips), suitable for training computer vision systems.  VIPY provides tools to easily edit videos and images so that the annotations are transformed along with the pixels.  This enables a clean interface for transforming complex datasets for input to your computer vision training and testing pipeline.

VIPY provides:

* Representation of videos with labeled activities that can be resized, clipped, rotated, scaled and cropped
* Representation of images with object bounding boxes that can be manipulated as easily as editing an image
* Clean visualization of annotated images and videos
* Lazy loading of images and videos suitable for distributed procesing (e.g. dask, spark)
* Straightforward integration into machine learning toolchains (e.g. torch, numpy)
* Fluent interface for chaining operations on videos and images
* Dataset download, unpack and import (e.g. Charades, AVA, ActivityNet, Kinetics, Moments in Time)
* Video and image web search tools with URL downloading and caching
* Minimum dependencies for easy installation (e.g. AWS Lambda)

# Design Goals

Vipy was created with three design goals.  

* **Simplicity**.  Annotated Videos and images should be as easy to manipulate as the pixels.  We provide a simple fluent API that enables the transformation of media so that pixels are transformed along with the annotations.  We provide a comprehensive unit test suite to validate this pipeline with continuous integration.
* **Portability**.  Vipy was designed with the goal of allowing it to be easily retargeted to new platforms.  For example, deployment on a serverless architecture such as AWS lambda has restrictions on the allowable code that can be executed in layers.  We designed Vipy with minimal dependencies on standard and mature machine learning tool chains (numpy, matplotlib, ffmpeg, pillow) to ensure that it can be ported to new computational environments. 
* **Efficiency**.  Vipy is written in pure python with the goal of performing in place operations and avoiding copies of media whenever possible.  This enables fast video processing by operating on videos as chains of transformations.  The documentation describes when an object is changed in place vs. copied.  Furthermore, loading of media is delayed until explicitly requested by the user (or the pixels are needed) to enable lazy loading for distributed processing.  


# Getting started

The VIPY tools are designed for simple and intuitive interaction with videos and images.  Try to create a `vipy.video.Scene` object:

```python
v = vipy.video.RandomScene()
```

Videos are constructed from URLs (e.g. RTSP/RTMP live camera streams, YouTube videos, public or keyed AWS S3 links), SSH accessible paths, local filenames, `vipy.image.Image` frame lists, numpy arrays or pytorch tensors.  In this example, we create a random video with tracks and activities.  Videos can be natively iterated:


```python
for im in v:
    print(im.numpy())
```

This will iterate and yield `vipy.image.Image` objects corresponding to each frame of the video.  You can use the `vipy.image.Image.numpy` method to extract the numpy array for this frame.  Long videos are streamed to avoid out of memory errors.  Under the hood, we represent each video as a filter chain to an FFMPEG pipe, which yields frames corresponding to the appropriate filter transform and framerate.  The yielded frames include all of the objects that are present in the video at that frame accessible with the `vipy.image.Scene.objects` method.

VIPY supports more complex iterators.  For example, a common use case for activity detection is iterating over short clips in a video.  You can do this using the stream iterator:


```python
for c in v.stream().clip(16):
    print(c.torch())
```
       
This will yield `vipy.video.Scene` objects each containing a `vipy.video.Stream.clip` of length 16 frames.  Each clip overlaps by 15 frames with the next clip, and each clip includes a threaded copy of the pixels.  This is useful to provide clips of a fixed length that are output for every frame of the video.  Each clip contais the tracks and activities within this clip time period.  The method `vipy.video.Video.torch` will output a torch tensor suitable for integration into a pytorch based system.

These python iterators can be combined together in complex ways

```python
for (im, c, imdelay) in (v, v.stream().clip(16), v.stream().frame(delay=10), a_gpu_function(v.stream().batch(16)))
    print(im, c.torch(), imdelay)
```

This will yield the current frame, a video `vipy.video.Stream.clip` of length 16, a `vipy.video.Stream.frame` 10 frames ago and a `vipy.video.Stream.batch` of 16 frames that is designed for computation and transformation on a GPU.  All of the pixels are copied in threaded processing which is efficiently hidden by GPU I/O bound operations.  For more examples of complex iterators in real world use cases, see the [HeyVi package](https://github.com/visym/heyvi) for open source visual analytics.

Videos can be transformed in complex ways, and the pixels will always be transformed along with the annotations.

```python
v.fliplr()          # flip horizontally
v.zeropad(10, 20)   # zero pad the video horizontally and vertically
v.mindim(256)       # change the minimum dimension of the video
v.framerate(10)     # change the framerate of the video 
```

The transformation is lazy and is incorporated into the FFMPEG complex filter chain so that the transformation is applied when the pixels are needed.  You can always access the current filter chain using `vipy.video.Video.commandline` which will output a commandline string for the ffmpeg executable that you can use to get a deeper underestanding of the transformations that are applied to the video pixels.

Finally, annotated videos can be displayed. 

```python
v.show()
v.show(notebook=True)
v.frame().show()
v.annotate(&#39;/path/to/visualization.mp4&#39;)
with vipy.video.Video(url=&#39;rtmps://youtu.be/...&#39;).mindim(512).framerate(5).stream(write=True) as s:
    for im in v.framerate(5):
        s.write(im.annotate().rgb())
```

This will `vipy.video.Scene.show` the video live on your desktop, in a jupyter notebook, show the first `vipy.video.Scene.frame` as a static image, `vipy.video.Scene.annotate` the video so that annotations are in the pixels and save the corresponding video, or live stream a 5Hz video to youtube.  All of the show methods can be configured to customize the colors or captions.

See the [demos](https://github.com/visym/vipy/tree/master/demo) for more examples.


## Parallelization

Vipy includes integration with [Dask Distributed](https://distributed.dask.org) for parallel processing of video and images.   This is useful for video preprocessing of datasets to prepare them for training.  

For example, we can construct a `vipy.dataset.Dataset` object from one or more videos.  This dataset can be transformed in parallel using two processes:

```python
D = vipy.dataset.Dataset(vipy.video.Scene(filename=&#39;/path/to/videofile.mp4&#39;))
with vipy.globals.parallel(2):
    R = D.map(lambda v, outdir=&#39;/newpath/to/&#39;: v.mindim(128).framerate(5).saveas(so.path.join(outdir, vipy.util.filetail(v.filename()))))
```

The result is a transformed dataset which contains transformed videos downsampled to have minimum dimension 128, framerate of 5Hz, with the annotations transformed accordingly.  The `vipy.dataset.Dataset.map` method allows for a lambda function to be applied in parallel to all elements in a dataset.  The fluent design of the VIPY objects allows for easy chaining of video operations to be expressed as a lambda function.  VIPY objects are designed for integration into parallel processing tool chains and can be easily serialized and deserialized for sending to parallel worker tasks.  

VIPY supports integration with distributed schedulers for massively parallel operation.  

```python
D = vipy.dataset.Dataset(&#39;/path/to/directory/of/jsonfiles&#39;)
with vipy.globals.parallel(scheduler=&#39;10.0.0.1:8785&#39;):
    R = D.map(lambda v, outdir=&#39;/newpath/to&#39;: vipy.util.bz2pkl(os.path.join(outdir, &#39;%s.pkl.bz2&#39; % v.videoid()), v.trackcrop().mindim(128).normalize(mean=(128,128,128)).torch()))
```

This will lazy load a directory of JSON files, where each JSON file corresponds to the annotations of a single video, such as those collected by [Visym Collector](https://visym.github.io/collector).   The `vipy.dataset.Dataset.map` method will communicate with a [scheduler](https://docs.dask.org/en/stable/how-to/deploy-dask/ssh.html) at a given IP address and port and will process the lambda function in parallel to the workers tasked by the scheduler.  In this example, the video will `vipy.video.Scene.trackcrop` the smallest bounding box containing all tracks in the video, resized so this crop is 128 on the smallest side, loaded and normalized to remove the mean, then saved as a torch tensor in a bzipped python pickle file.  This is useful for preprocesssing videos to torch tensors for fast loading of dataset augmentation during training.

## Import

Vipy was designed to define annotated videos and imagery as collections of python objects.  The core objects for images are:

* [vipy.image.Scene](image.html#vipy.image.Scene)
* [vipy.object.Detection](object.html#vipy.object.Detection)
* [vipy.geometry.BoundingBox](geometry.html#vipy.geometry.BoundingBox)

The core objects for videos:

* [vipy.video.Scene](video.html#vipy.video.Scene)
* [vipy.object.Track](object.html#vipy.object.Track)
* [vipy.activity.Activity](activity.html#vipy.activity.Activity)

See the documentation for each object for how to construct them.  Alternatively, see our [open source visual analytics](https://github.com/visym/heyvi) for construction of vipy objects from activity and object detectors.


## Export

All vipy objects can be imported and exported to JSON for interoperatability with other tool chains.  This allows for introspection of the vipy object state in an open format providing transparency

```python
vipy.image.owl().json()
```

## Environment variables

You can set the following environment variables to customize the output of vipy

* **VIPY_CACHE**=&#39;/path/to/directory&#39;.  This directory will contain all of the cached downloaded filenames when downloading URLs.  For example, the following will download all media to &#39;~/.vipy&#39;.

```python
os.environ[&#39;VIPY_CACHE&#39;] = vipy.util.remkdir(&#39;~/.vipy&#39;)
vipy.image.Image(url=&#39;https://upload.wikimedia.org/wikipedia/commons/thumb/2/23/Bubo_virginianus_06.jpg/1920px-Bubo_virginianus_06.jpg&#39;).download()
```

This will output an image object:
```python
&lt;vipy.image: filename=&#34;~/.vipy/1920px-Bubo_virginianus_06.jpg&#34;, filename=&#34;~/.vipy/1920px-Bubo_virginianus_06.jpg&#34;, url=&#34;https://upload.wikimedia.org/wikipedia/commons/thumb/2/23/Bubo_virginianus_06.jpg/1920px-Bubo_virginianus_06.jpg&#34;&gt;
```

This provides control over where large datasets are cached on your local file system.  By default, this will be cached to the system temp directory.

* **VIPY_AWS_ACCESS_KEY_ID**=&#39;MYKEY&#39;.  This is the [AWS key](https://docs.aws.amazon.com/general/latest/gr/aws-sec-cred-types.html) to download urls of the form &#34;s3://&#34;.  
* **VIPY_AWS_SECRET_ACCESS_KEY**=&#39;MYKEY&#39;.   This is the [AWS secret key](https://docs.aws.amazon.com/general/latest/gr/aws-sec-cred-types.html) to download urls of the form &#34;s3://&#34;.


## Versioning

To determine what vipy version you are running you can use:

&gt;&gt;&gt; vipy.__version__
&gt;&gt;&gt; vipy.version.is_at_least(&#39;1.11.1&#39;) 

# Tutorials

The following tutorials show fluent python chains to achieve transformations of annotated images and videos.

## Images

### Load an image

Images can be loaded from URLs, local image files, or numpy arrays.  The images exhibit lazy loading, so that pixels will not be fetched until they are needed.

```python
&gt;&gt;&gt; im = vipy.image.Image(filename=&#39;/path/to/in.jpg&#39;)  
&gt;&gt;&gt; im = vipy.image.Image(url=&#39;https://url/to/in.jpg&#39;)  
&gt;&gt;&gt; im = vipy.image.Image(array=np.random.rand(224,224,3).astype(np.float32))  
```

### Display an image to stdout

All objects have helpful string representations when printed to stdout.  This is accessible via the `vipy.image.Image.print` method or by using builtin print().  In this example, an image is created from a wikipedia URL.  Printing this image object shows the URL, but when it is loaded, the image object shows the size of the image, colorspace and the filename that the URL was downloaded to.  When in doubt, print!

```python
&gt;&gt;&gt; print(vipy.image.Scene(url=&#39;https://upload.wikimedia.org/wikipedia/commons/thumb/2/23/Bubo_virginianus_06.jpg/1920px-Bubo_virginianus_06.jpg&#39;))
&lt;vipy.image.scene: url=https://upload.wikimedia.org/wikipedia/commons/thumb/2/23/Bubo_virginianus_06.jpg/1920px-Bubo_virginianus_06.jpg&gt;

&gt;&gt;&gt; vipy.image.Scene(url=&#39;https://upload.wikimedia.org/wikipedia/commons/thumb/2/23/Bubo_virginianus_06.jpg/1920px-Bubo_virginianus_06.jpg&#39;).load().print()
&lt;vipy.image.scene: height=2400, width=1920, color=rgb, filename=&#34;/tmp/1920px-Bubo_virginianus_06.jpg&#34;, url=https://upload.wikimedia.org/wikipedia/commons/thumb/2/23/Bubo_virginianus_06.jpg/1920px-Bubo_virginianus_06.jpg&gt;
```

### Transform an image

Images can be transformed so that the annotations are updated along with the pixels.  In this example, the `vipy.image.owl` is a demo image to a wikipedia URL with a bounding box.  This can be resized and cropped or anisotropically scaled and the box is updated to match the pixels. 

```python
&gt;&gt;&gt; im = vipy.image.owl().mindim(512).fliplr().centersquare().show()
&gt;&gt;&gt; im = vipy.image.owl().resize(width=512, height=256).show()
```
&lt;img src=&#34;https://raw.githubusercontent.com/visym/vipy/master/docs/tutorials/transform_an_image_1.jpg&#34; height=&#34;250&#34;&gt;
&lt;img src=&#34;https://raw.githubusercontent.com/visym/vipy/master/docs/tutorials/transform_an_image_2.jpg&#34; height=&#34;250&#34;&gt;


### Export as numpy array

All images are represented internally as a private attribute `vipy.image.Image._array` which is a numpy array representation of the pixels.  Image transformations can be chained to operate sequentially on this pixel buffer.  In this example, the `vipy.image.owl` test image is cropped to retain the center square, converted from uint8 RGB to float32 greyscale, resized to 224x224 then exported to numpy array.  

```python
&gt;&gt;&gt; vipy.image.owl().centersquare().greyscale().mindim(224).numpy()
array([[0.11470564, 0.11794835, 0.13006495, ..., 0.15657625, 0.15867704,
        0.16140679],
       [0.11835834, 0.11993656, 0.12860955, ..., 0.15611856, 0.15460114,
        0.15652661],
       [0.12262769, 0.1245698 , 0.12809968, ..., 0.153694  , 0.15326852,
        0.15336327],
       ...,
       [0.42591274, 0.42745316, 0.4352066 , ..., 0.12994824, 0.13172676,
        0.13424061],
       [0.42972928, 0.43847743, 0.45459685, ..., 0.12558977, 0.12820148,
        0.13141613],
       [0.44050908, 0.45350933, 0.46908155, ..., 0.12246227, 0.1256479 ,
        0.12941177]], dtype=float32)
```

### Display an image

All images can be displayed using the matplotlib library.  Matplotlib is the most universally ported GUI library for python, and exhibits minimal dependencies.  We enable the user to show images using figure window or &#34;matlab style&#34; of image display.  This will show pixels with overlayed semi-transparent bounding boxes for objects with captions.

```python
&gt;&gt;&gt; im = vipy.image.owl().mindim(512).show()
```
&lt;img src=&#34;https://raw.githubusercontent.com/visym/vipy/master/docs/tutorials/display_an_image.jpg&#34; height=&#34;500&#34;&gt;

### Annotate an image

By default, images and annotations are represented independently.  However, it is sometimes useful to export the annotations into the pixels.  The `vipy.image.Scene.annotate` method will export the same visualization as when the image is displayed, but the pixel buffer will be overwritten with the shown image.  This means that calling `vipy.image.Image.numpy` will return the pixel buffer with boxes and captions in the pixels.

```python
&gt;&gt;&gt; vipy.image.owl().mindim(512).maxmatte().annotate().rgb().saveas(&#39;out.jpg&#39;)
```
&lt;img src=&#34;https://raw.githubusercontent.com/visym/vipy/master/docs/tutorials/annotate_an_image.jpg&#34; height=&#34;500&#34;&gt;


### Save an image

Images can be saved (without annotations) using the `vipy.image.Image.saveas` method.  Calling this method with no arguments will save to a random temporary image.  In this example, we crop the image, convert from RGB colorspace to BGR colorspace, flip up/down and resize.

```python
&gt;&gt;&gt; vipy.image.owl().centersquare().bgr().flipud().mindim(224).saveas(&#39;save_an_image.jpg&#39;)
```
&lt;img src=&#34;https://raw.githubusercontent.com/visym/vipy/master/docs/tutorials/save_an_image.jpg&#34; height=&#34;300&#34;&gt;

### Convert image colorspace

All images can be converted between different colorspaces (e.g. RGB, BGR, RGBA, BGRA, HSV, GREY, LUM, float).  This will convert the underlying pixel buffer to support the corresponding colorspace.  

``` 
&gt;&gt;&gt; vipy.image.owl().hsv().saveas(&#39;hsv.jpg&#39;)
```

### Rescale image

All images can be rescaled to a standard range, including the Matlab inspired `vipy.image.Image.mat2gray`, which will rescale the pixel buffer between [min, max] -&gt; [0, 1]


### Visualize scenes

Scenes containing objects can be visualized to display only a subset of objects.  In this example, we show the demo image `vipy.image.vehicles` which contains four annotated vehicles.  There are many more vehicles in this image, but the end user may be interested in these four in particular.  Each object is represented internally as a list of `vipy.object.Detection` objects which encodes a bounding box and category.  This can be visualized just as with images with single objects.

```python
&gt;&gt;&gt; vipy.image.vehicles().show().objects()
[&lt;vipy.object.detection: category=&#34;car&#34;, bbox=(xmin=210.2, ymin=263.2, width=41.1, height=32.6)&gt;,
 &lt;vipy.object.detection: category=&#34;car&#34;, bbox=(xmin=626.7, ymin=336.0, width=77.9, height=65.5)&gt;,
 &lt;vipy.object.detection: category=&#34;car&#34;, bbox=(xmin=140.8, ymin=284.5, width=53.1, height=53.1)&gt;,
 &lt;vipy.object.detection: category=&#34;car&#34;, bbox=(xmin=394.2, ymin=396.8, width=99.5, height=87.4)&gt;]
```
&lt;img src=&#34;https://raw.githubusercontent.com/visym/vipy/master/docs/tutorials/visualize_complex_scenes.jpg&#34; height=&#34;500&#34;&gt;

### Crop and resize annotated objects in a scene

```python
&gt;&gt;&gt; im = vipy.image.vehicles().show()
&gt;&gt;&gt; vipy.visualize.montage([o.dilate(1.2).maxsquare().crop() for o in im]).show()
```
&lt;img src=&#34;https://raw.githubusercontent.com/visym/vipy/master/docs/tutorials/vipy_image_vehicles.png&#34; height=&#34;300&#34;&gt;
&lt;img src=&#34;https://raw.githubusercontent.com/visym/vipy/master/docs/tutorials/vipy_image_vehicles_objectcrop.png&#34; height=&#34;300&#34;&gt;


### Find all images in directory

Searching for all images recursively from a root directory and lazy load them as `vipy.image.Image` objects.  This will not trigger loading pixels until the pixel buffers are needed.  This is helpful for importing large number of images.

```python
&gt;&gt;&gt; [vipy.image.Image(filename=f) for f in vipy.util.findimages(&#39;./docs/tutorials&#39;)]
[&lt;vipy.image: filename=&#34;/Users/jebyrne/dev/vipy/docs/tutorials/transform_an_image_1.jpg&#34;&gt;, &lt;vipy.image: filename=&#34;/Users/jebyrne/dev/vipy/docs/tutorials/transform_an_image_2.jpg&#34;&gt;, ... 
```

### Export scene to JSON

All annotated images can be imported and exported to an open JSON format. If images are loaded, then the pixels will be serialized in the JSON output.  If this is not desired, then use the `vipy.image.Image.flush`` method to clear the cached pixel buffer prior to serialization.  This can always be reloaded after deserialization as long as the source image or URL is acessible.

```python
&gt;&gt;&gt; json = vipy.image.owl().flush().json()
&gt;&gt;&gt; im = vipy.image.Scene.from_json(json)
&gt;&gt;&gt; print(json)
&#39;{&#34;_filename&#34;:&#34;\\/Users\\/jebyrne\\/.vipy\\/1920px-Bubo_virginianus_06.jpg&#34;,&#34;_url&#34;:&#34;https:\\/\\/upload.wikimedia.org\\/wikipedia\\/commons\\/thumb\\/2\\/23\\/Bubo_virginianus_06.jpg\\/1920px-Bubo_virginianus_06.jpg&#34;,&#34;_loader&#34;:null,&#34;_array&#34;:null,&#34;_colorspace&#34;:&#34;rgb&#34;,&#34;attributes&#34;:{},&#34;_category&#34;:&#34;Nature&#34;,&#34;_objectlist&#34;:[{&#34;_xmin&#34;:93.33333333333333,&#34;_ymin&#34;:85.33333333333333,&#34;_xmax&#34;:466.6666666666667,&#34;_ymax&#34;:645.3333333333334,&#34;_id&#34;:&#34;a047e21d&#34;,&#34;_label&#34;:&#34;Great Horned Owl&#34;,&#34;_shortlabel&#34;:&#34;Great Horned Owl&#34;}]}&#39;
```

### Export scene to CSV

All annotated images can be exported to a CSV format using object iterators.  Object precision can be changed using `vipy.object.Detection.int`.  CSV headers can be added with `vipy.util.writecsv`.

```python
&gt;&gt;&gt; im = vipy.image.vehicles()
&gt;&gt;&gt; vipy.util.writecsv([(im.filename(), o.category(), o.xmin(), o.ymin(), o.width(), o.height()) for o in im.objects()], &#39;out.csv&#39;)
&gt;&gt;&gt; cat out.csv
/Users/jebyrne/.vipy/I-80_Eastshore_Fwy.jpg,car,210.2222222222222,263.2,41.06666666666666,32.622222222222206
/Users/jebyrne/.vipy/I-80_Eastshore_Fwy.jpg,car,626.6666666666666,336.0444444444444,77.86666666666667,65.4666666666667
/Users/jebyrne/.vipy/I-80_Eastshore_Fwy.jpg,car,140.84444444444443,284.4888888888889,53.066666666666634,53.111111111111086
/Users/jebyrne/.vipy/I-80_Eastshore_Fwy.jpg,car,394.17777777777775,396.84444444444443,99.4666666666667,87.37777777777774
```

### Image deduplication

Vipy provides a 128 bit differential perceptual hashing function which is used for near-duplicate detection.  This is useful for identifying pairs of images that differ slightly due to cropping, resizing, watermarkings.  The binary Hamming distance between two perceptual hashes is a similarity metric that can be used to identify duplicates, such that smaller is more likely to be a duplicate.

```python
&gt;&gt;&gt; p = vipy.image.vehicles().perceptualhash()  # hex string
&gt;&gt;&gt; print(p)
&#39;50515541d545f04101a005e801c25945&#39;
&gt;&gt;&gt; q = vipy.image.vehicles().greyscale().perceptualhash()
&gt;&gt;&gt; print(q)
&#39;50515541d545f04101a905e801c27945&#39;
&gt;&gt;&gt; vipy.image.Image.perceptualhash_distance(p, q)  # Hamming distance
3
```

The perceptual hash function also allows for ignoring detected objects in the foreground.  A background hash `vipy.image.Scene.bghash` computes the perceptual hash function using only the regions not contained within the foreground bounding boxes.  This is useful for identifying near duplicate background locations where there may be different foreground objects in the scene between images.  If the `vipy.image.Scene` has no associated foreground objects, then the background hash is equivalent to the perceptual hash above.


### Blur Faces

```python
&gt;&gt;&gt; im = vipy.image.Image(url=&#39;https://upload.wikimedia.org/wikipedia/en/d/d6/Friends_season_one_cast.jpg&#39;)
&gt;&gt;&gt; im.facepixelize().show()
&gt;&gt;&gt; im.faceblur().show()
```
&lt;img src=&#34;https://raw.githubusercontent.com/visym/vipy/master/docs/tutorials/blur_faces_1.jpg&#34; height=&#34;250&#34;&gt;
&lt;img src=&#34;https://raw.githubusercontent.com/visym/vipy/master/docs/tutorials/blur_faces_2.jpg&#34; height=&#34;250&#34;&gt;



### Data augmentation for training

Data augmentation is the process of introducing synthetic transformations of a given image to introduce additional variation during training.  Data augmentation considers scales, crops, translations, mirrors, rotations or chromatic noise which are applied to a source image to generate one or more augmentations.  

```python
im = vipy.image.vehicles()
vipy.visualize.montage([[o.crop().fliplr(),                # spatial mirror
                         o.clone().dilate(1.2).crop(),     # zoom out
                         o.clone().translate(4,5).crop(),  # translation 
                         o.clone().translate(-2,9).crop(), # translation 
                         o.clone().dilate(0.8).crop(),     # zoom in 
                         o.crop().blur(sigma=1),           # spatial blur
                         o.crop().additive_noise()]        # chromatic noise 
                         for o in im])                     # for all objects in the scene
```
&lt;img src=&#34;https://raw.githubusercontent.com/visym/vipy/master/docs/tutorials/data_augmentation_for_training.jpg&#34; height=&#34;250&#34;&gt;


### Vipy vs. Torchvision

### Visualization behind SSH 

Data repositories are often accessed via data storage behind SSH.  You can set up port forwarding to visualize this data, but this may require root access to configure firewall rules.  If you have SSH public key access to your cluster machine, you can do the following:

On a remote machine (e.g. the cluster machine you have accessed via ssh), run:

```python
remote&gt;&gt;&gt; vipy.util.scpsave(vipy.image.owl())
[vipy.util.scpsave]: On a local machine where you have public key ssh access to this remote machine run:
&gt;&gt;&gt; V = vipy.util.scpload(&#39;scp://hostname:/var/folders/sn/6n34qjp513742_5y3lvmhnlw0000gn/T/c4237a25a99b776f.pkl&#39;)
```

Then, on your local machine (e.g. your laptop), run the command output above:

```python
local&gt;&gt;&gt; print(vipy.util.scpload(&#39;scp://hostname:/var/folders/sn/6n34qjp513742_5y3lvmhnlw0000gn/T/c4237a25a99b776f.pkl&#39;))
&lt;vipy.image.scene: height=640, width=512, color=rgb, filename=&#34;/Users/jebyrne/.vipy/1920px-Bubo_virginianus_06.jpg&#34;, url=https://upload.wikimedia.org/wikipedia/commons/thumb/2/23/Bubo_virginianus_06.jpg/1920px-Bubo_virginianus_06.jpg, category=&#34;Nature&#34;, objects=1&gt;
```

The method `vipy.util.scpsave` will save a list of vipy objects to a temporary pickle file, such that the URL of each object is prepended with &#34;scp://&#34;.  When calling `vipy.util.scpload` on the local machine, this will fetch the pickle file from the remote machine via scp using the default public key.  Then, when each vipy object is accessed, it will fetch the URL of the media object via scp from the remote machine.  This provides an on-demand fetching of each image from a data storage behind a SSH server without any port forwarding, and uses public key scp.  This allows for visualization of datasets that cannot be copied locally, but can be reduced on the local machine which are then fetched for visualization.


### Visualization behind AWS S3 


## Videos

### Load from YouTube

```python
v = vipy.video.Video(url=&#39;https://youtu.be/kpBCzzzX6zA&#39;)
```

### Inspect the FFMPEG command line

```python
print(vipy.video.Video(filename=&#39;/path/to/in.mp4&#39;).mindim(512).framerate(2).commandline())
```
```
&#39;ffmpeg -i /path/to/in.mp4 -filter_complex &#34;[0]fps=fps=2.0:round=up[s0];[s0]scale=-1:512[s1]&#34; -map &#34;[s1]&#34; dummyfile&#39;
```

### Export frames as vipy images

```python
frames = [im for im in v.framerate(1)]   # 1 Hz export
```

### Export frames as numpy array

```python
frames = v.framerate(0.1).numpy()   # 0.1 Hz export
```

### Generate WEBP animations

```python
v = vipy.video.RandomScene().clip(0,30).webp()
```

### Find all videos in directory

### Track objects in video

### Import RTSP camera streams

### Detect activities in video

### Blur People and Cars

### Make a video mosaic from many streams

### Split a video into activity clips

### Create quicklooks for fast video watching

### Stabilize background

### Create video thumbnails

### Create compressed and cached tensors for large-scale training


### Export to JSON

## Datasets

### Create a dataset

### Resize and crop

### Import to torch 

### Archive to .tar.gz

### Export to JSON

### Video data augmentation for training

### Create standalone HTML visualizations of images


# Contact

Visym Labs &lt;&lt;info@visym.com&gt;&gt;

&#34;&#34;&#34;

# Import all subpackages
import vipy.show  # matplotlib first
import vipy.activity
import vipy.annotation
import vipy.calibration
import vipy.downloader
import vipy.geometry
import vipy.image
import vipy.linalg
import vipy.math
import vipy.object
import vipy.util
import vipy.version
import vipy.video
import vipy.videosearch
import vipy.visualize
import vipy.dataset

__version__ = vipy.version.VERSION</code></pre>
</details>
</section>
<section>
<h2 class="section-title" id="header-submodules">Sub-modules</h2>
<dl>
<dt><code class="name"><a title="vipy.activity" href="activity.html">vipy.activity</a></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt><code class="name"><a title="vipy.annotation" href="annotation.html">vipy.annotation</a></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt><code class="name"><a title="vipy.batch" href="batch.html">vipy.batch</a></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt><code class="name"><a title="vipy.calibration" href="calibration.html">vipy.calibration</a></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt><code class="name"><a title="vipy.camera" href="camera.html">vipy.camera</a></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt><code class="name"><a title="vipy.data" href="data/index.html">vipy.data</a></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt><code class="name"><a title="vipy.dataset" href="dataset.html">vipy.dataset</a></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt><code class="name"><a title="vipy.downloader" href="downloader.html">vipy.downloader</a></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt><code class="name"><a title="vipy.flow" href="flow.html">vipy.flow</a></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt><code class="name"><a title="vipy.geometry" href="geometry.html">vipy.geometry</a></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt><code class="name"><a title="vipy.globals" href="globals.html">vipy.globals</a></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt><code class="name"><a title="vipy.gui" href="gui/index.html">vipy.gui</a></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt><code class="name"><a title="vipy.image" href="image.html">vipy.image</a></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt><code class="name"><a title="vipy.linalg" href="linalg.html">vipy.linalg</a></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt><code class="name"><a title="vipy.math" href="math.html">vipy.math</a></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt><code class="name"><a title="vipy.metrics" href="metrics.html">vipy.metrics</a></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt><code class="name"><a title="vipy.object" href="object.html">vipy.object</a></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt><code class="name"><a title="vipy.pyramid" href="pyramid.html">vipy.pyramid</a></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt><code class="name"><a title="vipy.show" href="show.html">vipy.show</a></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt><code class="name"><a title="vipy.ssim" href="ssim.html">vipy.ssim</a></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt><code class="name"><a title="vipy.torch" href="torch.html">vipy.torch</a></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt><code class="name"><a title="vipy.util" href="util.html">vipy.util</a></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt><code class="name"><a title="vipy.version" href="version.html">vipy.version</a></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt><code class="name"><a title="vipy.video" href="video.html">vipy.video</a></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt><code class="name"><a title="vipy.videosearch" href="videosearch.html">vipy.videosearch</a></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt><code class="name"><a title="vipy.visualize" href="visualize.html">vipy.visualize</a></code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
</article>
<nav id="sidebar">
<header>
<a class="homelink" rel="home" title="VIPY" href="https://github.com/visym/vipy/">
<img src="https://www.visym.com/labs/images/visym_logo_black_notext.png" alt="" width="60">
</a>
<h1 style="font-size:200%;"><b>VIPY:</b> Visual Dataset Transformation</h1>
</header>
<form>
<input id="lunr-search" name="q" placeholder="ðŸ”Ž Search ..." aria-label="Search"
disabled minlength="2">
</form>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/tingle/0.15.3/tingle.min.css" integrity="sha512-j1u8eUJ4f23xPPxwOrLUPQaCD2dwzNqqmDDcWS4deWsMv2ohLqmXXuP3hU7g8TyzbMSakP/mMqoNBYWj8AEIFg==" crossorigin>
<script src="https://cdnjs.cloudflare.com/ajax/libs/tingle/0.15.3/tingle.min.js" integrity="sha512-plGUER9JkeEWPPqQBE4sdLqBoQug5Ap+BCGMc7bJ8BXkm+VVj6QzkpBz5Yv2yPkkq+cqg9IpkBaGCas6uDbW8g==" crossorigin></script>
<style>
.modal-dialog iframe {
width: 100vw;
height: calc(100vh - 80px);
}
@media screen and (min-width: 700px) {
.modal-dialog iframe {
width: 70vw;
height: 80vh;
}
}
.modal-dialog .tingle-modal-box {width: auto;}
.modal-dialog .tingle-modal-box__content {padding: 0;}
</style>
<script>
const input = document.getElementById('lunr-search');
input.disabled = false;
input.form.addEventListener('submit', (ev) => {
ev.preventDefault();
const url = new URL(window.location);
url.searchParams.set('q', input.value);
history.replaceState({}, null, url.toString());
search(input.value);
});
const query = new URL(window.location).searchParams.get('q');
if (query)
search(query);
function search(query) {
const url = './doc-search.html#' + encodeURIComponent(query);
new tingle.modal({
cssClass: ['modal-dialog'],
onClose: () => {
const url = new URL(window.location);
url.searchParams.delete('q');
history.replaceState({}, null, url.toString());
setTimeout(() => input.focus(), 100);
}
}).setContent('<iframe src="' + url + '"></iframe>').open();
}
</script>
<h1>Index</h1>
<div class="toc">
<ul>
<li><a href="#design-goals">Design Goals</a></li>
<li><a href="#getting-started">Getting started</a><ul>
<li><a href="#parallelization">Parallelization</a></li>
<li><a href="#import">Import</a></li>
<li><a href="#export">Export</a></li>
<li><a href="#environment-variables">Environment variables</a></li>
<li><a href="#versioning">Versioning</a></li>
</ul>
</li>
<li><a href="#tutorials">Tutorials</a><ul>
<li><a href="#images">Images</a><ul>
<li><a href="#load-an-image">Load an image</a></li>
<li><a href="#display-an-image-to-stdout">Display an image to stdout</a></li>
<li><a href="#transform-an-image">Transform an image</a></li>
<li><a href="#export-as-numpy-array">Export as numpy array</a></li>
<li><a href="#display-an-image">Display an image</a></li>
<li><a href="#annotate-an-image">Annotate an image</a></li>
<li><a href="#save-an-image">Save an image</a></li>
<li><a href="#convert-image-colorspace">Convert image colorspace</a></li>
<li><a href="#rescale-image">Rescale image</a></li>
<li><a href="#visualize-scenes">Visualize scenes</a></li>
<li><a href="#crop-and-resize-annotated-objects-in-a-scene">Crop and resize annotated objects in a scene</a></li>
<li><a href="#find-all-images-in-directory">Find all images in directory</a></li>
<li><a href="#export-scene-to-json">Export scene to JSON</a></li>
<li><a href="#export-scene-to-csv">Export scene to CSV</a></li>
<li><a href="#image-deduplication">Image deduplication</a></li>
<li><a href="#blur-faces">Blur Faces</a></li>
<li><a href="#data-augmentation-for-training">Data augmentation for training</a></li>
<li><a href="#vipy-vs-torchvision">Vipy vs. Torchvision</a></li>
<li><a href="#visualization-behind-ssh">Visualization behind SSH</a></li>
<li><a href="#visualization-behind-aws-s3">Visualization behind AWS S3</a></li>
</ul>
</li>
<li><a href="#videos">Videos</a><ul>
<li><a href="#load-from-youtube">Load from YouTube</a></li>
<li><a href="#inspect-the-ffmpeg-command-line">Inspect the FFMPEG command line</a></li>
<li><a href="#export-frames-as-vipy-images">Export frames as vipy images</a></li>
<li><a href="#export-frames-as-numpy-array">Export frames as numpy array</a></li>
<li><a href="#generate-webp-animations">Generate WEBP animations</a></li>
<li><a href="#find-all-videos-in-directory">Find all videos in directory</a></li>
<li><a href="#track-objects-in-video">Track objects in video</a></li>
<li><a href="#import-rtsp-camera-streams">Import RTSP camera streams</a></li>
<li><a href="#detect-activities-in-video">Detect activities in video</a></li>
<li><a href="#blur-people-and-cars">Blur People and Cars</a></li>
<li><a href="#make-a-video-mosaic-from-many-streams">Make a video mosaic from many streams</a></li>
<li><a href="#split-a-video-into-activity-clips">Split a video into activity clips</a></li>
<li><a href="#create-quicklooks-for-fast-video-watching">Create quicklooks for fast video watching</a></li>
<li><a href="#stabilize-background">Stabilize background</a></li>
<li><a href="#create-video-thumbnails">Create video thumbnails</a></li>
<li><a href="#create-compressed-and-cached-tensors-for-large-scale-training">Create compressed and cached tensors for large-scale training</a></li>
<li><a href="#export-to-json">Export to JSON</a></li>
</ul>
</li>
<li><a href="#datasets">Datasets</a><ul>
<li><a href="#create-a-dataset">Create a dataset</a></li>
<li><a href="#resize-and-crop">Resize and crop</a></li>
<li><a href="#import-to-torch">Import to torch</a></li>
<li><a href="#archive-to-targz">Archive to .tar.gz</a></li>
<li><a href="#export-to-json_1">Export to JSON</a></li>
<li><a href="#video-data-augmentation-for-training">Video data augmentation for training</a></li>
<li><a href="#create-standalone-html-visualizations-of-images">Create standalone HTML visualizations of images</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#contact">Contact</a></li>
</ul>
</div>
<ul id="index">
<li><h3><a href="#header-submodules">Sub-modules</a></h3>
<ul>
<li><code><a title="vipy.activity" href="activity.html">vipy.activity</a></code></li>
<li><code><a title="vipy.annotation" href="annotation.html">vipy.annotation</a></code></li>
<li><code><a title="vipy.batch" href="batch.html">vipy.batch</a></code></li>
<li><code><a title="vipy.calibration" href="calibration.html">vipy.calibration</a></code></li>
<li><code><a title="vipy.camera" href="camera.html">vipy.camera</a></code></li>
<li><code><a title="vipy.data" href="data/index.html">vipy.data</a></code></li>
<li><code><a title="vipy.dataset" href="dataset.html">vipy.dataset</a></code></li>
<li><code><a title="vipy.downloader" href="downloader.html">vipy.downloader</a></code></li>
<li><code><a title="vipy.flow" href="flow.html">vipy.flow</a></code></li>
<li><code><a title="vipy.geometry" href="geometry.html">vipy.geometry</a></code></li>
<li><code><a title="vipy.globals" href="globals.html">vipy.globals</a></code></li>
<li><code><a title="vipy.gui" href="gui/index.html">vipy.gui</a></code></li>
<li><code><a title="vipy.image" href="image.html">vipy.image</a></code></li>
<li><code><a title="vipy.linalg" href="linalg.html">vipy.linalg</a></code></li>
<li><code><a title="vipy.math" href="math.html">vipy.math</a></code></li>
<li><code><a title="vipy.metrics" href="metrics.html">vipy.metrics</a></code></li>
<li><code><a title="vipy.object" href="object.html">vipy.object</a></code></li>
<li><code><a title="vipy.pyramid" href="pyramid.html">vipy.pyramid</a></code></li>
<li><code><a title="vipy.show" href="show.html">vipy.show</a></code></li>
<li><code><a title="vipy.ssim" href="ssim.html">vipy.ssim</a></code></li>
<li><code><a title="vipy.torch" href="torch.html">vipy.torch</a></code></li>
<li><code><a title="vipy.util" href="util.html">vipy.util</a></code></li>
<li><code><a title="vipy.version" href="version.html">vipy.version</a></code></li>
<li><code><a title="vipy.video" href="video.html">vipy.video</a></code></li>
<li><code><a title="vipy.videosearch" href="videosearch.html">vipy.videosearch</a></code></li>
<li><code><a title="vipy.visualize" href="visualize.html">vipy.visualize</a></code></li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>